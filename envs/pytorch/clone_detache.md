# Tensor:clone, detach

âŒšï¸: 2020å¹´8æœˆ9æ—¥

ğŸ“šå‚è€ƒ

---

tensorå¤åˆ¶å¯ä»¥ä½¿ç”¨`clone()`å‡½æ•°å’Œ`detach()`å‡½æ•°å³å¯å®ç°å„ç§éœ€æ±‚ã€‚

## 1. cloneä¸detachåŒºåˆ«

**clone**

clone()å‡½æ•°å¯ä»¥è¿”å›ä¸€ä¸ªå®Œå…¨ç›¸åŒçš„tensor,æ–°çš„tensorå¼€è¾Ÿæ–°çš„å†…å­˜ï¼Œä½†æ˜¯ä»ç„¶ç•™åœ¨è®¡ç®—å›¾ä¸­ã€‚

**detach**

detach()å‡½æ•°å¯ä»¥è¿”å›ä¸€ä¸ªå®Œå…¨ç›¸åŒçš„tensor,æ–°çš„tensorå¼€è¾Ÿä¸æ—§çš„tensorå…±äº«å†…å­˜ï¼Œæ–°çš„tensorä¼šè„±ç¦»è®¡ç®—å›¾ï¼Œä¸ä¼šç‰µæ‰¯æ¢¯åº¦è®¡ç®—ã€‚æ­¤å¤–ï¼Œä¸€äº›åŸåœ°æ“ä½œ(in-place, such as resize_ / resize_as_ / set_ / transpose_) åœ¨ä¸¤è€…ä»»æ„ä¸€ä¸ªæ‰§è¡Œéƒ½ä¼šå¼•å‘é”™è¯¯ã€‚

**ä½¿ç”¨åˆ†æ**

| # Operation             | New/Shared memory | Still in computation graph |
| ----------------------- | ----------------- | -------------------------- |
| tensor.clone()          | New               | Yes                        |
| tensor.detach()         | Shared            | No                         |
| tensor.clone().detach() | New               | No                         |

## 2. ä¾‹å­ğŸŒ°

å¦‚ä¸‹æ‰§è¡Œä¸€äº›å®ä¾‹ï¼š
é¦–å…ˆå¯¼å…¥åŒ…å¹¶å›ºå®šéšæœºç§å­

```python
import torch
torch.manual_seed(0)
```



1.clone()ä¹‹åçš„tensor requires_grad=True,detach()ä¹‹åçš„tensor requires_grad=Falseï¼Œä½†æ˜¯æ¢¯åº¦å¹¶ä¸ä¼šæµå‘clone()ä¹‹åçš„tensor

```python
x= torch.tensor([1., 2., 3.], requires_grad=True)
clone_x = x.clone()
detach_x = x.detach()
clone_detach_x = x.clone().detach()

f = torch.nn.Linear(3, 1)
y = f(x)
y.backward()

print(x.grad)
print(clone_x.requires_grad)
print(clone_x.grad)
print(detach_x.requires_grad)
print(clone_detach_x.requires_grad)
```

è¾“å‡ºï¼š

```python
tensor([-0.0043,  0.3097, -0.4752])
True
None
False
False
```



2.å°†è®¡ç®—å›¾ä¸­å‚ä¸è¿ç®—tensorå˜ä¸ºclone()åçš„tensorã€‚æ­¤æ—¶æ¢¯åº¦ä»ç„¶åªæµå‘äº†åŸå§‹çš„tensorã€‚

```python
x= torch.tensor([1., 2., 3.], requires_grad=True)
clone_x = x.clone()
detach_x = x.detach()
clone_detach_x = x.detach().clone()

f = torch.nn.Linear(3, 1)
y = f(clone_x)
y.backward()

print(x.grad)
print(clone_x.grad)
print(detach_x.requires_grad)
print(clone_detach_x.requires_grad)
```

è¾“å‡ºï¼š

```python
tensor([-0.0043,  0.3097, -0.4752])
None
False
False
```



3.å°†åŸå§‹tensorè®¾ä¸ºrequires_grad=Falseï¼Œclone()åçš„æ¢¯åº¦è®¾ä¸º.requires_grad_()ï¼Œclone()åçš„tensorå‚ä¸è®¡ç®—å›¾çš„è¿ç®—ï¼Œåˆ™æ¢¯åº¦ç©¿å‘clone()åçš„tensorã€‚

```python
x= torch.tensor([1., 2., 3.], requires_grad=False)
clone_x = x.clone().requires_grad_()
detach_x = x.detach()
clone_detach_x = x.detach().clone()

f = torch.nn.Linear(3, 1)
y = f(clone_x)
y.backward()

print(x.grad)
print(clone_x.grad)
print(detach_x.requires_grad)
print(clone_detach_x.requires_grad)
```

è¾“å‡ºï¼š

```python
None
tensor([-0.0043,  0.3097, -0.4752])
False
False
```



4.detach()åçš„tensorç”±äºä¸åŸå§‹tensorå…±äº«å†…å­˜ï¼Œæ‰€ä»¥åŸå§‹tensoråœ¨è®¡ç®—å›¾ä¸­æ•°å€¼åå‘ä¼ æ’­æ›´æ–°ä¹‹åï¼Œdetach()çš„tensorå€¼ä¹Ÿå‘ç”Ÿäº†æ”¹å˜ã€‚

```python
x = torch.tensor([1., 2., 3.], requires_grad=True)
f = torch.nn.Linear(3, 1)
w = f.weight.detach()
print(f.weight)
print(w)

y = f(x)
y.backward()

optimizer = torch.optim.SGD(f.parameters(), 0.1)
optimizer.step()

print(f.weight)
print(w)
```

è¾“å‡ºï¼š

```python
Parameter containing:
tensor([[-0.0043,  0.3097, -0.4752]], requires_grad=True)
tensor([[-0.0043,  0.3097, -0.4752]])
Parameter containing:
tensor([[-0.1043,  0.1097, -0.7752]], requires_grad=True)
tensor([[-0.1043,  0.1097, -0.7752]])
```