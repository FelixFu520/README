# APIå­¦ä¹ 

âŒšï¸:2021å¹´09æœˆ23æ—¥

ğŸ“šå‚è€ƒ



---

## ä¸€ã€æ¦‚å¿µæ¢³ç†

- (https://www.cnblogs.com/mrlonely2018/p/14842107.html)
- https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html

### 1ã€Logger, Context, Engine, Builder, Network, Parser çŸ¥è¯†æ¢³ç†

å…ˆæ¥çœ‹ä¸€ä¸‹å®˜æ–¹æ–‡æ¡£çš„apiç®€ä»‹ï¼š

> TensorRT provides a C++ implementation on all supported platforms, and a Python implementation on Linux. Python is not currently supported on Windows or QNX.
>
> The key interfaces in the TensorRT core library are:
> **Network Definition**
> The Network Definition interface provides methods for the application to define a network. Input and output tensors can be specified, and layers can be added and configured. As well as layer types, such as convolutional and recurrent layers, a Plugin layer type allows the application to implement functionality not natively supported by TensorRT.
>
> **Optimization Profile**
> An optimization profile specifies constraints on dynamic dimensions.
>
> **Builder Configuration**
> The Builder Configuration interface specifies details for creating an engine. It allows the application to specify optimization profiles, maximum workspace size, the minimum acceptable level of precision, timing iteration counts for autotuning, and an interface for quantizing networks to run in 8-bit precision.
>
> **Builder**
> The Builder interface allows the creation of an optimized engine from a network definition and a builder configuration.
>
> **Engine**
> The Engine interface allows the application to execute inference. It supports synchronous and asynchronous execution, profiling, and enumeration and querying of the bindings for the engine inputs and outputs. A single-engine can have multiple execution contexts, allowing a single set of trained parameters to be used for the simultaneous execution of multiple inferences.
>
> **ONNX Parser**
> This parser can be used to parse an ONNX model.
>
> **C++ API vs Python API**
> In theory, the C++ API and the Python API should be close to identical in supporting your needs. The C++ API should be used in any performance-critical scenarios, as well as in situations where safety is important, for example, in automotive.
> The main benefit of the Python API is that data preprocessing and postprocessing are easy to use because youâ€™re able to use a variety of libraries like NumPy and SciPy.

è°·æ­Œç¿»è¯‘ï¼š

> TensorRT åœ¨æ‰€æœ‰æ”¯æŒçš„å¹³å°ä¸Šæä¾› C++ å®ç°ï¼Œåœ¨ Linux ä¸Šæä¾› Python å®ç°ã€‚ Windows æˆ– QNX ç›®å‰ä¸æ”¯æŒ Pythonã€‚
>
> TensorRT æ ¸å¿ƒåº“ä¸­çš„å…³é”®æ¥å£æ˜¯ï¼š
> **ç½‘ç»œå®šä¹‰ Network Definition**
> ç½‘ç»œå®šä¹‰æ¥å£ä¸ºåº”ç”¨ç¨‹åºæä¾›äº†å®šä¹‰ç½‘ç»œçš„æ–¹æ³•ã€‚å¯ä»¥æŒ‡å®šè¾“å…¥å’Œè¾“å‡ºå¼ é‡ï¼Œå¹¶ä¸”å¯ä»¥æ·»åŠ å’Œé…ç½®å±‚ã€‚é™¤äº†å±‚ç±»å‹ï¼Œä¾‹å¦‚å·ç§¯å±‚å’Œå¾ªç¯å±‚ï¼Œæ’ä»¶å±‚ç±»å‹è¿˜å…è®¸åº”ç”¨ç¨‹åºå®ç° TensorRT æœ¬èº«ä¸æ”¯æŒçš„åŠŸèƒ½ã€‚
>
> **ä¼˜åŒ–é…ç½®æ–‡ä»¶ Optimization Profile**
> ä¼˜åŒ–é…ç½®æ–‡ä»¶æŒ‡å®šå¯¹åŠ¨æ€ç»´åº¦çš„çº¦æŸã€‚
>
> **æ„å»ºå™¨é…ç½® Builder Configuration**
> æ„å»ºå™¨é…ç½®æ¥å£æŒ‡å®šäº†åˆ›å»ºå¼•æ“çš„è¯¦ç»†ä¿¡æ¯ã€‚å®ƒå…è®¸åº”ç”¨ç¨‹åºæŒ‡å®šä¼˜åŒ–é…ç½®æ–‡ä»¶ã€æœ€å¤§å·¥ä½œç©ºé—´å¤§å°ã€æœ€å°å¯æ¥å—ç²¾åº¦æ°´å¹³ã€è‡ªåŠ¨è°ƒæ•´çš„æ—¶åºè¿­ä»£è®¡æ•°ä»¥åŠç”¨äºé‡åŒ–ç½‘ç»œä»¥ 8 ä½ç²¾åº¦è¿è¡Œçš„æ¥å£ã€‚
>
> **æ„å»ºå™¨Builder**
> Builder æ¥å£å…è®¸æ ¹æ®ç½‘ç»œå®šä¹‰å’Œæ„å»ºå™¨é…ç½®åˆ›å»ºä¼˜åŒ–å¼•æ“ã€‚
>
> **å¼•æ“**
> Engine æ¥å£å…è®¸åº”ç”¨ç¨‹åºæ‰§è¡Œæ¨ç†ã€‚å®ƒæ”¯æŒåŒæ­¥å’Œå¼‚æ­¥æ‰§è¡Œã€åˆ†æã€æšä¸¾å’ŒæŸ¥è¯¢å¼•æ“è¾“å…¥å’Œè¾“å‡ºçš„ç»‘å®šã€‚å•ä¸ªå¼•æ“å¯ä»¥æœ‰å¤šä¸ªæ‰§è¡Œä¸Šä¸‹æ–‡ï¼ˆexecution contextsï¼‰ï¼Œå…è®¸ä½¿ç”¨ä¸€ç»„ç»è¿‡è®­ç»ƒçš„å‚æ•°åŒæ—¶æ‰§è¡Œå¤šä¸ªæ¨ç†ã€‚
>
> **ONNXè§£æå™¨**
> æ­¤è§£æå™¨å¯ç”¨äºè§£æ ONNX æ¨¡å‹ã€‚
>
> **C++ API ä¸ Python API**
> ç†è®ºä¸Šï¼ŒC++ API å’Œ Python API åœ¨æ”¯æŒæ‚¨çš„éœ€æ±‚æ–¹é¢åº”è¯¥æ¥è¿‘ç›¸åŒã€‚ C++ API åº”è¯¥ç”¨äºä»»ä½•å¯¹æ€§èƒ½è‡³å…³é‡è¦çš„åœºæ™¯ï¼Œä»¥åŠå®‰å…¨å¾ˆé‡è¦çš„æƒ…å†µï¼Œä¾‹å¦‚åœ¨æ±½è½¦ä¸­ã€‚
> Python API çš„ä¸»è¦å¥½å¤„æ˜¯æ•°æ®é¢„å¤„ç†å’Œåå¤„ç†æ˜“äºä½¿ç”¨ï¼Œå› ä¸ºæ‚¨å¯ä»¥ä½¿ç”¨å„ç§åº“ï¼Œå¦‚ NumPy å’Œ SciPyã€‚

https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#api

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

#### ä»€ä¹ˆæ˜¯**Logger** ï¼Ÿ 

é¡¾åæ€ä¹‰ï¼Œè¿™æ˜¯æ—¥å¿—ç»„ä»¶ï¼Œç”¨äºç®¡ç†builder, engine å’Œ runtime çš„æ—¥å¿—ä¿¡æ¯ã€‚

æ ¹æ® tensorrt logging.h å¤´æ–‡ä»¶ä¸­å¯¹ Loggerç±» çš„æ³¨é‡Šï¼š

> This class provides a common interface for TensorRT tools and samples to log information to the console, and supports logging two types of messages:
> \- Debugging messages with an associated severity (info, warning, error, or internal error/fatal)
> \- Test pass/fail messages
> The advantage of having all samples use this class for logging as opposed to emitting directly to stdout/stderr is that the logic for controlling the verbosity and formatting of sample output is centralized in one location.
> In the future, this class could be extended to support dumping test results to a file in some standard format(for example, JUnit XML), and providing additional metadata (e.g. timing the duration of a test run).

è°·æ­Œç¿»è¯‘ï¼š

> è¯¥ç±»ä¸º TensorRT å·¥å…·å’Œç¤ºä¾‹æä¾›äº†ä¸€ä¸ªé€šç”¨æ¥å£æ¥å°†ä¿¡æ¯è®°å½•åˆ°æ§åˆ¶å°ï¼Œå¹¶æ”¯æŒè®°å½•ä¸¤ç§ç±»å‹çš„æ¶ˆæ¯ï¼š
> \- å…·æœ‰ç›¸å…³ä¸¥é‡æ€§ï¼ˆä¿¡æ¯ã€è­¦å‘Šã€é”™è¯¯æˆ–å†…éƒ¨é”™è¯¯/è‡´å‘½ï¼‰çš„è°ƒè¯•æ¶ˆæ¯
> \- æµ‹è¯•é€šè¿‡/å¤±è´¥æ¶ˆæ¯
> ä¸ç›´æ¥å‘é€åˆ° stdout/stderr ç›¸æ¯”ï¼Œè®©æ‰€æœ‰æ ·æœ¬éƒ½ä½¿ç”¨æ­¤ç±»è¿›è¡Œæ—¥å¿—è®°å½•çš„ä¼˜åŠ¿åœ¨äºï¼Œæ§åˆ¶æ ·æœ¬è¾“å‡ºçš„è¯¦ç»†ç¨‹åº¦å’Œæ ¼å¼çš„é€»è¾‘é›†ä¸­åœ¨ä¸€ä¸ªä½ç½®ã€‚
> å°†æ¥ï¼Œå¯ä»¥æ‰©å±•æ­¤ç±»ä»¥æ”¯æŒå°†æµ‹è¯•ç»“æœè½¬å‚¨åˆ°æŸç§æ ‡å‡†æ ¼å¼ï¼ˆä¾‹å¦‚ï¼ŒJUnit XMLï¼‰çš„æ–‡ä»¶ä¸­ï¼Œå¹¶æä¾›é¢å¤–çš„å…ƒæ•°æ®ï¼ˆä¾‹å¦‚ï¼Œå¯¹æµ‹è¯•è¿è¡Œçš„æŒç»­æ—¶é—´è¿›è¡Œè®¡æ—¶ï¼‰ã€‚

é€šå¸¸æ¥è¯´ï¼Œlogger ä¼šä½œä¸ºä¸€ä¸ªå¿…é¡»çš„å‚æ•°ä¼ é€’ç»™ builder runtime parserçš„å®ä¾‹åŒ–æ¥å£ï¼š

```
IBuilder* builder = createInferBuilder(gLogger);
IRuntime* runtime = createInferRuntime(gLogger);
auto parser = nvonnxparser::createParser(*network, gLogger);
```

Loggeråœ¨å†…éƒ¨è¢«è§†ä¸ºå•ä¾‹ï¼Œå› æ­¤ IRuntime å’Œ/æˆ– IBuilder çš„å¤šä¸ªå®ä¾‹å¿…é¡»éƒ½ä½¿ç”¨ç›¸åŒçš„Loggerã€‚

å‚è€ƒï¼š

https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/classnvinfer1_1_1_i_logger.html#details

https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#initialize_library

logging.h

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

#### **Context æ‰§è¡Œä¸Šä¸‹æ–‡**

ç±» IExecutionContext å®šä¹‰åœ¨ [NvInferRuntime.h](https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/_nv_infer_runtime_8h_source.html) å¤´æ–‡ä»¶ä¸­ï¼š

> Context for executing inference using an engine, with functionally unsafe features.
>
> Multiple execution contexts may exist for one [ICudaEngine](https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/classnvinfer1_1_1_i_cuda_engine.html) instance, allowing the same engine to be used for the execution of multiple batches simultaneously. If the engine supports dynamic shapes, each execution context in concurrent use must use a separate optimization profile.
>
> - Warning
>
>   Do not inherit from this class, as doing so will break forward-compatibility of the API and ABI.

è°·æ­Œç¿»è¯‘ï¼š

> ä½¿ç”¨å¼•æ“æ‰§è¡Œæ¨ç†çš„ä¸Šä¸‹æ–‡ï¼Œå…·æœ‰åŠŸèƒ½ä¸å®‰å…¨çš„ç‰¹æ€§ã€‚
>
> ä¸€ä¸ª ICudaEngine å®ä¾‹å¯èƒ½å­˜åœ¨å¤šä¸ªæ‰§è¡Œä¸Šä¸‹æ–‡ï¼Œå…è®¸ä½¿ç”¨åŒä¸€ä¸ªå¼•æ“åŒæ—¶æ‰§è¡Œå¤šä¸ªæ‰¹å¤„ç†ã€‚ å¦‚æœå¼•æ“æ”¯æŒåŠ¨æ€å½¢çŠ¶ï¼Œåˆ™å¹¶å‘ä½¿ç”¨çš„æ¯ä¸ªæ‰§è¡Œä¸Šä¸‹æ–‡å¿…é¡»ä½¿ç”¨å•ç‹¬çš„ä¼˜åŒ–é…ç½®æ–‡ä»¶ã€‚
>
> è­¦å‘Š
> ä¸è¦ä»æ­¤ç±»ç»§æ‰¿ï¼Œå› ä¸ºè¿™æ ·åšä¼šç ´å API å’Œ ABI çš„å‘å‰å…¼å®¹æ€§ã€‚

ä¸€èˆ¬çš„ä½¿ç”¨æ–¹æ³•ï¼š

> In order to run inference, use the interface IExecutionContext. In order to create an object of type IExecutionContext, first create an object of type ICudaEngine (the engine).
>
> The builder or runtime will be created with the GPU context associated with the creating thread.Even though it is possible to avoid creating the CUDA context, (the default context will be created for you), it is not advisable. It is recommended to create and configure the CUDA context before creating a runtime or builder object.

è°·æ­Œç¿»è¯‘ï¼š

> ä¸ºäº†è¿è¡Œæ¨ç†ï¼Œè¯·ä½¿ç”¨æ¥å£ IExecutionContextã€‚ ä¸ºäº†åˆ›å»ºä¸€ä¸ª IExecutionContext ç±»å‹çš„å¯¹è±¡ï¼Œé¦–å…ˆåˆ›å»ºä¸€ä¸ª ICudaEngineï¼ˆå¼•æ“ï¼‰ç±»å‹çš„å¯¹è±¡ã€‚
>
> æ„å»ºå™¨æˆ–è¿è¡Œæ—¶å°†ä½¿ç”¨ä¸åˆ›å»ºçº¿ç¨‹å…³è”çš„ GPU ä¸Šä¸‹æ–‡åˆ›å»ºã€‚å³ä½¿å¯ä»¥é¿å…åˆ›å»º CUDA ä¸Šä¸‹æ–‡ï¼ˆå°†ä¸ºæ‚¨åˆ›å»ºé»˜è®¤ä¸Šä¸‹æ–‡ï¼‰ï¼Œä½†ä¸å»ºè®®è¿™æ ·åšã€‚ å»ºè®®åœ¨åˆ›å»ºè¿è¡Œæ—¶æˆ–æ„å»ºå™¨å¯¹è±¡ä¹‹å‰åˆ›å»ºå’Œé…ç½® CUDA ä¸Šä¸‹æ–‡ã€‚

å¸¸è§ç”¨æ³•ï¼š

```
const ICudaEngine& engine = context.getEngine();
IExecutionContext* context = engine->createExecutionContext();
context->destroy();
context.enqueue(batchSize, buffers, stream, nullptr);
//TensorRT execution is typically asynchronous, so enqueue the kernels on a CUDA stream.
//It is common to enqueue asynchronous memcpy() before and after the kernels to move data from the GPU if it is not already there. 
//The final argument to enqueueV2() is an optional CUDA event which will be signaled when the input buffers have been consumed and their memory may be safely reused.
//For more information, refer to enqueue() for implicit batch networks and enqueueV2() for explicit batch networks. 
//In the event that asynchronous is not wanted, see execute() and executeV2().
//The IExecutionContext contains shared resources, therefore, calling enqueue or enqueueV2 in from the same IExecutionContext object with different CUDA streams concurrently results in undefined behavior. 
//To perform inference concurrently in multiple CUDA streams, use one IExecutionContext per CUDA stream.
```

 ä½¿ç”¨å¼•æ“è¿›è¡Œæ¨ç†éœ€è¦ç”¨åˆ°æ‰§è¡Œä¸Šä¸‹æ–‡ã€‚

```
IExecutionContext *context = engine->createExecutionContext();
```

> Create some space to store intermediate activation values. Since the engine holds the network definition and trained parameters, additional space is necessary. These are held in an execution context.
>
> An engine can have multiple execution contexts, allowing one set of weights to be used for multiple overlapping inference tasks. For example, you can process images in parallel CUDA streams using one engine and one context per stream. Each context will be created on the same GPU as the engine.

 

> åˆ›å»ºä¸€äº›ç©ºé—´æ¥å­˜å‚¨ä¸­é—´æ¿€æ´»å€¼ã€‚ ç”±äºå¼•æ“ä¿å­˜ç½‘ç»œå®šä¹‰å’Œè®­ç»ƒå‚æ•°ï¼Œå› æ­¤éœ€è¦é¢å¤–çš„ç©ºé—´ã€‚ è¿™äº›ä¿å­˜åœ¨æ‰§è¡Œä¸Šä¸‹æ–‡ä¸­ã€‚
>
> ä¸€ä¸ªå¼•æ“å¯ä»¥æœ‰å¤šä¸ªæ‰§è¡Œä¸Šä¸‹æ–‡ï¼Œå…è®¸ä¸€ç»„æƒé‡ç”¨äºå¤šä¸ªé‡å çš„æ¨ç†ä»»åŠ¡ã€‚ ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä¸€ä¸ªå¼•æ“å’Œæ¯ä¸ªæµä¸€ä¸ªä¸Šä¸‹æ–‡å¤„ç†å¹¶è¡Œ CUDA æµä¸­çš„å›¾åƒã€‚ æ¯ä¸ªä¸Šä¸‹æ–‡éƒ½å°†åœ¨ä¸å¼•æ“ç›¸åŒçš„ GPU ä¸Šåˆ›å»ºã€‚

å‚è€ƒï¼š

https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#perform_inference_c

https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#initialize_library

https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/classnvinfer1_1_1_i_execution_context.html#details

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

#### **Engine** 

ç±» ICudaEngine å®šä¹‰åœ¨ [NvInferRuntime.h](https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/_nv_infer_runtime_8h_source.html) å¤´æ–‡ä»¶ä¸­ï¼š

> An engine for executing inference on a built network, with functionally unsafe features.
>
> - Warning
>
>   Do not inherit from this class, as doing so will break forward-compatibility of the API and ABI.

 

> ç”¨äºåœ¨æ„å»ºçš„ç½‘ç»œä¸Šæ‰§è¡Œæ¨ç†çš„å¼•æ“ï¼Œå…·æœ‰åŠŸèƒ½ä¸å®‰å…¨çš„ç‰¹æ€§ã€‚
>
> è­¦å‘Š
> ä¸è¦ä»æ­¤ç±»ç»§æ‰¿ï¼Œå› ä¸ºè¿™æ ·åšä¼šç ´å API å’Œ ABI çš„å‘å‰å…¼å®¹æ€§ã€‚

ä½¿ç”¨builderåˆ›å»ºengineï¼š

```
IBuilderConfig* config = builder->createBuilderConfig();
config->setMaxWorkspaceSize(1 << 20);
ICudaEngine* engine = builder->buildEngineWithConfig(*network, *config);
```

å½“ç„¶ï¼Œåœ¨æ­¤ä¹‹å‰éœ€è¦æ­å»ºå®Œæ•´çš„ç½‘ç»œï¼š

![img](imgs/1559087-20210603110445225-1108459406.png)

å‚è€ƒï¼š

https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/classnvinfer1_1_1_i_cuda_engine.html#details

https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#build_engine_c

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

#### **Network**

ç±» [INetworkDefinition](https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/classnvinfer1_1_1_i_network_definition.html) å®šä¹‰åœ¨ NvInfer.h å¤´æ–‡ä»¶ä¸­ï¼š

> A network definition for input to the builder.
>
> A network definition defines the structure of the network, and combined with a [IBuilderConfig](https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/classnvinfer1_1_1_i_builder_config.html), is built into an engine using an [IBuilder](https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/classnvinfer1_1_1_i_builder.html). An [INetworkDefinition](https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/classnvinfer1_1_1_i_network_definition.html) can either have an implicit batch dimensions, specified at runtime, or all dimensions explicit, full dims mode, in the network definition. When a network has been created using createNetwork(), only implicit batch size mode is supported. The function [hasImplicitBatchDimension()](https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/classnvinfer1_1_1_i_network_definition.html#a728af2da798dcda6cc859d05e1989724) is used to query the mode of the network.
>
> A network with implicit batch dimensions returns the dimensions of a layer without the implicit dimension, and instead the batch is specified at execute/enqueue time. If the network has all dimensions specified, then the first dimension follows elementwise broadcast rules: if it is 1 for some inputs and is some value N for all other inputs, then the first dimension of each outut is N, and the inputs with 1 for the first dimension are broadcast. Having divergent batch sizes across inputs to a layer is not supported.

 

> ä½œä¸ºæ„å»ºå™¨è¾“å…¥çš„ç½‘ç»œå®šä¹‰ã€‚
>
> ç½‘ç»œå®šä¹‰å®šä¹‰äº†ç½‘ç»œçš„ç»“æ„ï¼Œä¸ IBuilderConfig ç»“åˆä½¿ç”¨ IBuilder æ„å»ºåˆ°å¼•æ“ä¸­ã€‚ INetworkDefinition å¯ä»¥å…·æœ‰åœ¨è¿è¡Œæ—¶æŒ‡å®šçš„éšå¼æ‰¹å¤„ç†ç»´åº¦ï¼Œæˆ–æ‰€æœ‰ç»´åº¦æ˜¾å¼ã€å®Œå…¨ç»´åº¦æ¨¡å¼ã€‚ä½¿ç”¨ createNetwork() åˆ›å»ºç½‘ç»œåï¼Œä»…æ”¯æŒéšå¼æ‰¹é‡å¤§å°æ¨¡å¼ã€‚å‡½æ•° hasImplicitBatchDimension() ç”¨äºæŸ¥è¯¢ç½‘ç»œçš„æ¨¡å¼ã€‚
>
> å…·æœ‰éšå¼æ‰¹å¤„ç†ç»´åº¦çš„ç½‘ç»œè¿”å›æ²¡æœ‰éšå¼ç»´åº¦çš„å±‚çš„ç»´åº¦ï¼Œè€Œæ˜¯åœ¨execute/enqueue æ—¶æŒ‡å®šæ‰¹å¤„ç†å¤§å°ã€‚å¦‚æœç½‘ç»œæŒ‡å®šäº†æ‰€æœ‰ç»´åº¦ï¼Œåˆ™ç¬¬ä¸€ä¸ªç»´åº¦éµå¾ªå…ƒç´ å¹¿æ’­è§„åˆ™ï¼šå¦‚æœæŸäº›è¾“å…¥ä¸º 1ï¼Œæ‰€æœ‰å…¶ä»–è¾“å…¥ä¸ºæŸä¸ªå€¼ Nï¼Œåˆ™æ¯ä¸ªè¾“å‡ºçš„ç¬¬ä¸€ä¸ªç»´åº¦ä¸º Nï¼Œè¾“å…¥ä¸º 1ç¬¬ä¸€ä¸ªç»´åº¦æ˜¯å¹¿æ’­ã€‚ä¸æ”¯æŒè·¨å±‚çš„è¾“å…¥å…·æœ‰ä¸åŒçš„æ‰¹æ¬¡å¤§å°ã€‚

ç½‘ç»œæ­å»ºç¤ºä¾‹ï¼šï¼ˆC++æ¥å£ï¼Œç¬¬ä¸€ä¸ªæ˜¯å®é™…ç”¨ä¾‹ï¼Œç¬¬äºŒä¸ªæ˜¯å®˜æ–¹æ–‡æ¡£ç¤ºä¾‹ï¼‰

Use method IBuilder::createNetworkV2 to create an object of type INetworkDefinition.

```
INetworkDefinition* network = builder->createNetworkV2(0U);
IBuilder* builder = createInferBuilder(gLogger);
INetworkDefinition* network = builder->createNetworkV2(1U << static_cast<uint32_t>(NetworkDefinitionCreationFlag::kEXPLICIT_BATCH));
```

Add the Input layer to the network, with the input dimensions, including dynamic batch.

```
ITensor* data = network->addInput(INPUT_BLOB_NAME, dt, Dims3{1, INPUT_H, INPUT_W});
auto data = network->addInput(INPUT_BLOB_NAME, dt, Dims3{-1, 1, INPUT_H, INPUT_W});
```

Add the Convolution layer

```
IConvolutionLayer* conv1 = network->addConvolutionNd(*data, 6, DimsHW{5, 5}, weightMap["conv1.weight"], weightMap["conv1.bias"]);
conv1->setStrideNd(DimsHW{1, 1});
auto conv1 = network->addConvolution(*data->getOutput(0), 20, DimsHW{5, 5}, weightMap["conv1filter"], weightMap["conv1bias"]);
conv1->setStride(DimsHW{1, 1});
```

Note: Weights passed to TensorRT layers are in host memory.

Add the Pooling layer

```
IPoolingLayer* pool1 = network->addPoolingNd(*relu1->getOutput(0), PoolingType::kAVERAGE, DimsHW{2, 2});
pool1->setStrideNd(DimsHW{2, 2});
auto pool1 = network->addPooling(*conv1->getOutput(0), PoolingType::kMAX, DimsHW{2, 2});
pool1->setStride(DimsHW{2, 2});
```

Add activation layer using the ReLU algorithm

```
IActivationLayer* relu1 = network->addActivation(*conv1->getOutput(0), ActivationType::kRELU);
auto relu1 = network->addActivation(*ip1->getOutput(0), ActivationType::kRELU);
```

 Add fully connected layer

```
IFullyConnectedLayer* fc1 = network->addFullyConnected(*pool2->getOutput(0), 120, weightMap["fc1.weight"], weightMap["fc1.bias"]);
auto ip1 = network->addFullyConnected(*pool1->getOutput(0), 500, weightMap["ip1filter"], weightMap["ip1bias"]);
```

Add the SoftMax layer to calculate the final probabilities and set it as the output:

```
ISoftMaxLayer* prob = network->addSoftMax(*fc3->getOutput(0));
prob->getOutput(0)->setName(OUTPUT_BLOB_NAME);
network->markOutput(*prob->getOutput(0));
auto prob = network->addSoftMax(*relu1->getOutput(0));
prob->getOutput(0)->setName(OUTPUT_BLOB_NAME);
network->markOutput(*prob->getOutput(0));
```

å‚è€ƒï¼š

https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#create_network_c

https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/classnvinfer1_1_1_i_network_definition.html#details

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

#### **parser**

è§£æå™¨ä¸»è¦ç”¨äºè§£æONNXæ¨¡å‹å¹¶å°†å…¶è½¬æ¢ä¸ºtensorrtæ¨¡å‹ã€‚ç±»IParser å®šä¹‰åœ¨[NvOnnxParser.h](https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/_nv_onnx_parser_8h_source.html) å¤´æ–‡ä»¶ä¸­ï¼š

> an object for parsing ONNX models into a TensorRT network definition

 Create an ONNX parser using the INetwork definition as the input:

```
auto parser = nvonnxparser::createParser(*network, gLogger);
```

å‚è€ƒï¼š

https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#initialize_library

https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/classnvonnxparser_1_1_i_parser.html#details

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

###  2ã€TensorRTå¤šGPUçš„ä½¿ç”¨

- https://blog.csdn.net/qq_29007291/article/details/110551881


Q: How do I use TensorRT on multiple GPUs?
å¦‚ä½•åœ¨å¤šGPUç¯å¢ƒä¸­ä½¿ç”¨TensorRT

A: Each ICudaEngine object is bound to a specific GPU when it is instantiated, either
by the builder or on deserialization. To select the GPU, use cudaSetDevice() before
calling the builder or deserializing the engine. Each IExecutionContext is bound
to the same GPU as the engine from which it was created. When calling execute()
or enqueue(), ensure that the thread is associated with the correct device by calling
cudaSetDevice() if necessary

æ¯ä¸ªICudaEngineå¯¹è±¡è¢«å®ä¾‹åŒ–çš„æ—¶å€™ï¼ˆbuilder æˆ–è€…deserializationï¼‰éƒ½ä¼šç»‘å®šåœ¨æŒ‡å®šçš„GPUä¸Šã€‚å¦‚æœè¦é€‰æ‹©GPU, åˆ™åº”è¯¥åœ¨åˆ›å»ºengineæˆ–è€…ååºåˆ—åŒ–engineçš„æ—¶å€™ä½¿ç”¨cudaSetDeviceï¼ˆï¼‰è¿›è¡Œè®¾å®šã€‚æ¯ä¸€ä¸ªIExecutionContextéƒ½è¢«ç»‘å®šåœ¨äº†engineè¢«åˆ›å»ºçš„é‚£ä¸ªGPUä¸Šã€‚å½“ä½¿ç”¨execute()æˆ–è€…enqueue() éœ€è¦æ˜ç¡®ä¸å½“å‰æ˜¾å¡æœ‰å…³çš„çº¿ç¨‹

### 3ã€onnxè½¬TensorRTæ¨¡å‹æŠ¥é”™


æŠ¥é”™ä¿¡æ¯ï¼š

```
[2021-07-26 07:16:07   ERROR] 2: [ltWrapper.cpp::setupHeuristic::327] Error Code 2: Internal Error (Assertion cublasStatus == CUBLAS_STATUS_SUCCESS failed.)
terminate called after throwing an instance of 'std::runtime_error'
  what():  Failed to create object
å·²æ”¾å¼ƒ (æ ¸å¿ƒå·²è½¬å‚¨)
```


è§£å†³åŠæ³•ï¼š
æ­¤å¤„æŠ¥é”™ä½¿ç”¨çš„æ˜¯cuda10.2
åœ¨æ­¤ä¸‹è½½è¡¥ä¸1å¹¶å®‰è£…å³å¯

### 4ã€TensorRT å®˜æ–¹æ‰‹å†Œä»‹ç»

- [å®˜æ–¹è‹±æ–‡](https://docs.nvidia.com/deeplearning/tensorrt/index.html)
- [ä¸­æ–‡](https://blog.csdn.net/irving512/article/details/114019280)

### 5ã€TensorRT dynamic batchï¼ˆpythonï¼‰

- https://github.com/TD-4/Tensorrt-CV

### 6ã€