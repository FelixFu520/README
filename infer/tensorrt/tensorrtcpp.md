## 1ã€æ¨ç†æ¼”ç¤º | å…«æ­¥åŠ©ä½ æå®štensorRT C++ SDKè°ƒç”¨ï¼

âŒšï¸:2021-03-12

ğŸ“šå‚è€ƒ

- [åŸæ–‡](https://cloud.tencent.com/developer/article/1800743)

---



### èƒŒæ™¯

Helloï¼Œ2020å¹´åº•æˆ‘å®‰è£…é…ç½®å¥½äº†TensorRT7ï¼Œå†™äº†ä¸€ç¯‡æ–‡ç« æ€»ç»“äº†åœ¨Widnows10ç³»ç»Ÿä¸­å¦‚ä½•é…ç½®ä¸è¿è¡Œä»£ç å®ç°TensorRTå¼€å‘ç¯å¢ƒçš„æ­å»ºã€‚æ–‡ç« ä¸­è¯¦ç»†ä»‹ç»äº†é…ç½®çš„æµç¨‹ä¸æ­¥éª¤ï¼Œæ–‡ç« çš„é“¾æ¥å¦‚ä¸‹ï¼š

[äº”åˆ†é’Ÿæå®šVS2017+TensorRTç¯å¢ƒæ­å»º](http://mp.weixin.qq.com/s?__biz=MzA4MDExMDEyMw==&mid=2247491109&idx=1&sn=2a69c87f78e0c330163de913d0e81944&chksm=9fa86961a8dfe0775bbaacf4acad833766f32b80606859a56ed79e57b3a58772c70cbbeeba0a&scene=21#wechat_redirect)

å½“æ—¶è¿˜å½•äº†ä¸€ä¸ªè§†é¢‘ï¼Œæˆ‘ä¸Šä¼ åˆ°äº†Bç«™ï¼Œè§‰å¾—çœ‹æ–‡ç« éº»çƒ¦ï¼Œå°±çœ‹**Bç«™è§†é¢‘**å§ï¼åœ°å€åœ¨è¿™é‡Œï¼š

```javascript
https://www.bilibili.com/video/BV1Bf4y167Ty
```

ä»Šå¤©ç»™å¤§å®¶åˆ†äº«ä¸€ä¸‹ï¼Œæˆ‘è·‘çš„ç¬¬ä¸€ä¸ªPytorch + TensorRTçš„æ¨¡å‹è®­ç»ƒä¸éƒ¨ç½²ä¾‹å­ã€‚ä¸ä¹…ä»¥å‰æˆ‘å†™è¿‡ä¸€ä¸ªç³»åˆ—æ–‡ç« å«åšã€Šè½»æ¾å­¦Pytorchç³»åˆ—ã€‹ï¼Œå…¶ä¸­æœ‰ä¸€ç¯‡CNNå…¥é—¨çš„æ–‡ç« ï¼Œæ˜¯è®²å¦‚ä½•é€šè¿‡CNNè®­ç»ƒmnistæ•°æ®é›†ï¼Œç„¶åå¯¼å‡ºæ¨¡å‹ONNXæ ¼å¼ï¼Œåœ¨OpenCV DNNä¸­è°ƒç”¨çš„ã€‚ä»Šå¤©æˆ‘å°±è¿˜ç»§ç»­ç”¨æˆ‘å¯¼å‡ºçš„ONNXæ¨¡å‹ï¼Œå®ç°å®ƒåœ¨TensorRT7ä¸­çš„è°ƒç”¨ï¼Œå®Œæˆä¸€ä¸ªTensorRTç‰ˆæœ¬çš„æ‰‹å†™æ•°å­—è¯†åˆ«æ¨¡å‹çš„éƒ¨ç½²ã€‚

é…ç½®

è¦æƒ³è¿è¡Œè¿™ä¸ªä¾‹å­ï¼Œè¿˜éœ€è¦é…ç½®ä¸€æ³¢å¼€å‘ç¯å¢ƒï¼Œåœ¨ä¹‹å‰çš„é…ç½®çš„åŸºç¡€ä¸Šï¼Œåˆ†åˆ«å€’å…¥å®‰è£…å¥½çš„CUDAçš„includeç›®å½•ï¼Œlibç›®å½•ï¼Œç„¶åæŠŠlibç›®å½•é‡Œé¢çš„*.libæ–‡ä»¶ç»Ÿç»Ÿæ‰”åˆ°é“¾æ¥å™¨ä¸­å»ã€‚è¿™æ ·å°±å®Œæˆäº†å¼€å‘é…ç½®ã€‚æˆ‘çš„CUDAå®‰è£…è·¯å¾„å¦‚ä¸‹ï¼š

```javascript
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0
includeç›®å½•ä¸ºï¼š
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\include
Libç›®å½•ä¸ºï¼š
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\lib
```

æ¨ç†ä¸æ¼”ç¤º

TensorRTçš„åŠ è½½æ¨¡å‹æ‰§è¡Œæ¨ç†çš„æ­¥éª¤åŸºæœ¬ä¸Šè·ŸOpenVINOä¸OpenCV DNNå¾ˆç›¸ä¼¼ï¼Œå”¯ä¸€åŒºåˆ«çš„åœ°æ–¹åœ¨äºä½¿ç”¨tensorRTåšæ¨ç†ï¼Œé¦–å…ˆéœ€è¦æŠŠæ•°æ®ä»å†…å­˜æ¬åˆ°æ˜¾å­˜ï¼Œå¤„ç†å®Œä¹‹åå†é‡æ–°æ¬å›å†…å­˜ï¼Œç„¶åè§£æè¾“å‡ºã€‚åŸºæœ¬æ­¥éª¤ä¸ä»£ç å¦‚ä¸‹ï¼š

### **åˆ›å»ºç½‘ç»œ**

```javascript
IBuilder* builder = createInferBuilder(gLogger);
nvinfer1::INetworkDefinition* network = builder->createNetworkV2(1U << static_cast<uint32_t>(NetworkDefinitionCreationFlag::kEXPLICIT_BATCH));
auto parser = nvonnxparser::createParser(*network, gLogger);
```

### **è§£æONNXæ ¼å¼æ¨¡å‹æ–‡ä»¶**

```javascript
// è§£æONNXæ¨¡å‹
parser->parseFromFile(onnx_filename.c_str(), 2);
for (int i = 0; i < parser->getNbErrors(); ++i)
{
    std::cout << parser->getError(i)->desc() << std::endl;
}
printf("tensorRT load onnx mnist model...\n");
```

### **åˆ›å»ºæ¨ç†å¼•æ“**

```javascript
// åˆ›å»ºæ¨ç†å¼•æ“
IBuilderConfig* config = builder->createBuilderConfig();
config->setMaxWorkspaceSize(1 << 20);
config->setFlag(nvinfer1::BuilderFlag::kFP16);
ICudaEngine* engine = builder->buildEngineWithConfig(*network, *config);
IExecutionContext *context = engine->createExecutionContext();
```

### **è·å–è¾“å…¥è¾“å‡ºæ ¼å¼ä¸åç§°**

```javascript
// è·å–è¾“å…¥ä¸è¾“å‡ºåç§°ï¼Œæ ¼å¼
const char* input_blob_name = network->getInput(0)->getName();
const char* output_blob_name = network->getOutput(0)->getName();
printf("input_blob_name : %s \n", input_blob_name);
printf("output_blob_name : %s \n", output_blob_name);

const int inputH = network->getInput(0)->getDimensions().d[2];
const int inputW = network->getInput(0)->getDimensions().d[3];
printf("inputH : %d, inputW: %d \n", inputH, inputW);
```

### **è®¾ç½®è¾“å…¥æ•°æ®**

```javascript
// é¢„å¤„ç†è¾“å…¥æ•°æ®
Mat image = imread("D:/images/9_99.png", IMREAD_GRAYSCALE);
imshow("è¾“å…¥å›¾åƒ", image);
Mat img2;
image.convertTo(img2, CV_32F);
img2 = (img2 / 255 - 0.5) / 0.5;
```

### **åˆ›å»ºè¾“å…¥/è¾“å‡º æ˜¾å­˜ç¼“å†²åŒº**

```javascript
// åˆ›å»ºGPUæ˜¾å­˜è¾“å…¥/è¾“å‡ºç¼“å†²åŒº
void* buffers[2] = { NULL, NULL };
int nBatchSize = 1;
int nOutputSize = 10;
cudaMalloc(&buffers[0], nBatchSize * inputH * inputW * sizeof(float));
cudaMalloc(&buffers[1], nBatchSize * nOutputSize * sizeof(float));

// åˆ›å»ºcudaæµ
cudaStream_t stream;
cudaStreamCreate(&stream);
void *data = malloc(nBatchSize * inputH * inputW * sizeof(float));
memcpy(data, img2.ptr<float>(0), inputH * inputW * sizeof(float));
```

### æ‰§è¡Œæ¨ç†

```javascript
// å†…å­˜åˆ°GPUæ˜¾å­˜
cudaMemcpyAsync(buffers[0], data, \
nBatchSize * inputH * inputW * sizeof(float), cudaMemcpyHostToDevice, stream);
std::cout << "start to infer image..." << std::endl;

// æ¨ç†
context->enqueueV2(buffers, stream, nullptr);

// æ˜¾å­˜åˆ°å†…å­˜
float prob[10];
cudaMemcpyAsync(prob, buffers[1], 1 * nOutputSize * sizeof(float), cudaMemcpyDeviceToHost, stream);

// åŒæ­¥ç»“æŸï¼Œé‡Šæ”¾èµ„æº
cudaStreamSynchronize(stream);
cudaStreamDestroy(stream);
```

### **è§£æè¾“å‡ºä¸æ‰“å°**

```javascript
// è§£æè¾“å‡º
std::cout << "image inference finished!" << std::endl;
Mat result = Mat(1, 10, CV_32F, (float*)prob);
float max = result.at<float>(0, 0);
int index = 0;
for (int i = 0; i < 10; i++)
{
    if (max < result.at<float>(0,i)) {
        max = result.at<float>(0, i);
        index = i;
    }
}
std::cout << "predict digit: " << index << std::end
```

è¿è¡Œç»“æœå¦‚ä¸‹ï¼š

![image-20210508090844643](imgs/image-20210508090844643.png)

## 2ã€TensorRT æºç ç®€ä»‹

http://zengzeyu.com/2020/07/09/tensorrt_02_introduction/

NVIDIA TensorRTæ˜¯ä¸€ç§é«˜æ€§èƒ½ç¥ç»ç½‘ç»œæ¨ç†(Inference)å¼•æ“ï¼Œç”¨äºåœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²æ·±åº¦å­¦ä¹ åº”ç”¨ç¨‹åºï¼Œåº”ç”¨æœ‰å›¾åƒåˆ†ç±»ã€åˆ†å‰²å’Œç›®æ ‡æ£€æµ‹ç­‰ï¼Œå¯æä¾›æœ€å¤§çš„æ¨ç†ååé‡å’Œæ•ˆç‡ã€‚TensorRTæ˜¯ç¬¬ä¸€æ¬¾å¯ç¼–ç¨‹æ¨ç†åŠ é€Ÿå™¨ï¼Œèƒ½åŠ é€Ÿç°æœ‰å’Œæœªæ¥çš„ç½‘ç»œæ¶æ„ã€‚

### TensorRT åº“æ„æˆ

ä»¥ç¼–è¯‘åæºç å‹ç¼©åŒ…å®‰è£…æ–¹å¼è¿›è¡Œå®‰è£…çš„TensorRTåº“ä¸»è¦æœ‰ä»¥ä¸‹æ–‡ä»¶å¤¹ï¼š

```
â”œâ”€â”€ data
â”œâ”€â”€ doc
â”œâ”€â”€ include  # æ‰€æœ‰å¤´æ–‡ä»¶ï¼Œå¯ä»¥æŸ¥çœ‹æ‰€æœ‰å‡½æ•°çš„æ¥å£å’Œè¯´æ˜
â”œâ”€â”€ lib      # æ‰€æœ‰åŠ¨æ€é“¾æ¥åº“.soæ–‡ä»¶
â”œâ”€â”€ python   # python API ä¾‹å­
â””â”€â”€ samples  # c++ ä¾‹å­
```

### includeæ–‡ä»¶å¤¹

`include`æ–‡ä»¶å¤¹å†…éƒ¨åŒ…å«æ–‡ä»¶å¦‚ä¸‹ï¼š

```
â”œâ”€â”€ include
â”‚   â”œâ”€â”€ NvCaffeParser.h
â”‚   â”œâ”€â”€ NvInfer.h
â”‚   â”œâ”€â”€ NvInferPlugin.h
â”‚   â”œâ”€â”€ NvInferPluginUtils.h
â”‚   â”œâ”€â”€ NvInferRuntimeCommon.h
â”‚   â”œâ”€â”€ NvInferRuntime.h
â”‚   â”œâ”€â”€ NvInferVersion.h
â”‚   â”œâ”€â”€ NvOnnxConfig.h
â”‚   â”œâ”€â”€ NvOnnxParser.h
â”‚   â”œâ”€â”€ NvOnnxParserRuntime.h
â”‚   â”œâ”€â”€ NvUffParser.h
â”‚   â””â”€â”€ NvUtils.h
```

ä¸‹é¢æŒ‰ç…§æ–‡ä»¶ä¹‹é—´ä¾èµ–å…³ç³»ï¼Œå¤§è‡´ä»‹ç»ä¸Šè¿°å¤´æ–‡ä»¶ã€‚

- `NvInferRuntimeCommon.h` ï¼šå®šä¹‰äº† TRT è¿è¡Œæ—¶æ‰€éœ€çš„åŸºç¡€æ•°æ®ç»“æ„ï¼ˆå¦‚`Dims`ï¼Œ`PluginField`ï¼‰å’Œå¤§éƒ¨åˆ†åŸºç±»ï¼ˆ`class ILogger`ï¼Œ `class IPluginV2`ï¼‰
- `NvInferRuntime.h`: ç»§æ‰¿`NvInferRuntimeCommon.h`ä¸­çš„åŸºç±»ï¼Œå®šä¹‰äº†runtimeæ—¶çš„æ‹“å±•åŠŸèƒ½å­ç±»
- `NvInfer.h`ï¼šç»§æ‰¿`NvInferRuntime.h`ä¸­çš„åŸºç±»ï¼Œå®šä¹‰äº†å¤§éƒ¨åˆ†å¯ä»¥ç›´æ¥åŠ å…¥çš„ç¥ç»ç½‘ç»œå±‚ï¼ˆå¦‚`class IConvolutionLayer`ï¼Œ`class IPoolingLayer`ï¼Œ`class IPoolingLayer`ï¼‰
- `NvInferPluginUtils.h`ï¼šå®šä¹‰`plugin layer`æ‰€éœ€çš„åŸºç¡€æ•°æ®ç»“æ„
- `NvInferPlugin.h`ï¼šåˆå§‹åŒ–æ³¨å†Œæ‰€æœ‰è‹¦è¡·åŒ…å«`plugin layer`
- å…¶ä»–çš„ä»æ–‡ä»¶åå³å¯çœ‹å‡ºæ—¶åˆ†åˆ«Caffeï¼ŒONNXï¼ŒUFF çš„è§£æå™¨parserï¼Œåˆ†åˆ«å¯¹åº”è®­ç»ƒåçš„Caffeï¼ŒPytorchï¼ŒTensorFlowç½‘ç»œ



## 3. TensorRT çš„ C++ API ä½¿ç”¨è¯¦è§£

2019-04-24 20:06:09 

https://blog.csdn.net/u010552731/article/details/89501819

### 1. TensorRT çš„ C++ API ä½¿ç”¨ç¤ºä¾‹

è¿›è¡Œæ¨ç†ï¼Œéœ€è¦å…ˆåˆ›å»ºIExecutionContextå¯¹è±¡ï¼Œè¦åˆ›å»ºè¿™ä¸ªå¯¹è±¡ï¼Œå°±éœ€è¦å…ˆåˆ›å»ºä¸€ä¸ªICudaEngineçš„å¯¹è±¡ï¼ˆengineï¼‰ã€‚



ä¸¤ç§åˆ›å»ºengineçš„æ–¹å¼ï¼š



1. ä½¿ç”¨æ¨¡å‹æ–‡ä»¶åˆ›å»ºengineï¼Œå¹¶å¯æŠŠåˆ›å»ºçš„engineåºåˆ—åŒ–åå­˜å‚¨åˆ°ç¡¬ç›˜ä»¥ä¾¿åé¢ç›´æ¥ä½¿ç”¨ï¼›
2. ä½¿ç”¨ä¹‹å‰å·²ç»åºåˆ—åŒ–å­˜å‚¨çš„engineï¼Œè¿™ç§æ–¹å¼æ¯”è¾ƒé«˜æ•ˆäº›ï¼Œå› ä¸ºè§£ææ¨¡å‹å¹¶ç”Ÿæˆengineè¿˜æ˜¯æŒºæ…¢çš„ã€‚

æ— è®ºå“ªç§æ–¹å¼ï¼Œéƒ½éœ€è¦åˆ›å»ºä¸€ä¸ªå…¨å±€çš„iLoggerå¯¹è±¡ï¼Œå¹¶è¢«ç”¨æ¥ä½œä¸ºå¾ˆå¤šTensorRT APIæ–¹æ³•çš„å‚æ•°ä½¿ç”¨ã€‚å¦‚ä¸‹æ˜¯ä¸€ä¸ªloggeråˆ›å»ºç¤ºä¾‹ï¼š

```
class Logger : public ILogger           
 {
     void log(Severity severity, const char* msg) override
     {
         // suppress info-level messages
         if (severity != Severity::kINFO)
             std::cout << msg << std::endl;
     }
 } gLogger;
```

### 2. ç”¨ C++ API åˆ›å»ºTensorRTç½‘ç»œ

#### 2.1 ä½¿ç”¨ C++ çš„ parser API å¯¼å…¥æ¨¡å‹

**1.åˆ›å»ºTensorRT builderå’Œnetwork**

```
IBuilder* builder = createInferBuilder(gLogger);
nvinfer1::INetworkDefinition* network = builder->createNetwork();
```

**2. é’ˆå¯¹ç‰¹å®šæ ¼å¼åˆ›å»ºTensorRT parser**

```
// ONNX
auto parser = nvonnxparser::createParser(*network,
        gLogger);
// UFF
auto parser = createUffParser();
// NVCaffe
ICaffeParser* parser = createCaffeParser();
```

**3.ä½¿ç”¨parserè§£æå¯¼å…¥çš„æ¨¡å‹å¹¶å¡«å……network**

`parser->parse(args);`



å…·ä½“çš„argsè¦çœ‹ä½¿ç”¨ä»€ä¹ˆæ ¼å¼çš„parserã€‚

å¿…é¡»åœ¨ç½‘ç»œä¹‹å‰åˆ›å»ºæ„å»ºå™¨ï¼Œå› ä¸ºå®ƒå……å½“ç½‘ç»œçš„å·¥å‚ã€‚ ä¸åŒçš„è§£æå™¨åœ¨æ ‡è®°ç½‘ç»œè¾“å‡ºæ—¶æœ‰ä¸åŒçš„æœºåˆ¶ã€‚

#### 2.2 ä½¿ç”¨ C++ Parser API å¯¼å…¥ Caffe æ¨¡å‹

**1.åˆ›å»ºbuilderå’Œnetwork**

```
IBuilder* builder = createInferBuilder(gLogger);
INetworkDefinition* network = builder->createNetwork();
```



2.åˆ›å»ºcaffe parser

`ICaffeParser* parser = createCaffeParser();`



3.è§£æå¯¼å…¥çš„æ¨¡å‹

```
const IBlobNameToTensor* blobNameToTensor = parser->parse("deploy_file", 
              "modelFile", 
              *network, 
              DataType::kFLOAT);
```


è¿™å°†æŠŠCaffeæ¨¡å‹å¡«å……åˆ°TensorRTçš„networkã€‚æœ€åä¸€ä¸ªå‚æ•°æŒ‡ç¤ºè§£æå™¨ç”Ÿæˆæƒé‡ä¸º32ä½æµ®ç‚¹æ•°çš„ç½‘ç»œã€‚ä½¿ç”¨DataType :: kHALFå°†ç”Ÿæˆå…·æœ‰16ä½æƒé‡çš„æ¨¡å‹ã€‚

é™¤äº†å¡«å……ç½‘ç»œå®šä¹‰ä¹‹å¤–ï¼Œparserè¿˜è¿”å›ä¸€ä¸ªå­—å…¸ï¼Œè¯¥å­—å…¸æ˜¯ä» Caffe çš„ blob names åˆ° TensorRT çš„ tensors çš„æ˜ å°„ã€‚ä¸Caffeä¸åŒï¼ŒTensorRTç½‘ç»œå®šä¹‰æ²¡æœ‰in-placeçš„æ¦‚å¿µã€‚å½“Caffeæ¨¡å‹ä½¿ç”¨in-placeæ“ä½œæ—¶ï¼Œå­—å…¸ä¸­è¿”å›çš„ç›¸åº”çš„TensorRT tensorsæ˜¯å¯¹é‚£ä¸ªblobçš„æœ€åä¸€æ¬¡å†™å…¥ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ˜¯ä¸€ä¸ªå·ç§¯å†™å…¥åˆ°äº†blobå¹¶ä¸”åé¢è·Ÿçš„æ˜¯ReLUï¼Œåˆ™è¯¥blobçš„åå­—æ˜ å°„åˆ°TensorRT tensorså°±æ˜¯ReLUçš„è¾“å‡ºã€‚



4.ç»™networkåˆ†é…è¾“å‡º

```
for (auto& s : outputs)
    network->markOutput(*blobNameToTensor->find(s.c_str()));
```



#### 2.3. ä½¿ç”¨ C++ Parser API å¯¼å…¥ TensorFlow æ¨¡å‹

å¯¹äºä¸€ä¸ªæ–°çš„å·¥ç¨‹ï¼Œæ¨èä½¿ç”¨é›†æˆçš„TensorFlow-TensorRTä½œä¸ºè½¬æ¢TensorFlow networkåˆ°TensorRTçš„æ–¹æ³•æ¥è¿›è¡Œæ¨ç†ã€‚å…·ä½“å¯å‚è€ƒï¼šIntegrating TensorFlow With TensorRT

ä»TensorFlowæ¡†æ¶å¯¼å…¥ï¼Œéœ€è¦å…ˆå°†TensorFlowæ¨¡å‹è½¬æ¢ä¸ºä¸­é—´æ ¼å¼ï¼šUFFï¼ˆUniversal Framework Formatï¼‰ã€‚ç›¸å…³è½¬æ¢å¯å‚è€ƒCoverting A Frozen Graph to UFF

æ›´å¤šå…³äºUFFå¯¼å…¥çš„ä¿¡æ¯å¯ä»¥å‚è€ƒï¼šhttps://docs.nvidia.com/deeplearning/sdk/tensorrt-sample-support-guide/index.html#mnist_uff_sample

å…ˆæ¥çœ‹ä¸‹å¦‚ä½•ç”¨C++ Parser APIæ¥å¯¼å…¥TensorFlowæ¨¡å‹ã€‚



1.åˆ›å»ºbuilderå’Œnetwork

```
IBuilder* builder = createInferBuilder(gLogger);
INetworkDefinition* network = builder->createNetwork();
```



2.åˆ›å»ºUFF parser

`IUFFParser* parser = createUffParser();`

3.å‘UFF parserå£°æ˜networkçš„è¾“å…¥å’Œè¾“å‡º

```
parser->registerInput("Input_0", DimsCHW(1, 28, 28), UffInputOrder::kNCHW);
parser->registerOutput("Binary_3");
```


æ³¨æ„ï¼šTensorRTæœŸæœ›çš„è¾“å…¥tensoræ˜¯CHWé¡ºåºçš„ã€‚ä»TensorFlowå¯¼å…¥æ—¶åŠ¡å¿…ç¡®ä¿è¿™ä¸€ç‚¹ï¼Œå¦‚ä¸æ˜¯CHWï¼Œé‚£å°±å…ˆè½¬æ¢æˆCHWé¡ºåºçš„ã€‚





4.è§£æå·²å¯¼å…¥çš„æ¨¡å‹åˆ°network

`parser->parse(uffFile, *network, nvinfer1::DataType::kFLOAT);`


#### 2.4. ä½¿ç”¨ C++ Parser API å¯¼å…¥ ONNX æ¨¡å‹

ä½¿ç”¨é™åˆ¶ï¼šæ³¨æ„ç‰ˆæœ¬é—®é¢˜ï¼ŒTensorRT5.1 é™„å¸¦çš„ ONNX Parseræ”¯æŒçš„ONNX IRç‰ˆæœ¬æ˜¯0.0.3ï¼Œopsetç‰ˆæœ¬æ˜¯9ã€‚é€šå¸¸ï¼Œè¾ƒæ–°çš„ONNX parseræ˜¯åå‘å…¼å®¹çš„ã€‚æ›´å¤šä¿¡æ¯å¯å‚è€ƒï¼šONNX Model Opset Version Converter å’Œ onnx-tensorrtã€‚

æ›´å¤šå…³äºONNXå¯¼å…¥çš„ä¿¡æ¯ä¹Ÿå¯å‚è€ƒï¼š
https://docs.nvidia.com/deeplearning/sdk/tensorrt-sample-support-guide/index.html#onnx_mnist_sample



1.åˆ›å»ºONNX parserï¼Œparserä½¿ç”¨è¾…åŠ©é…ç½®ç®¡ç†SampleConfigå¯¹è±¡å°†è¾“å…¥å‚æ•°ä»ç¤ºä¾‹çš„å¯æ‰§è¡Œæ–‡ä»¶ä¼ é€’åˆ°parserå¯¹è±¡

```
nvonnxparser::IOnnxConfig* config = nvonnxparser::createONNXConfig();
//Create Parser
nvonnxparser::IONNXParser* parser = nvonnxparser::createONNXParser(*config);
```



2.å¡«å……æ¨¡å‹

`parser->parse(onnx_filename, DataType::kFLOAT);`

3.è½¬æ¢æ¨¡å‹åˆ°TensorRTçš„network

`parser->convertToTRTNetwork();`

4.ä»æ¨¡å‹è·å–network

`nvinfer1::INetworkDefinition* trtNetwork = parser->getTRTNetwork();`

### 3. ç”¨ C++ API æ„å»º engine

ä¸‹ä¸€æ­¥æ˜¯è°ƒç”¨TensorRTçš„builderæ¥åˆ›å»ºä¼˜åŒ–çš„runtimeã€‚ builderçš„å…¶ä¸­ä¸€ä¸ªåŠŸèƒ½æ˜¯æœç´¢å…¶CUDAå†…æ ¸ç›®å½•ä»¥è·å¾—æœ€å¿«çš„å®ç°ï¼Œå› æ­¤ç”¨æ¥æ„å»ºä¼˜åŒ–çš„engineçš„GPUè®¾å¤‡å’Œå®é™…è·‘çš„GPUè®¾å¤‡ä¸€å®šè¦æ˜¯ç›¸åŒçš„æ‰è¡Œã€‚

builderå…·æœ‰è®¸å¤šå±æ€§ï¼Œå¯ä»¥é€šè¿‡è®¾ç½®è¿™äº›å±æ€§æ¥æ§åˆ¶ç½‘ç»œè¿è¡Œçš„ç²¾åº¦ï¼Œä»¥åŠè‡ªåŠ¨è°ƒæ•´å‚æ•°ã€‚è¿˜å¯ä»¥æŸ¥è¯¢builderä»¥æ‰¾å‡ºç¡¬ä»¶æœ¬èº«æ”¯æŒçš„é™ä½çš„ç²¾åº¦ç±»å‹ã€‚

æœ‰ä¸¤ä¸ªç‰¹åˆ«é‡è¦çš„å±æ€§ï¼šæœ€å¤§batch sizeå’Œæœ€å¤§workspace sizeã€‚

- æœ€å¤§batch sizeæŒ‡å®šTensorRTå°†è¦ä¼˜åŒ–çš„batchå¤§å°ã€‚åœ¨è¿è¡Œæ—¶ï¼Œåªèƒ½é€‰æ‹©æ¯”è¿™ä¸ªå€¼å°çš„batchã€‚
- å„ç§layerç®—æ³•é€šå¸¸éœ€è¦ä¸´æ—¶å·¥ä½œç©ºé—´ã€‚è¿™ä¸ªå‚æ•°é™åˆ¶äº†ç½‘ç»œä¸­æ‰€æœ‰çš„å±‚å¯ä»¥ä½¿ç”¨çš„æœ€å¤§çš„workspaceç©ºé—´å¤§å°ã€‚ å¦‚æœåˆ†é…çš„ç©ºé—´ä¸è¶³ï¼ŒTensorRTå¯èƒ½æ— æ³•æ‰¾åˆ°ç»™å®šå±‚çš„å®ç°ã€‚



1.ç”¨builderå¯¹è±¡åˆ›å»ºæ„å»ºengine

```
builder->setMaxBatchSize(maxBatchSize);
builder->setMaxWorkspaceSize(1 << 20);
ICudaEngine* engine = builder->buildCudaEngine(*network);
```



2.ç”¨å®Œåˆ†é…è¿‡çš„networkï¼Œbuilderå’Œparserè®°å¾—è§£æ

```
parser->destroy();
network->destroy();
builder->destroy();
```



### 4.ç”¨ C++ API åºåˆ—åŒ–ä¸€ä¸ªæ¨¡å‹

åºåˆ—åŒ–æ¨¡å‹ï¼Œå³æŠŠengineè½¬æ¢ä¸ºå¯å­˜å‚¨çš„æ ¼å¼ä»¥å¤‡åç”¨ã€‚æ¨ç†æ—¶ï¼Œå†ç®€å•çš„ååºåˆ—åŒ–ä¸€ä¸‹è¿™ä¸ªengineå³å¯ç›´æ¥ç”¨æ¥åšæ¨ç†ã€‚é€šå¸¸åˆ›å»ºä¸€ä¸ªengineè¿˜æ˜¯æ¯”è¾ƒèŠ±æ—¶é—´çš„ï¼Œå¯ä»¥ä½¿ç”¨è¿™ç§åºåˆ—åŒ–çš„æ–¹æ³•é¿å…æ¯æ¬¡é‡æ–°åˆ›å»ºengineã€‚

æ³¨æ„ï¼šåºåˆ—åŒ–çš„engineä¸èƒ½è·¨å¹³å°æˆ–åœ¨ä¸åŒç‰ˆæœ¬çš„TensorRTé—´ç§»æ¤ä½¿ç”¨ã€‚å› ä¸ºå…¶ç”Ÿæˆæ˜¯æ ¹æ®ç‰¹å®šç‰ˆæœ¬çš„TensorRTå’ŒGPUçš„ã€‚

1.åºåˆ—åŒ–

```
IHostMemory *serializedModel = engine->serialize();
// store model to disk
// <â€¦>
serializedModel->destroy();
```



2.åˆ›å»ºä¸€ä¸ªruntimeå¹¶ç”¨æ¥ååºåˆ—åŒ–

```
IRuntime* runtime = createInferRuntime(gLogger);
ICudaEngine* engine = runtime->deserializeCudaEngine(modelData, modelSize, nullptr);
```



### 5.ç”¨ C++ API æ‰§è¡Œæ¨ç†

1.åˆ›å»ºä¸€ä¸ªContextç”¨æ¥å­˜å‚¨ä¸­é—´æ¿€æ´»å€¼

`IExecutionContext *context = engine->createExecutionContext();`

ä¸€ä¸ªengineå¯ä»¥æœ‰å¤šä¸ªexecution contextï¼Œå¹¶å…è®¸å°†åŒä¸€å¥—weightsç”¨äºå¤šä¸ªæ¨ç†ä»»åŠ¡ã€‚å¯ä»¥åœ¨å¹¶è¡Œçš„CUDA streamsæµä¸­æŒ‰æ¯ä¸ªstreamæµä¸€ä¸ªengineå’Œä¸€ä¸ªcontextæ¥å¤„ç†å›¾åƒã€‚æ¯ä¸ªcontextåœ¨engineç›¸åŒçš„GPUä¸Šåˆ›å»ºã€‚



2.ç”¨inputå’Œoutputçš„blobåå­—è·å–å¯¹åº”çš„inputå’Œoutputçš„index

`int inputIndex = engine.getBindingIndex(INPUT_BLOB_NAME);`

`int outputIndex = engine.getBindingIndex(OUTPUT_BLOB_NAME);`

3.ä½¿ç”¨ä¸Šé¢çš„indicesï¼Œåœ¨GPUä¸Šåˆ›å»ºä¸€ä¸ªæŒ‡å‘inputå’Œoutputç¼“å†²åŒºçš„bufferæ•°ç»„

```
void* buffers[2];
buffers[inputIndex] = inputbuffer;
buffers[outputIndex] = outputBuffer;
```



4.é€šå¸¸TensorRTçš„æ‰§è¡Œæ˜¯å¼‚æ­¥çš„ï¼Œå› æ­¤å°†kernelsåŠ å…¥é˜Ÿåˆ—æ”¾åœ¨CUDA streamæµä¸Š

`context.enqueue(batchSize, buffers, stream, nullptr);`

é€šå¸¸åœ¨kernelsä¹‹å‰å’Œä¹‹åæ¥enququeå¼‚æ­¥memcpy()ä»¥ä»GPUç§»åŠ¨æ•°æ®ï¼ˆå¦‚æœå°šæœªå­˜åœ¨ï¼‰ã€‚

enqueue()çš„æœ€åä¸€ä¸ªå‚æ•°æ˜¯ä¸€ä¸ªå¯é€‰çš„CUDAäº‹ä»¶ï¼Œå½“è¾“å…¥ç¼“å†²åŒºè¢«æ¶ˆè€—ä¸”å®ƒä»¬çš„å†…å­˜å¯ä»¥å®‰å…¨åœ°é‡ç”¨æ—¶è¿™ä¸ªäº‹ä»¶ä¾¿ä¼šè¢«ä¿¡å·è§¦å‘ã€‚

ä¸ºäº†ç¡®å®škernelsï¼ˆæˆ–å¯èƒ½å­˜åœ¨çš„memcpy()ï¼‰ä½•æ—¶å®Œæˆï¼Œè¯·ä½¿ç”¨æ ‡å‡†CUDAåŒæ­¥æœºåˆ¶ï¼ˆå¦‚äº‹ä»¶ï¼‰æˆ–ç­‰å¾…æµã€‚



### 6.C++ API çš„å†…å­˜ç®¡ç†

TensorRTæä¾›äº†ä¸¤ç§æœºåˆ¶æ¥å…è®¸åº”ç”¨ç¨‹åºå¯¹è®¾å¤‡å†…å­˜è¿›è¡Œæ›´å¤šçš„æ§åˆ¶ã€‚

é»˜è®¤æƒ…å†µä¸‹ï¼Œåœ¨åˆ›å»ºIExecutionContextæ—¶ï¼Œä¼šåˆ†é…æŒä¹…è®¾å¤‡å†…å­˜æ¥ä¿å­˜æ¿€æ´»æ•°æ®ã€‚ä¸ºé¿å…è¿™ä¸ªåˆ†é…ï¼Œè¯·è°ƒç”¨createExecutionContextWithoutDeviceMemoryã€‚ç„¶ååº”ç”¨ç¨‹åºä¼šè°ƒç”¨IExecutionContext :: setDeviceMemory()æ¥æä¾›è¿è¡Œç½‘ç»œæ‰€éœ€çš„å†…å­˜ã€‚å†…å­˜å—çš„å¤§å°ç”±ICudaEngine :: getDeviceMemorySize()è¿”å›ã€‚

æ­¤å¤–ï¼Œåº”ç”¨ç¨‹åºå¯ä»¥é€šè¿‡å®ç°IGpuAllocatoræ¥å£æ¥æä¾›åœ¨æ„å»ºå’Œè¿è¡Œæ—¶ä½¿ç”¨çš„è‡ªå®šä¹‰åˆ†é…å™¨ã€‚å®ç°æ¥å£åï¼Œè¯·è°ƒç”¨ï¼š

`setGpuAllocator(&allocator);`

åœ¨IBuilderæˆ–IRuntimeæ¥å£ä¸Šã€‚æ‰€æœ‰çš„è®¾å¤‡å†…å­˜éƒ½å°†é€šè¿‡è¿™ä¸ªæ¥å£æ¥åˆ†é…å’Œé‡Šæ”¾ã€‚

### 7.è°ƒæ•´engine

TensorRTå¯ä»¥ä¸ºä¸€ä¸ªengineè£…å¡«æ–°çš„weightsï¼Œè€Œä¸ç”¨é‡æ–°buildã€‚

1.åœ¨buildä¹‹å‰ç”³è¯·ä¸€ä¸ªå¯refittableçš„engine

```
...
builder->setRefittable(true); 
builder->buildCudaEngine(network);
```



2.åˆ›å»ºä¸€ä¸ªrefitterå¯¹è±¡

`ICudaEngine* engine = ...;`

`IRefitter* refitter = createInferRefitter(*engine,gLogger)`

3.æ›´æ–°ä½ æƒ³æ›´æ–°çš„weights
å¦‚ï¼šä¸ºä¸€ä¸ªå«MyLayerçš„å·ç§¯å±‚æ›´æ–°kernel weights

`Weights newWeights = ...;`

`refitter.setWeights("MyLayer",WeightsRole::kKERNEL,
                    newWeights);`


è¿™ä¸ªæ–°çš„weightsè¦å’ŒåŸå§‹ç”¨æ¥build engineçš„weightså…·æœ‰ç›¸åŒçš„æ•°é‡ã€‚
setWeightså‡ºé”™æ—¶ä¼šè¿”å›falseã€‚



4.æ‰¾å‡ºå“ªäº›weightséœ€è¦æä¾›ã€‚
è¿™é€šå¸¸éœ€è¦è°ƒç”¨ä¸¤æ¬¡IRefitter::getMissingï¼Œç¬¬ä¸€æ¬¡è°ƒç”¨å¾—åˆ°Weightså¯¹è±¡çš„æ•°ç›®ï¼Œç¬¬äºŒæ¬¡å¾—åˆ°ä»–ä»¬çš„layerså’Œrolesã€‚

```
const int n = refitter->getMissing(0, nullptr, nullptr);
std::vector<const char*> layerNames(n);
std::vector<WeightsRole> weightsRoles(n);
refitter->getMissing(n, layerNames.data(), 
                        weightsRoles.data());
```



5.æä¾›missingçš„weightsï¼ˆé¡ºåºæ— æ‰€è°“ï¼‰

```
for (int i = 0; i < n; ++i)
    refitter->setWeights(layerNames[i], weightsRoles[i],
                         Weights{...});
```


åªéœ€æä¾›missingçš„weightså³å¯ï¼Œå¦‚æœæä¾›äº†é¢å¤–çš„weightså¯èƒ½ä¼šè§¦å‘æ›´å¤šweightsçš„éœ€è¦ã€‚



6.æ›´æ–°engine

```
bool success = refitter->refitCudaEngine();
assert(success);
```


å¦‚æœsuccessçš„å€¼ä¸ºfalseï¼Œå¯ä»¥æ£€æŸ¥ä¸€ä¸‹diagnostic logï¼Œä¹Ÿè®¸æœ‰äº›weightsè¿˜æ˜¯missingçš„ã€‚



7.é”€æ¯refitter

`refitter->destroy();`
å¦‚æœæƒ³è¦æŸ¥çœ‹engineä¸­æ‰€æœ‰å¯é‡æ–°è°ƒæ•´çš„æƒé‡ï¼Œå¯ä»¥ä½¿ç”¨refitter-> getAll(...)ï¼Œç±»ä¼¼äºæ­¥éª¤4ä¸­çš„å¦‚ä½•ä½¿ç”¨getMissingã€‚

## 4. å¦‚ä½•æä¾›æ€§èƒ½

http://yeahflash.com/programming/tensorrt/tensorrt_tutorials_5.html#_3-2-%E5%A6%82%E4%BD%95%E6%8F%90%E9%AB%98tensorrt%E6%80%A7%E8%83%BD

### 3.1. å¦‚ä½•è¯„ä¼°æ€§èƒ½

- æŒ‡æ ‡
  - Latencyï¼Œinferenceæ—¶é—´ï¼Œ
  - Throughoutï¼Œååé‡ï¼Œå›ºå®šæ—¶é—´å†…æ‰§è¡Œäº†å¤šå°‘æ¬¡æ¨ç†
- å¦‚ä½•é€‰æ‹©æ—¶é—´ç‚¹ï¼ˆæ€§èƒ½éƒ½æœ‰æ—¶é—´ï¼Œæ—¶é—´çš„èµ·æ­¢æ—¶é—´ç‚¹é€‰æ‹©éå¸¸é‡è¦ï¼‰
  - æ•´ä½“ç³»ç»Ÿçš„æ€§èƒ½ä¸€èˆ¬ä¼šè®¡ç®—æ‰€æœ‰æ—¶é—´ï¼ˆåŒ…æ‹¬æ•°æ®é¢„å¤„ç†ä¸åå¤„ç†ç­‰ï¼‰
  - ä½†ä¸åŒä»»åŠ¡çš„æ•°æ®é¢„å¤„ç†ã€åå¤„ç†ç­‰æ—¶é—´å·®è·å¤ªå¤§ï¼Œæ‰€ä»¥æœ¬æ–‡åªè€ƒè™‘æ¨¡å‹æ¨ç†æ—¶é—´ã€‚
  - å¦å¤–ä¸€ç§æµ‹è¯•æ–¹å¼æ˜¯ï¼Œç¡®å®šlatencyçš„æœ€å¤§å€¼ï¼ˆçŒœæµ‹è¶…è¿‡æœ€å¤§å€¼å°±ç»“æŸæœ¬æ¬¡æ¨ç†ï¼‰ï¼Œè®¡ç®—å›ºå®šæ—¶é—´å†…çš„inferenceæ¬¡æ•°ã€‚è¿™ç§æ–¹æ³•æ˜¯ `quality-of-service measurement`ï¼Œå¯ä»¥å¾ˆå¥½çš„æ¯”è¾ƒç”¨æˆ·ä½“éªŒä¸ç³»ç»Ÿæ€§èƒ½ã€‚
- å·¥å…·ï¼š
  - `trtexec`æä¾›äº†ç›¸å…³å·¥å…·
  - ä¹Ÿå¯ä»¥é€šè¿‡NVIDIA Triton Inference Serveræ¥æµ‹è¯•å¹¶è¡Œæ¨ç†æ€§èƒ½ã€‚
- CPUæ—¶é—´æµ‹è¯•ï¼Œæœ‰ä¸€æ®µæµ‹è¯•ä»£ç 
- CUDA Eventsï¼šç”±äºå­˜åœ¨ host/device åŒæ­¥é—®é¢˜ï¼Œä¸èƒ½é€šè¿‡ç›´æ¥çš„æ–¹æ³•è·å–æ—¶é—´ã€‚è¿™å¯ä»¥é€šè¿‡CUDA Eventsæ¥å®ç°
- TensorRTå†…ç½®Profile
- CUDA Profilingï¼šè¯´æœ‰ä¿©å·¥å…·[NVIDIA Nsight Compute (opens new window)](https://developer.nvidia.com/nsight-compute)å’Œ[NVIDIA Nsight Systems (opens new window)](https://developer.nvidia.com/nsight-systems)ï¼Œæ²¡ç»†çœ‹ã€‚
- å†…å­˜ï¼šè¯´æ˜¯é€šè¿‡a simple custom GPU allocatoræ¥ç›‘æ§ï¼Œä½†ä¹Ÿæ²¡ç»†è¯´ã€‚

### 3.2. å¦‚ä½•æé«˜TensorRTæ€§èƒ½

- è¯´ç™½äº†ï¼Œå°±æ˜¯ä¸ºäº†æé«˜æ€§èƒ½ï¼ŒTensorRTæä¾›äº†å“ªäº›åŠŸèƒ½
  - æ–‡æ¡£é‡Œè¿˜è¯´äº†ä¸€å¥ï¼ŒCUDAç¨‹åºå‘˜çœ‹è¿™äº›å¾ˆåˆé€‚ï¼Œå…¶ä»–èœé¸¡ï¼ˆæ¯”å¦‚æˆ‘ï¼‰å¯èƒ½å°±çœ‹ä¸æ‡‚äº†
- Mixed Precision
  - æ··åˆç²¾åº¦ï¼Œä¹Ÿå°±æ˜¯æƒé‡çš„æ•°æ®ç±»å‹ï¼Œæ”¯æŒFP32/FP16/INT8
  - é»˜è®¤æ˜¯FP32ï¼Œå¦‚æœé€‰æ‹©FP16æ¨¡å¼åˆ™ä½¿ç”¨FP16æˆ–FP32ï¼Œå¦‚æœé€‰æ‹©INT8æ¨¡å¼åˆ™ä½¿ç”¨INT8æˆ–FP32
  - ä¸ºäº†è·å¾—æ›´å¥½çš„æ€§èƒ½ï¼Œå¯ä»¥åŒæ—¶æŒ‡å®šFP16å’ŒINT8æ¨¡å¼ï¼Œä¸‰ç§ç²¾åº¦æ··ç”¨ã€‚
  - è¿˜å¯ä»¥ä½¿ç”¨`trtexec`ä¸­çš„`--best`é€‰é¡¹
- Batching
  - ä¸€ä¸ªBatchå°±æ˜¯ä¸€ç»„è¾“å…¥ã€‚
  - è¿™ç§ç­–ç•¥å¢åŠ äº†æ¯ä¸ªè¾“å…¥çš„latencyï¼Œä½†æé«˜äº†æ€»ä½“ååé‡
- Streaming
  - CUDA ä¸­çš„streamsæ˜¯å¤„ç†åŒæ­¥ä»»åŠ¡çš„ä¸€ç§æ–¹å¼ï¼Œå³åŒæ­¥å‘½ä»¤æ”¾åˆ°ä¸€ä¸ªstreamä¸­ï¼Œå…¶ä¸­çš„å‘½ä»¤ä¼šæŒ‰é¡ºåºä¾æ¬¡æ‰§è¡Œã€‚åŒä¸€ä¸ªstreamä¸­æ˜¯ç¡®å®šåŒæ­¥æ‰§è¡Œçš„ï¼Œå¤šä¸ªstreamsåˆ™æ˜¯å¼‚æ­¥çš„ã€‚
  - ä½¿ç”¨å¤šä¸ªstreamæé«˜å¹¶è¡Œåº¦ä»è€Œæé«˜æ€§èƒ½ã€‚
  - ä½¿ç”¨æµç¨‹å¤§æ¦‚æ˜¯ï¼š
    - Identify the batches of inferences that are independent.
    - Create a single engine for the network.
    - Create a CUDA stream using cudaStreamCreate for each independent batch and an IExecutionContext for each independent batch.
    - Launch inference work by requesting asynchronous results using IExecutionContext::enqueue from the appropriate IExecutionContext and passing in the appropriate stream.
    - After all the work has been launched, synchronize with all the streams to wait for results. The execution contexts and streams can be reused for later batches of independent work.
- Thread Safety
  - ä¸€ä¸ªTesnorRT builderåªèƒ½è¢«ä¸€ä¸ªçº¿ç¨‹ä½¿ç”¨ï¼Œå¦‚æœè¦å¤šçº¿ç¨‹å°±éœ€è¦åˆ›å»ºå¤šä¸ªbuilder
  - åªè¦æ¯ä¸ªobjectä½¿ç”¨ä¸åŒçš„execution contextï¼Œé‚£ä¹ˆTensorRT runtimeå°±å¯ä»¥è¢«å¤šä¸ªçº¿ç¨‹åŒæ—¶ä½¿ç”¨
- Initializing The Engine
  - åˆå§‹åŒ–Engineçš„æ—¶å€™ä¼šè¿›è¡Œå¾ˆå¤šä¼˜åŒ–ï¼Œå¹¶è¿›è¡Œæµ‹è¯•
- Enabling Fusionï¼šä½¿ç”¨Fusionæ“ä½œï¼Œå…¶å®å°±æ˜¯åˆå¹¶æ“ä½œ

## 5.æ¡ˆä¾‹

ä»£ç ï¼šhttps://github.com/FelixFu-TD/Win10_TensorRT_Pytorch_ONNX

åšå®¢ï¼šhttps://arleyzhang.github.io/articles/7f4b25ce/

