# Design and Architecture[ğŸ”—](https://tvm.apache.org/docs/dev/index.html#example-compilation-flow)ï¼ˆTVMæ¶æ„å’Œè®¾è®¡ï¼‰

âŒšï¸:2020å¹´11æœˆ30æ—¥

ğŸ“šå‚è€ƒ

---

This document is intended for developers who want to understand the architecture of TVM and/or actively develop on the project. This page is organized as follows:

æœ¬æ–‡æ¡£é€‚ç”¨äºå¸Œæœ›äº†è§£TVMä½“ç³»ç»“æ„å’Œ/æˆ–åœ¨é¡¹ç›®ä¸Šè¿›è¡Œç§¯æå¼€å‘çš„å¼€å‘äººå‘˜ã€‚è¯¥é¡µé¢çš„ç»„ç»‡å¦‚ä¸‹ï¼š



- The [Example Compilation Flow](https://tvm.apache.org/docs/dev/index.html#example-compilation-flow) gives an overview of the steps that TVM takes to turn a high level description of a model into a deployable module. To get started, please read this section first.
- ç¤ºä¾‹ç¼–è¯‘æµç¨‹  æ¦‚è¿°äº†TVMå°†æ¨¡å‹çš„é«˜çº§æ¦‚å¿µè½¬æ¢ä¸ºå¯éƒ¨ç½²æ¨¡å—çš„æ­¥éª¤ã€‚é¦–å…ˆï¼Œè¯·å…ˆé˜…è¯»æœ¬èŠ‚ã€‚
- The [Logical Architecture Components](https://tvm.apache.org/docs/dev/index.html#logical-architecture-components) section describes the logical components. The sections after are specific guides focused on each logical component, organized by the componentâ€™s name.
- é€»è¾‘æ¶æ„ç»„ä»¶éƒ¨åˆ† æè¿°é€»è¾‘ç»„ä»¶ã€‚é’ˆå¯¹æ¯ä¸ªé€»è¾‘ç»„ä»¶ï¼ŒæŒ‰ç»„ä»¶çš„åç§°è¿›è¡Œç»„ç»‡ã€‚
- Feel free to also checkout the [Developer How-To Guide](https://tvm.apache.org/docs/dev/how_to.html#dev-how-to) for useful development tips.
- ä¹Ÿå¯ä»¥éšæ—¶æŸ¥çœ‹å¼€å‘äººå‘˜å¦‚ä½•æŒ‡å¯¼æœ‰ç”¨çš„å¼€å‘æŠ€å·§



This guide provides a few complementary views of the architecture. First, we review a single end-to-end compilation flow and discuss the key data structures and the transformations. This runtime-based view focuses on the interactions of each components when running the compiler. Then we will review the logical modules of the codebase and their relationship. This part provides a static overarching view of the design.

æœ¬æŒ‡å—æä¾›äº†ä¸€äº›ä½“ç³»ç»“æ„çš„è¡¥å……è§†å›¾ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å›é¡¾ä¸€ä¸ªç«¯åˆ°ç«¯çš„ç¼–è¯‘æµç¨‹ï¼Œå¹¶è®¨è®ºå…³é”®çš„æ•°æ®ç»“æ„å’Œè½¬æ¢ï¼ˆkey data structures and the transformationsï¼‰ã€‚è¿™ä¸ªåŸºäºè¿è¡Œæ—¶çš„è§†å›¾ï¼ˆruntime-based viewï¼‰ç€é‡äºè¿è¡Œç¼–è¯‘å™¨æ—¶æ¯ä¸ªç»„ä»¶çš„äº¤äº’ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†å›é¡¾ä»£ç åº“çš„é€»è¾‘æ¨¡å—åŠå…¶å…³ç³»ã€‚è¿™éƒ¨åˆ†æä¾›äº†è®¾è®¡çš„é™æ€æ€»ä½“è§†å›¾ã€‚

## 1. Example Compilation Flowï¼ˆç¼–è¯‘æµç¨‹ç¤ºä¾‹ï¼‰

In this guide, we will study an example compilation flow in the compiler. The figure below shows the flow. At a high-level, it contains several steps:

åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†ç ”ç©¶ç¼–è¯‘å™¨ä¸­çš„ç¤ºä¾‹ç¼–è¯‘æµç¨‹ã€‚ä¸‹å›¾æ˜¾ç¤ºäº†æµç¨‹ã€‚ä»é«˜å±‚æ¬¡ä¸Šè®²ï¼Œå®ƒåŒ…å«å‡ ä¸ªæ­¥éª¤ï¼š

- Import: The frontend component ingests a model into an IRModule, which contains a collection of functions that internally represent the model.
- å¯¼å…¥ï¼šå‰ç«¯ç»„ä»¶å°†æ¨¡å‹æå–åˆ°IRModuleä¸­ï¼Œè¯¥æ¨¡å—åŒ…å«å†…éƒ¨è¡¨ç¤ºæ¨¡å‹çš„å‡½æ•°çš„é›†åˆã€‚



- Transformation: The compiler transforms an IRModule to another functionally equivalent or approximately equivalent(e.g. in the case of quantization) IRModule. Many of the transformatons are target (backend) independent. We also allow target to affect the configuration of the transformation pipeline.
- è½¬æ¢ï¼šç¼–è¯‘å™¨å°†IRModuleè½¬æ¢ä¸ºå¦ä¸€ä¸ªåŠŸèƒ½ä¸Šç­‰æ•ˆæˆ–è¿‘ä¼¼ç­‰æ•ˆçš„ï¼ˆä¾‹å¦‚ï¼Œåœ¨é‡åŒ–çš„æƒ…å†µä¸‹ï¼‰IRModuleã€‚è®¸å¤šè½¬æ¢éƒ½æ˜¯ç‹¬ç«‹äºç›®æ ‡ï¼ˆåç«¯ï¼‰çš„ã€‚æˆ‘ä»¬è¿˜å…è®¸ç›®æ ‡å½±å“è½¬æ¢ç®¡é“çš„é…ç½®ã€‚



- Target Translation: The compiler translates(codegen) the IRModule to an executable format specified by the target. The target translation result is encapsulated as a runtime.Module that can be exported, loaded, and executed on the target runtime environment.
- ç›®æ ‡ç¿»è¯‘ï¼šç¼–è¯‘å™¨å°†IRModuleè½¬æ¢ï¼ˆä»£ç ç”Ÿæˆï¼‰ä¸ºç›®æ ‡æŒ‡å®šçš„å¯æ‰§è¡Œæ ¼å¼ã€‚ç›®æ ‡ç¿»è¯‘ç»“æœè¢«å°è£…ä¸ºä¸€ä¸ªruntime.Moduleï¼Œmoduleå¯ä»¥å¯¼å‡ºï¼ŒåŠ è½½å’Œæ‰§è¡Œå¯¹ç›®æ ‡è¿è¡Œæ—¶ç¯å¢ƒã€‚



- Runtime Execution: the user loads back a runtime.Module and runs the compiled functions in the supported runtime environment.

- è¿è¡Œæ—¶æ‰§è¡Œï¼šç”¨æˆ·è´Ÿè½½èƒŒäº†runtime.Moduleåœ¨æ”¯æŒçš„è¿è¡Œç¯å¢ƒå’Œè¿è¡Œç¼–è¯‘åŠŸèƒ½ã€‚

![](imgs/tvm_dyn_workflow.svg)

### 1.1 Key data structures(å…³é”®æ•°æ®ç»“æ„)

One of the best ways to design and understand a complex system is to identify the key data structures and APIs that manipulate (transform) these data structures. Once we identified the key data structures, we can then breakdown a system into logical components that either define a collection of key data structures or transformations among the data structures.

è®¾è®¡å’Œç†è§£å¤æ‚ç³»ç»Ÿçš„æœ€ä½³æ–¹æ³•ä¹‹ä¸€å°±æ˜¯è¯†åˆ«å…³é”®æ•°æ®ç»“æ„å’Œæ“ä½œï¼ˆè½¬æ¢ï¼‰è¿™äº›æ•°æ®ç»“æ„çš„APIã€‚ä¸€æ—¦ç¡®å®šäº†å…³é”®æ•°æ®ç»“æ„ï¼Œæˆ‘ä»¬ä¾¿å¯ä»¥å°†ç³»ç»Ÿåˆ†è§£ä¸ºé€»è¾‘ç»„ä»¶ï¼Œè¿™äº›é€»è¾‘ç»„ä»¶å¯ä»¥å®šä¹‰å…³é”®æ•°æ®ç»“æ„çš„é›†åˆæˆ–æ•°æ®ç»“æ„ä¹‹é—´çš„è½¬æ¢ã€‚



**IRModule** is the primary data structure used across the entire stack. An IRModule (intermediate representation module) contains a collection of functions. Currently, we support two primary variants of functions.

**IRModule**æ˜¯æ•´ä¸ªå †æ ˆä¸­ä½¿ç”¨çš„ä¸»è¦æ•°æ®ç»“æ„ã€‚IRModuleï¼ˆä¸­é—´è¡¨ç¤ºæ¨¡å—ï¼‰åŒ…å«å‡½æ•°çš„é›†åˆã€‚å½“å‰ï¼Œæˆ‘ä»¬æ”¯æŒä¸¤ç§ä¸»è¦çš„åŠŸèƒ½å˜ä½“ã€‚



- **relay::Function** is a high-level functional program representation. A relay.Function usually corresponds to an end-to-end model. You can view a relay.Function as a computational graph with additional support for control-flow, recursion, and complex data structures.
- **relay :: Function**æ˜¯é«˜çº§åŠŸèƒ½ç¨‹åºè¡¨ç¤ºã€‚A relay. Functioné€šå¸¸å¯¹åº”äºç«¯åˆ°ç«¯æ¨¡å‹ã€‚æ‚¨å¯ä»¥å°†relay functionä½œä¸ºè®¡ç®—å›¾æ¥æŸ¥çœ‹ï¼Œå¹¶é¢å¤–æ”¯æŒæ§åˆ¶æµï¼Œé€’å½’å’Œå¤æ‚çš„æ•°æ®ç»“æ„ã€‚



- **tir::PrimFunc** is a low-level program representation that contains elements including loop-nest choices, multi-dimensional load/store, threading, and vector/tensor instructions. It is usually used to represent an operator program that executes a (possibly-fused) layer in a model.
- **tir :: PrimFunc**æ˜¯ä¸€ä¸ªä½çº§ç¨‹åºè¡¨ç¤ºï¼Œå…¶ä¸­åŒ…å«ä¸€äº›å…ƒç´ ï¼ŒåŒ…æ‹¬å¾ªç¯åµŒå¥—é€‰æ‹©ï¼Œ**å¤šç»´åŠ **è½½/å­˜å‚¨ï¼Œçº¿ç¨‹å’Œå‘é‡/å¼ é‡æŒ‡ä»¤ã€‚å®ƒé€šå¸¸ç”¨äºè¡¨ç¤ºæ‰§è¡Œæ¨¡å‹ä¸­ï¼ˆå¯èƒ½æ˜¯èåˆçš„ï¼‰å±‚çš„æ“ä½œå‘˜ç¨‹åºã€‚



During the compilation, a relay function may be lowered to multiple tir::PrimFunc functions and a top-level function that calls into those tir::PrimFunc functions.

åœ¨ç¼–è¯‘æœŸé—´ï¼Œå¯ä»¥å°†ä¸­ç»§å‡½æ•°é™ä½ä¸ºå¤šä¸ªtir :: PrimFuncå‡½æ•°å’Œä¸€ä¸ªè°ƒç”¨è¿™äº›tir :: PrimFuncå‡½æ•°çš„é¡¶çº§å‡½æ•°ã€‚

### 1.2 Transformations(è½¬å˜)

Now that we have covered the key data structures, let us talk about the transformations. Each transformation could serve one of the following purposes:

æ—¢ç„¶æˆ‘ä»¬å·²ç»æ¶µç›–äº†å…³é”®æ•°æ®ç»“æ„ï¼Œé‚£ä¹ˆè®©æˆ‘ä»¬æ¥è°ˆè°ˆè½¬æ¢ã€‚æ¯ä¸ªè½¬æ¢å¯ä»¥æ»¡è¶³ä»¥ä¸‹ç›®çš„ä¹‹ä¸€ï¼š



- optimization: transform a program to an equivalent, possibly more optimized version.
- ä¼˜åŒ–ï¼šå°†ç¨‹åºè½¬æ¢ä¸ºç­‰æ•ˆçš„ï¼Œå¯èƒ½æ›´ä¼˜åŒ–çš„ç‰ˆæœ¬ã€‚



- lowering: transform a program to a lower-level representation that is closer to the target.

- é™ä½ï¼šå°†ç¨‹åºè½¬æ¢ä¸ºæ›´æ¥è¿‘ç›®æ ‡çš„è¾ƒä½å±‚è¡¨ç¤ºã€‚



**relay/transform** contains a collection of passes that optimize the model. The optimizations include common program optimizations such as constant folding and dead-code elimination, and tensor-computation specific passes such as layout transformation and scaling factor folding.

**ä¸­ç»§/è½¬æ¢**åŒ…å«ä¼˜åŒ–æ¨¡å‹çš„éé›†åˆã€‚è¿™äº›ä¼˜åŒ–åŒ…æ‹¬å¸¸è§çš„ç¨‹åºä¼˜åŒ–ï¼Œä¾‹å¦‚æ’å®šæŠ˜å å’Œæ­»ä»£ç æ¶ˆé™¤ï¼Œä»¥åŠå¼ é‡è®¡ç®—ç‰¹å®šçš„éå†ï¼ˆä¾‹å¦‚å¸ƒå±€è½¬æ¢å’Œç¼©æ”¾å› å­æŠ˜å ï¼‰ã€‚



Near the end of the relay optimization pipeline, we will run a pass(FuseOps) to break the end-to-end function(e.g. MobileNet) into sub-function(e.g. conv2d-relu) segments. We call these segments of functions. This process helps us to divide the original problem into two sub-problems:

åœ¨ä¸­ç»§ä¼˜åŒ–ç®¡é“çš„æœ«ç«¯é™„è¿‘ï¼Œæˆ‘ä»¬å°†è¿è¡Œpassï¼ˆFuseOpsï¼‰å°†ç«¯åˆ°ç«¯åŠŸèƒ½ï¼ˆä¾‹å¦‚MobileNetï¼‰åˆ’åˆ†ä¸ºå­åŠŸèƒ½ï¼ˆä¾‹å¦‚conv2d-reluï¼‰æ®µã€‚æˆ‘ä»¬ç§°è¿™äº›åŠŸèƒ½ä¸ºæ®µã€‚æ­¤è¿‡ç¨‹å¯å¸®åŠ©æˆ‘ä»¬å°†åŸå§‹é—®é¢˜åˆ†ä¸ºä¸¤ä¸ªå­é—®é¢˜ï¼š



- Compilation and optimization for each sub-function.
- æ¯ä¸ªå­åŠŸèƒ½çš„ç¼–è¯‘å’Œä¼˜åŒ–ã€‚



- Overall execution structure: we need to do a sequence of calls into the generated sub-functions to execute the whole model.
- æ€»ä½“æ‰§è¡Œç»“æ„ï¼šæˆ‘ä»¬éœ€è¦å¯¹æ‰€ç”Ÿæˆçš„å­å‡½æ•°è¿›è¡Œä¸€ç³»åˆ—è°ƒç”¨ï¼Œä»¥æ‰§è¡Œæ•´ä¸ªæ¨¡å‹ã€‚



We use the low-level tir phase to compile and optimize each sub-functions. For specific targets, we may also directly go to the target translation phase and use external code generators.

æˆ‘ä»¬ä½¿ç”¨ä½çº§çš„Tiré˜¶æ®µæ¥ç¼–è¯‘å’Œä¼˜åŒ–æ¯ä¸ªå­åŠŸèƒ½ã€‚å¯¹äºç‰¹å®šç›®æ ‡ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ç›´æ¥è¿›å…¥ç›®æ ‡ç¿»è¯‘é˜¶æ®µå¹¶ä½¿ç”¨å¤–éƒ¨ä»£ç ç”Ÿæˆå™¨ã€‚



There are a few different ways(in relay/backend) to handle the calls into the overall execution problem. For simple models with known shapes and no control flow, we can lower to a graph runtime that stores the execution structure in a graph. We also support a virtual machine backend for dynamic executions. Finally, we plan to support ahead of time compilation that compiles the high-level execution structure into the executable and generated primitive functions. All of these execution modes are encapsulated by a unified **runtime.Module** interface, which we will discuss in the latter part of the guide.

æœ‰å‡ ç§ä¸åŒçš„æ–¹æ³•ï¼ˆåœ¨ä¸­ç»§/åç«¯ï¼‰æ¥å¤„ç†å¯¹æ•´ä¸ªæ‰§è¡Œé—®é¢˜çš„è°ƒç”¨ã€‚å¯¹äºå…·æœ‰å·²çŸ¥å½¢çŠ¶ä¸”æ— æ§åˆ¶æµçš„ç®€å•æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥é™ä½åˆ°å°†æ‰§è¡Œç»“æ„å­˜å‚¨åœ¨å›¾ä¸­çš„å›¾è¿è¡Œæ—¶ã€‚æˆ‘ä»¬è¿˜æ”¯æŒè™šæ‹Ÿæœºåç«¯è¿›è¡ŒåŠ¨æ€æ‰§è¡Œã€‚æœ€åï¼Œæˆ‘ä»¬è®¡åˆ’æ”¯æŒæå‰ç¼–è¯‘ï¼Œè¯¥ç¼–è¯‘å°†é«˜çº§æ‰§è¡Œç»“æ„ç¼–è¯‘ä¸ºå¯æ‰§è¡Œæ–‡ä»¶å’Œç”Ÿæˆçš„åŸå§‹å‡½æ•°ã€‚æ‰€æœ‰è¿™äº›æ‰§è¡Œæ¨¡å¼éƒ½ç”±ç»Ÿä¸€çš„**runtime.Module** æ¥å£å°è£… ï¼Œæˆ‘ä»¬å°†åœ¨æœ¬æŒ‡å—çš„åé¢éƒ¨åˆ†ä¸­è¿›è¡Œè®¨è®ºã€‚



**tir/transform** contains transformation passes for TIR level functions. Many tir passes serve the purpose of lowering. For example, there are passes to flatten multi-dimensional access to one-dimensional pointer access, to expand the intrinsics into target-specific ones, and to decorate the function entry to meet the runtime calling convention. Of course, there are also optimizations passes, such as access index simplification and dead code elimination.

**tir / transform**åŒ…å«ç”¨äºTIRçº§åˆ«åŠŸèƒ½çš„è½¬æ¢è¿‡ç¨‹ã€‚è®¸å¤šTiré€šè¡Œè¯çš„ç›®çš„æ˜¯é™ä½ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥é€šè¿‡å¤šç§é€”å¾„å°†å¤šç»´è®¿é—®å±•å¹³åˆ°ä¸€ç»´æŒ‡é’ˆè®¿é—®ï¼Œå°†å†…åœ¨å‡½æ•°æ‰©å±•ä¸ºç‰¹å®šäºç›®æ ‡çš„å†…åœ¨å‡½æ•°ï¼Œå¹¶ä¿®é¥°å‡½æ•°æ¡ç›®ä»¥æ»¡è¶³è¿è¡Œæ—¶è°ƒç”¨çº¦å®šã€‚å½“ç„¶ï¼Œä¹Ÿæœ‰ä¸€äº›ä¼˜åŒ–è¿‡ç¨‹ï¼Œä¾‹å¦‚ç®€åŒ–è®¿é—®ç´¢å¼•å’Œæ¶ˆé™¤æ— æ•ˆä»£ç ã€‚



Many low-level optimizations can be handled in the target phase by the LLVM, CUDA C, and other target compilers. As a result, we leave low-level optimizations such as register allocation to the downstream compilers and only focus on optimizations that are not covered by them.

LLVMï¼ŒCUDA Cå’Œå…¶ä»–ç›®æ ‡ç¼–è¯‘å™¨å¯ä»¥åœ¨ç›®æ ‡é˜¶æ®µå¤„ç†è®¸å¤šä½çº§ä¼˜åŒ–ã€‚ç»“æœï¼Œæˆ‘ä»¬å°†ä½çº§ä¼˜åŒ–ï¼ˆä¾‹å¦‚å¯„å­˜å™¨åˆ†é…ï¼‰ç•™ç»™äº†ä¸‹æ¸¸ç¼–è¯‘å™¨ï¼Œè€Œåªä¸“æ³¨äºå®ƒä»¬æœªæ¶µç›–çš„ä¼˜åŒ–ã€‚

### 1.3 Search-space and Learning-based Transformationsï¼ˆæœç´¢ç©ºé—´å’ŒåŸºäºå­¦ä¹ çš„è½¬æ¢ï¼‰

The transformation passes we described so far are deterministic and rule-based. One design goal of the TVM stack is to support high-performance code optimizations for different hardware platforms. To do so, we will need to investigate as many optimizations choices as possible, including but not limited to, multi-dimensional tensor access, loop tiling behavior, special accelerator memory hierarchy, and threading.

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬æè¿°çš„è½¬æ¢è¿‡ç¨‹æ˜¯ç¡®å®šæ€§çš„ä¸”åŸºäºè§„åˆ™çš„ã€‚TVMå †æ ˆçš„ä¸€ä¸ªè®¾è®¡ç›®æ ‡æ˜¯æ”¯æŒé’ˆå¯¹ä¸åŒç¡¬ä»¶å¹³å°çš„é«˜æ€§èƒ½ä»£ç ä¼˜åŒ–ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†éœ€è¦ç ”ç©¶å°½å¯èƒ½å¤šçš„ä¼˜åŒ–é€‰æ‹©ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºå¤šç»´å¼ é‡è®¿é—®ï¼Œå¾ªç¯åˆ‡ç‰‡è¡Œä¸ºï¼Œç‰¹æ®Šçš„åŠ é€Ÿå™¨å†…å­˜å±‚æ¬¡ç»“æ„å’Œçº¿ç¨‹åŒ–ã€‚



It is hard to define a heuristic to make all of the choices. Instead, we will take a search and learning-based approach. We first define a collection of actions we can take to transform a program. Example actions include loop transformations, inlining, vectorization. We call these actions **scheduling primitives**. The collection of scheduling primitives defines a search space of possible optimizations we can make to a program. The system then searches over different possible scheduling sequence to pick the best scheduling combination. The search procedure is usually guided by a machine learning algorithm.

å¾ˆéš¾å®šä¹‰åšå‡ºæ‰€æœ‰é€‰æ‹©çš„è¯•æ¢æ³•ã€‚ç›¸åï¼Œæˆ‘ä»¬å°†é‡‡ç”¨åŸºäºæœç´¢å’Œå­¦ä¹ çš„æ–¹æ³•ã€‚æˆ‘ä»¬é¦–å…ˆå®šä¹‰å¯ä»¥ç”¨æ¥è½¬æ¢ç¨‹åºçš„åŠ¨ä½œçš„é›†åˆã€‚ç¤ºä¾‹åŠ¨ä½œåŒ…æ‹¬å¾ªç¯è½¬æ¢ï¼Œå†…è”ï¼Œå‘é‡åŒ–ã€‚æˆ‘ä»¬ç§°è¿™äº›åŠ¨ä½œä¸º**è°ƒåº¦åŸè¯­**ã€‚è°ƒåº¦åŸè¯­çš„é›†åˆå®šä¹‰äº†æˆ‘ä»¬å¯ä»¥å¯¹ç¨‹åºè¿›è¡Œçš„å¯èƒ½ä¼˜åŒ–çš„æœç´¢ç©ºé—´ã€‚ç„¶åï¼Œç³»ç»Ÿæœç´¢ä¸åŒçš„å¯èƒ½è°ƒåº¦åºåˆ—ä»¥é€‰æ‹©æœ€ä½³è°ƒåº¦ç»„åˆã€‚æœç´¢è¿‡ç¨‹é€šå¸¸ä»¥æœºå™¨å­¦ä¹ ç®—æ³•ä¸ºæŒ‡å¯¼ã€‚



We can record the best schedule sequence for an (possibly-fused) operator once the search is completed. The compiler can then just lookup the best schedule sequence and apply it to the program. Notably, this schedule application phase is **exactly like** the rule-based transformations, enabling us to share the same interface convention with tradition passes.

æœç´¢å®Œæˆåï¼Œæˆ‘ä»¬å¯ä»¥ä¸ºï¼ˆå¯èƒ½æ˜¯èåˆçš„ï¼‰æ“ä½œå‘˜è®°å½•æœ€ä½³è°ƒåº¦é¡ºåºã€‚ç„¶åï¼Œç¼–è¯‘å™¨å¯ä»¥ä»…æŸ¥æ‰¾æœ€ä½³è°ƒåº¦åºåˆ—ï¼Œå¹¶å°†å…¶åº”ç”¨äºç¨‹åºã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ­¤è®¡åˆ’åº”ç”¨ç¨‹åºé˜¶æ®µ**å®Œå…¨ç±»ä¼¼äº**åŸºäºè§„åˆ™çš„è½¬æ¢ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿä¸ä¼ ç»Ÿæµç¨‹å…±äº«ç›¸åŒçš„æ¥å£çº¦å®šã€‚



We use search based optimizations to handle the initial tir function generation problem. This part of the module is called AutoTVM(auto_scheduler). We expect to expand the learning-based transformations to more areas as we continue to develop the TVM stack.

æˆ‘ä»¬ä½¿ç”¨åŸºäºæœç´¢çš„ä¼˜åŒ–æ¥å¤„ç†æœ€åˆçš„Tirå‡½æ•°ç”Ÿæˆé—®é¢˜ã€‚æ¨¡å—çš„æ­¤éƒ¨åˆ†ç§°ä¸ºAutoTVMï¼ˆauto_schedulerï¼‰ã€‚éšç€æˆ‘ä»¬ç»§ç»­å¼€å‘TVMå †æ ˆï¼Œæˆ‘ä»¬å¸Œæœ›å°†åŸºäºå­¦ä¹ çš„è½¬æ¢æ‰©å±•åˆ°æ›´å¤šé¢†åŸŸã€‚

### 1.4 Target Translationï¼ˆç›®æ ‡ç¿»è¯‘ï¼‰

The target translation phase transforms an IRModule to the corresponding target executable format. For backends such as x86 and ARM, we use the LLVM IRBuilder to build in-memory LLVM IR. We can also generate source-level languages such as CUDA C and OpenCL. Finally, we support direct translations of a Relay function (sub-graph) to specific targets via external code generators. It is important that the final code generation phase is as lightweight as possible. Vast majority of transformations and lowering should be performed before the target translation phase.

ç›®æ ‡è½¬æ¢é˜¶æ®µå°†IRModuleè½¬æ¢ä¸ºç›¸åº”çš„ç›®æ ‡å¯æ‰§è¡Œæ ¼å¼ã€‚å¯¹äºx86å’ŒARMç­‰åç«¯ï¼Œæˆ‘ä»¬ä½¿ç”¨LLVM IRBuilderæ¥æ„å»ºå†…å­˜ä¸­çš„LLVM IRã€‚æˆ‘ä»¬è¿˜å¯ä»¥ç”Ÿæˆè¯¸å¦‚CUDA Cå’ŒOpenCLä¹‹ç±»çš„æºä»£ç çº§è¯­è¨€ã€‚æœ€åï¼Œæˆ‘ä»¬æ”¯æŒé€šè¿‡å¤–éƒ¨ä»£ç ç”Ÿæˆå™¨å°†Relayå‡½æ•°ï¼ˆå­å›¾ï¼‰ç›´æ¥è½¬æ¢ä¸ºç‰¹å®šç›®æ ‡ã€‚é‡è¦çš„æ˜¯ï¼Œæœ€ç»ˆä»£ç ç”Ÿæˆé˜¶æ®µåº”å°½å¯èƒ½è½»å·§ã€‚ç»å¤§éƒ¨åˆ†çš„è½¬æ¢å’Œé™ä½éƒ½åº”åœ¨ç›®æ ‡ç¿»è¯‘é˜¶æ®µä¹‹å‰è¿›è¡Œã€‚



We also provide a Target structure to specify the compilation target. The transformations before the target translation phase can also be affected by the target â€” for example, a targetâ€™s vector length would change the vectorization behavior.

æˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªTargetç»“æ„æ¥æŒ‡å®šç¼–è¯‘ç›®æ ‡ã€‚ç›®æ ‡ç¿»è¯‘é˜¶æ®µä¹‹å‰çš„è½¬æ¢ä¹Ÿå¯èƒ½å—åˆ°ç›®æ ‡çš„å½±å“-ä¾‹å¦‚ï¼Œç›®æ ‡çš„å‘é‡é•¿åº¦ä¼šæ”¹å˜å‘é‡åŒ–è¡Œä¸ºã€‚

### 1.5 Runtime Executionï¼ˆè¿è¡Œæ—¶æ‰§è¡Œï¼‰

The main goal of TVMâ€™s runtime is to provide a minimal API for loading and executing the compiled artifact in a language of their choice, including Python, C++, Rust, Go, Java, and JavaScript. The code snippet below shows such an example in Python:

TVMè¿è¡Œæ—¶çš„ä¸»è¦ç›®æ ‡æ˜¯æä¾›ä¸€ä¸ªæœ€å°çš„APIï¼Œä»¥ä½¿ç”¨ä»–ä»¬é€‰æ‹©çš„è¯­è¨€ï¼ˆåŒ…æ‹¬Pythonï¼ŒC ++ï¼ŒRustï¼ŒGoï¼ŒJavaå’ŒJavaScriptï¼‰åŠ è½½å’Œæ‰§è¡Œå·²ç¼–è¯‘çš„å·¥ä»¶ã€‚ä¸‹é¢çš„ä»£ç ç‰‡æ®µæ˜¾ç¤ºäº†Pythonä¸­çš„è¿™æ ·ä¸€ä¸ªç¤ºä¾‹ï¼š



```python
import tvm
# Example runtime execution program in python, with type annotated
mod: tvm.runtime.Module = tvm.runtime.load_module("compiled_artifact.so")
arr: tvm.runtime.NDArray = tvm.nd.array([1, 2, 3], ctx=tvm.gpu(0))
fun: tvm.runtime.PackedFunc = mod["addone"]
fun(a)
print(a.asnumpy())
```

[`tvm.runtime.Module`](https://tvm.apache.org/docs/api/python/runtime.html#tvm.runtime.Module) encapsulates the result of compilation. A runtime.Module contains a GetFunction method to obtain PackedFuncs by name.

[`tvm.runtime.Module`](https://tvm.apache.org/docs/api/python/runtime.html#tvm.runtime.Module)å°è£…ç¼–è¯‘ç»“æœã€‚runtime.ModuleåŒ…å«ä¸€ä¸ªGetFunctionæ–¹æ³•ï¼Œç”¨äºæŒ‰åç§°è·å–PackedFuncsã€‚



[`tvm.runtime.PackedFunc`](https://tvm.apache.org/docs/api/python/runtime.html#tvm.runtime.PackedFunc) is a type-erased function interface for both the generated functions. A runtime.PackedFunc can take arguments and return values with the following types: POD types(int, float), string, runtime.PackedFunc, runtime.Module, runtime.NDArray, and other sub-classes of runtime.Object.

[`tvm.runtime.PackedFunc`](https://tvm.apache.org/docs/api/python/runtime.html#tvm.runtime.PackedFunc)æ˜¯ä¸¤ä¸ªç”Ÿæˆçš„å‡½æ•°çš„ç±»å‹æ“¦é™¤çš„å‡½æ•°æ¥å£ã€‚runtime.PackedFuncå¯ä»¥é‡‡ç”¨ä»¥ä¸‹ç±»å‹çš„å‚æ•°å¹¶è¿”å›å€¼ï¼šPODç±»å‹ï¼ˆintï¼Œfloatï¼‰ï¼Œå­—ç¬¦ä¸²ï¼Œruntime.PackedFuncï¼Œruntime.Moduleï¼Œruntime.NDArrayä»¥åŠruntime.Objectçš„å…¶ä»–å­ç±»ã€‚



[`tvm.runtime.Module`](https://tvm.apache.org/docs/api/python/runtime.html#tvm.runtime.Module) and [`tvm.runtime.PackedFunc`](https://tvm.apache.org/docs/api/python/runtime.html#tvm.runtime.PackedFunc) are powerful mechanisms to modularize the runtime. For example, to get the above addone function on CUDA, we can use LLVM to generate the host-side code to compute the launching parameters(e.g. size of the thread groups) and then call into another PackedFunc from a CUDAModule that is backed by the CUDA driver API. The same mechanism can be used for OpenCL kernels.

[`tvm.runtime.Module`](https://tvm.apache.org/docs/api/python/runtime.html#tvm.runtime.Module)å¹¶ä¸”[`tvm.runtime.PackedFunc`](https://tvm.apache.org/docs/api/python/runtime.html#tvm.runtime.PackedFunc)æ˜¯å°†è¿è¡Œæ—¶æ¨¡å—åŒ–çš„å¼ºå¤§æœºåˆ¶ã€‚ä¾‹å¦‚ï¼Œè¦åœ¨CUDAä¸Šè·å¾—ä¸Šè¿°addoneå‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨LLVMç”Ÿæˆä¸»æœºç«¯ä»£ç ä»¥è®¡ç®—å¯åŠ¨å‚æ•°ï¼ˆä¾‹å¦‚çº¿ç¨‹ç»„çš„å¤§å°ï¼‰ï¼Œç„¶åä»CUDAModuleè°ƒç”¨å¦ä¸€ä¸ªç”±PackedFuncæ”¯æŒçš„PackedFuncã€‚ CUDAé©±åŠ¨ç¨‹åºAPIã€‚ç›¸åŒçš„æœºåˆ¶å¯ç”¨äºOpenCLå†…æ ¸ã€‚



The above example only deals with a simple addone function. The code snippet below gives an example of an end-to-end model execution using the same interface:

ä¸Šé¢çš„ç¤ºä¾‹ä»…å¤„ç†ç®€å•çš„addoneå‡½æ•°ã€‚ä¸‹é¢çš„ä»£ç æ®µç»™å‡ºäº†ä½¿ç”¨åŒä¸€æ¥å£æ‰§è¡Œç«¯åˆ°ç«¯æ¨¡å‹çš„ç¤ºä¾‹ï¼š

```python
import tvm
# Example runtime execution program in python, with types annotated
factory: tvm.runtime.Module = tvm.runtime.load_module("resnet18.so")
# Create a stateful graph execution module for resnet18 on gpu(0)
gmod: tvm.runtime.Module = factory["resnet18"](tvm.gpu(0))
data: tvm.runtime.NDArray = get_input_data()
# set input
gmod["set_input"](0, data)
# execute the model
gmod["run"]()
# get the output
result = gmod["get_output"](0).asnumpy()
```

The main take away is that runtime.Module and runtime.PackedFunc are sufficient to encapsulate both operator level programs (such as addone), as well as the end-to-end models.

ä¸»è¦ä¼˜ç‚¹æ˜¯runtime.Moduleå’Œruntime.PackedFuncè¶³ä»¥å°è£…æ“ä½œå‘˜çº§ç¨‹åºï¼ˆä¾‹å¦‚addoneï¼‰ä»¥åŠç«¯åˆ°ç«¯æ¨¡å‹ã€‚

### 1.6 Summary and Discussions(æ€»ç»“ä¸è®¨è®º)

In summary, the key data structures in the compilation flows are:

æ€»ä¹‹ï¼Œç¼–è¯‘æµç¨‹ä¸­çš„å…³é”®æ•°æ®ç»“æ„ä¸ºï¼š

- IRModule: contains relay.Function and tir.PrimFunc
- runtime.Module: contains runtime.PackedFunc



Most parts of the compilation are transformations among the key data structures.

ç¼–è¯‘çš„å¤§éƒ¨åˆ†å†…å®¹æ˜¯å…³é”®æ•°æ®ç»“æ„ä¹‹é—´çš„è½¬æ¢ã€‚

- relay/transform and tir/transform are determinstic rule-based transformationsã€‚relay / transformå’Œtir / transformæ˜¯åŸºäºè§„åˆ™çš„ç¡®å®šæ€§è½¬æ¢
- auto_scheduler and autotvm contains the search-based transformationsã€‚auto_schedulerå’ŒautotvmåŒ…å«åŸºäºæœç´¢çš„è½¬æ¢



Finally, the compilation flow example is only a typical use-case of the TVM stack. We expose these key data structures and transformations to python and C++ APIs. As a result, you can use TVM just like the way you use numpy, except that the data structure of interest changes from the numpy.ndarray to tvm.IRModule. Here are some example use-cases:

æœ€åï¼Œç¼–è¯‘æµç¨‹ç¤ºä¾‹åªæ˜¯TVMå †æ ˆçš„å…¸å‹ç”¨ä¾‹ã€‚æˆ‘ä»¬å°†è¿™äº›å…³é”®æ•°æ®ç»“æ„å’Œè½¬æ¢å…¬å¼€ç»™pythonå’ŒC ++ APIã€‚å› æ­¤ï¼Œé™¤äº†æ„Ÿå…´è¶£çš„æ•°æ®ç»“æ„ä»numpy.ndarrayæ›´æ”¹ä¸ºtvm.IRModuleä¹‹å¤–ï¼Œæ‚¨å¯ä»¥åƒä½¿ç”¨numpyä¸€æ ·ä½¿ç”¨TVMã€‚ä»¥ä¸‹æ˜¯ä¸€äº›ç”¨ä¾‹ç¤ºä¾‹ï¼š

- Directly construct IRModule using the python API. ä½¿ç”¨python APIç›´æ¥æ„é€ IRModuleã€‚
- Compose a custom set of transformations(e.g. customize quantization).  ç»„æˆä¸€ç»„è‡ªå®šä¹‰çš„è½¬æ¢ï¼ˆä¾‹å¦‚ï¼Œè‡ªå®šä¹‰é‡åŒ–ï¼‰ã€‚
- Manipulate the IR directly using TVMâ€™s python API. ä½¿ç”¨TVMçš„python APIç›´æ¥æ“ä½œIRã€‚



## 2. Logical Architecture Componentsï¼ˆé€»è¾‘æ¶æ„ç»„ä»¶ï¼‰

![https://raw.githubusercontent.com/tvmai/web-data/main/images/design/tvm_static_overview.svg](imgs/tvm_static_overview.svg)

TVM Architecture Diagram

TVMä½“ç³»ç»“æ„å›¾



The above figure shows the major logical components in the project. Please read the following sections for information about the components and their relations.

ä¸Šå›¾æ˜¾ç¤ºäº†é¡¹ç›®ä¸­çš„ä¸»è¦é€»è¾‘ç»„ä»¶ã€‚è¯·é˜…è¯»ä»¥ä¸‹å„èŠ‚ï¼Œä»¥è·å–æœ‰å…³ç»„ä»¶åŠå…¶å…³ç³»çš„ä¿¡æ¯ã€‚

### 2.1 tvm/support

The support module contains the most common utilities for the infrastructure, such as generic arena allocator, socket, and logging.

æ”¯æŒæ¨¡å—åŒ…å«æœ€å¸¸ç”¨çš„åŸºç¡€è®¾æ–½å®ç”¨ç¨‹åºï¼Œä¾‹å¦‚é€šç”¨ç«æŠ€åœºåˆ†é…å™¨ï¼Œå¥—æ¥å­—å’Œæ—¥å¿—è®°å½•ã€‚

### 2.2 tvm/runtime

The runtime serves as the foundation of the TVM stack. It provides the mechanism to load and execute compiled artifacts. The runtime defines a stable standard set of C APIs to interface with frontend languages such as Python and Rust.

è¿è¡Œæ˜¯TVMå †æ ˆçš„åŸºç¡€ã€‚å®ƒæä¾›äº†åŠ è½½å’Œæ‰§è¡Œå·²ç¼–è¯‘å·¥ä»¶çš„æœºåˆ¶ã€‚runtimeå®šä¹‰äº†ä¸€ç»„ç¨³å®šçš„æ ‡å‡†C APIï¼Œä»¥ä¸è¯¸å¦‚Pythonå’ŒRustçš„å‰ç«¯è¯­è¨€è¿›è¡Œæ¥å£ã€‚



runtime::Object is one of the primary data structures in TVM runtime besides the runtime::PackedFunc. It is a reference-counted base class with a type index to support runtime type checking and downcasting. The object system allows the developer to introduce new data structures to the runtime, such as Array, Map, and new IR data structures.

é™¤äº†runtime :: PackedFuncä¹‹å¤–ï¼Œruntime :: Objectæ˜¯TVMè¿è¡Œæ—¶ä¸­çš„ä¸»è¦æ•°æ®ç»“æ„ä¹‹ä¸€ã€‚å®ƒæ˜¯å¸¦æœ‰ç±»å‹ç´¢å¼•çš„å¼•ç”¨è®¡æ•°åŸºç±»ï¼Œä»¥æ”¯æŒè¿è¡Œæ—¶ç±»å‹æ£€æŸ¥å’Œå‘ä¸‹è½¬æ¢ã€‚å¯¹è±¡ç³»ç»Ÿå…è®¸å¼€å‘äººå‘˜å‘è¿è¡Œæ—¶å¼•å…¥æ–°çš„æ•°æ®ç»“æ„ï¼Œä¾‹å¦‚æ•°ç»„ï¼Œæ˜ å°„å’Œæ–°çš„IRæ•°æ®ç»“æ„ã€‚



Besides deployment use-cases, the compiler itself also makes heavy use of TVMâ€™s runtime mechanism. All of the IR data structures are subclasses of runtime::Object, as a result, they can be directly accessed and manipulated from the Python frontend. We use the PackedFunc mechanism to expose various APIs to the frontend.

é™¤äº†éƒ¨ç½²ç”¨ä¾‹ä¹‹å¤–ï¼Œç¼–è¯‘å™¨æœ¬èº«è¿˜å¤§é‡ä½¿ç”¨TVMçš„è¿è¡Œæ—¶æœºåˆ¶ã€‚æ‰€æœ‰çš„IRæ•°æ®ç»“æ„éƒ½æ˜¯runtime :: Objectçš„å­ç±»ï¼Œå› æ­¤ï¼Œå¯ä»¥ä»Pythonå‰ç«¯ç›´æ¥è®¿é—®å’Œæ“ä½œå®ƒä»¬ã€‚æˆ‘ä»¬ä½¿ç”¨PackedFuncæœºåˆ¶å°†å„ç§APIå…¬å¼€ç»™å‰ç«¯ã€‚



Runtime support for different hardware backends are defined in subdirectories of runtime(e.g. runtime/opencl). These hardware-specific runtime modules define APIs for device memory allocation and device function serialization.

åœ¨è¿è¡Œæ—¶çš„å­ç›®å½•ä¸­å®šä¹‰äº†å¯¹ä¸åŒç¡¬ä»¶åç«¯çš„è¿è¡Œæ—¶æ”¯æŒï¼ˆä¾‹å¦‚runtime / openclï¼‰ã€‚è¿™äº›ç‰¹å®šäºç¡¬ä»¶çš„è¿è¡Œæ—¶æ¨¡å—å®šä¹‰äº†ç”¨äºè®¾å¤‡å†…å­˜åˆ†é…å’Œè®¾å¤‡åŠŸèƒ½åºåˆ—åŒ–çš„APIã€‚



runtime/rpc implements an RPC support for PackedFunc. We can use the RPC mechanism to send a cross-compiled library to a remote device and benchmark the execution performance. The rpc infrastructure enables data collection from a wide range of hardware backends for learning-based optimizations.

runtime / rpcä¸ºPackedFuncå®ç°RPCæ”¯æŒã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨RPCæœºåˆ¶å°†äº¤å‰ç¼–è¯‘çš„åº“å‘é€åˆ°è¿œç¨‹è®¾å¤‡ï¼Œå¹¶å¯¹æ‰§è¡Œæ€§èƒ½è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚rpcåŸºç¡€ç»“æ„æ”¯æŒä»å¹¿æ³›çš„ç¡¬ä»¶åç«¯æ”¶é›†æ•°æ®ï¼Œä»¥è¿›è¡ŒåŸºäºå­¦ä¹ çš„ä¼˜åŒ–ã€‚

- [TVM Runtime System](https://tvm.apache.org/docs/dev/runtime.html)
- [Debugger](https://tvm.apache.org/docs/dev/debugger.html)
- [Putting the VM in TVM: The Relay Virtual Machine](https://tvm.apache.org/docs/dev/virtual_machine.html)
- [Introduction to Module Serialization](https://tvm.apache.org/docs/dev/introduction_to_module_serialization.html)



### 2.3 tvm/node

The node module adds additional features on top of the runtime::Object for IR data structures. The main features include reflection, serialization, structural equivalence, and hashing.

èŠ‚ç‚¹æ¨¡å—åœ¨runtime :: Objectçš„åŸºç¡€ä¸Šä¸ºIRæ•°æ®ç»“æ„æ·»åŠ äº†å…¶ä»–åŠŸèƒ½ã€‚ä¸»è¦åŠŸèƒ½åŒ…æ‹¬åå°„ï¼Œåºåˆ—åŒ–ï¼Œç»“æ„ç­‰æ•ˆå’Œæ•£åˆ—ã€‚



Thanks to the node module, we can directly access any field of the TVMâ€™s IRNode by their name in Python.

ç”±äºä½¿ç”¨äº†èŠ‚ç‚¹æ¨¡å—ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å®ƒä»¬åœ¨Pythonä¸­çš„åç§°ç›´æ¥è®¿é—®TVMçš„IRNodeçš„ä»»ä½•å­—æ®µã€‚

```
x = tvm.tir.Var("x", "int32")
y = tvm.tir.Add(x, x)
# a and b are fields of a tir.Add node
# we can directly use the field name to access the IR structures
assert y.a == x
```

We can also serialize arbitrary IR node into a JSON format, and load them back. The ability to save/store, and inspect an IR node provides a foundation for making the compiler more accessible.

æˆ‘ä»¬è¿˜å¯ä»¥å°†ä»»æ„IRèŠ‚ç‚¹åºåˆ—åŒ–ä¸ºJSONæ ¼å¼ï¼Œç„¶åå°†å…¶åŠ è½½å›ã€‚ä¿å­˜/å­˜å‚¨å’Œæ£€æŸ¥IRèŠ‚ç‚¹çš„èƒ½åŠ›ä¸ºä½¿ç¼–è¯‘å™¨æ›´æ˜“äºè®¿é—®æä¾›äº†åŸºç¡€ã€‚

### 2.4 tvm/ir

The tvm/ir folder contains the unified data structure and interfaces across for all IR function variants. The components in tvm/ir are shared by tvm/relay and tvm/tir, notable ones include

åœ¨TVM / IRæ–‡ä»¶å¤¹åŒ…å«è·¨æ‰€æœ‰IRåŠŸèƒ½å˜å¼‚ä½“çš„ç»Ÿä¸€çš„æ•°æ®ç»“æ„å’Œæ¥å£ã€‚tvm / irä¸­çš„ç»„ä»¶ç”±tvm / relayå’Œtvm / tirå…±äº«ï¼Œå€¼å¾—æ³¨æ„çš„åŒ…æ‹¬

- IRModule
- Type
- PassContext and Pass
- Op

Different variants of functions(e.g. relay.Function and tir.PrimFunc) can co-exist in an IRModule. While these variants may not have the same content representation, they use the same data structure to represent types. As a consequence, we use the same data structure to represent function (type) signatures of these variants. The unified type system allows one function variant to call another function once we clearly define the calling convention. This opens doors for future cross-function-variant optimizations.

Functionsçš„ä¸åŒå˜ä½“ï¼ˆä¾‹å¦‚relay.Functionå’Œtir.PrimFuncï¼‰å¯ä»¥å…±å­˜äºIRModuleä¸­ã€‚å°½ç®¡è¿™äº›å˜ä½“å¯èƒ½ä¸å…·æœ‰ç›¸åŒçš„å†…å®¹è¡¨ç¤ºï¼Œä½†æ˜¯å®ƒä»¬ä½¿ç”¨ç›¸åŒçš„æ•°æ®ç»“æ„æ¥è¡¨ç¤ºç±»å‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨ç›¸åŒçš„æ•°æ®ç»“æ„æ¥è¡¨ç¤ºè¿™äº›å˜é‡çš„åŠŸèƒ½ï¼ˆç±»å‹ï¼‰ç­¾åã€‚ä¸€æ—¦æˆ‘ä»¬æ˜ç¡®å®šä¹‰äº†è°ƒç”¨çº¦å®šï¼Œç»Ÿä¸€ç±»å‹ç³»ç»Ÿå°±å…è®¸ä¸€ä¸ªå‡½æ•°å˜ä½“è°ƒç”¨å¦ä¸€ä¸ªå‡½æ•°ã€‚è¿™ä¸ºå°†æ¥çš„è·¨åŠŸèƒ½å˜é‡ä¼˜åŒ–æ‰“å¼€äº†å¤§é—¨ã€‚



We also provide a unified PassContext for configuring the pass behavior, and common composite passes to execute a pass pipeline. The following code snippet gives an example of PassContext configuration.

æˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„PassContextç”¨äºé…ç½®ä¼ é€’è¡Œä¸ºï¼Œå¹¶æä¾›äº†é€šç”¨çš„å¤åˆä¼ é€’æ¥æ‰§è¡Œä¼ é€’ç®¡é“ã€‚ä»¥ä¸‹ä»£ç æ®µç»™å‡ºäº†PassContexté…ç½®çš„ç¤ºä¾‹ã€‚



```
# configure the behavior of the tir.UnrollLoop pass
with tvm.transform.PassContext(config={"tir.UnrollLoop": { "auto_max_step": 10 }}):
    # code affected by the pass context
```

Op is the common class to represent all system-defined primitive operator/intrinsics. Developers can register new Ops as well as their additional attributes(e.g. whether the Op is elementwise) to the system.

Opæ˜¯è¡¨ç¤ºæ‰€æœ‰ç³»ç»Ÿå®šä¹‰çš„åŸå§‹è¿ç®—ç¬¦/å†…éƒ¨å‡½æ•°çš„é€šç”¨ç±»ã€‚å¼€å‘äººå‘˜å¯ä»¥å‘ç³»ç»Ÿæ³¨å†Œæ–°çš„Opsä»¥åŠå®ƒä»¬çš„å…¶ä»–å±æ€§ï¼ˆä¾‹å¦‚Opæ˜¯å¦æ˜¯å…ƒç´ åŒ–çš„ï¼‰ã€‚



- [Pass Infrastructure](https://tvm.apache.org/docs/dev/pass_infra.html)

### 2.5 tvm/target

The target module contains all the code generators that translate an IRModule to a target runtime.Module. It also provides a common Targetclass that describes the target.

ç›®æ ‡æ¨¡å—åŒ…å«å°†IRModuleè½¬æ¢ä¸ºç›®æ ‡runtime.Moduleçš„æ‰€æœ‰ä»£ç ç”Ÿæˆå™¨ã€‚å®ƒè¿˜æä¾›äº†æè¿°ç›®æ ‡çš„é€šç”¨Targetç±»ã€‚



The compilation pipeline can be customized according to the target by querying the attribute information in the target and builtin information registered to each target id(cuda, opencl).

é€šè¿‡æŸ¥è¯¢ç›®æ ‡ä¸­çš„å±æ€§ä¿¡æ¯å’Œæ³¨å†Œåˆ°æ¯ä¸ªç›®æ ‡idï¼ˆcudaï¼Œopenclï¼‰çš„å†…ç½®ä¿¡æ¯ï¼Œå¯ä»¥æ ¹æ®ç›®æ ‡å®šåˆ¶ç¼–è¯‘ç®¡é“ã€‚

### 2.6 tvm/tir

TIR contains the definition of the low-level program representations. We use tir::PrimFunc to represent functions that can be transformed by TIR passes. Besides the IR data structures, the tir module also defines a set of builtin intrinsics and their attributes via the common Op registry, as well as transformation passes in tir/transform.

TIRåŒ…å«ä½çº§ç¨‹åºè¡¨ç¤ºçš„å®šä¹‰ã€‚æˆ‘ä»¬ä½¿ç”¨tir :: PrimFuncè¡¨ç¤ºå¯ä»¥é€šè¿‡TIRä¼ é€’è½¬æ¢çš„å‡½æ•°ã€‚é™¤IRæ•°æ®ç»“æ„å¤–ï¼Œtiræ¨¡å—è¿˜é€šè¿‡å…¬å…±Opæ³¨å†Œè¡¨ä»¥åŠtir / transformä¸­çš„è½¬æ¢ä¼ é€’å®šä¹‰äº†ä¸€ç»„å†…ç½®çš„å†…åœ¨å‡½æ•°åŠå…¶å±æ€§ã€‚

### 2.7 tvm/arith

This module is closely tied to the TIR. One of the key problems in the low-level code generation is the analysis of the indicesâ€™ arithmetic properties â€” the positiveness, variable bound, and the integer set that describes the iterator space. arith module provides a collection of tools that do (primarily integer) analysis. A TIR pass can use these analyses to simplify and optimize the code.

æ­¤æ¨¡å—ä¸TIRç´§å¯†ç›¸å…³ã€‚ä½çº§ä»£ç ç”Ÿæˆä¸­çš„å…³é”®é—®é¢˜ä¹‹ä¸€æ˜¯åˆ†æç´¢å¼•çš„ç®—æœ¯å±æ€§-æ­£æ€§ï¼Œå˜é‡è¾¹ç•Œä»¥åŠæè¿°è¿­ä»£å™¨ç©ºé—´çš„æ•´æ•°é›†ã€‚arithæ¨¡å—æä¾›äº†ä¸€ç»„è¿›è¡Œï¼ˆä¸»è¦æ˜¯æ•´æ•°ï¼‰åˆ†æçš„å·¥å…·ã€‚TIRé€šè¡Œè¯å¯ä»¥ä½¿ç”¨è¿™äº›åˆ†ææ¥ç®€åŒ–å’Œä¼˜åŒ–ä»£ç ã€‚

### 2.8 tvm/te

The name te stands for â€œtensor expressionâ€. This is a domain-specific language module that allows us to construct tir::PrimFunc variants quickly by writing tensor expressions. Importantly, a tensor expression itself is not a self-contained function that can be stored into IRModule. Instead, it is a fragment of IR that we can stitch together to build an IRModule.

teä»£è¡¨â€œå¼ é‡è¡¨è¾¾å¼â€ã€‚è¿™æ˜¯ä¸€ä¸ªç‰¹å®šé¢†åŸŸçš„è¯­è¨€æ¨¡å—ï¼Œå…è®¸æˆ‘ä»¬é€šè¿‡ç¼–å†™å¼ é‡è¡¨è¾¾å¼æ¥å¿«é€Ÿæ„å»ºtir :: PrimFuncå˜ä½“ã€‚é‡è¦çš„æ˜¯ï¼Œå¼ é‡è¡¨è¾¾å¼æœ¬èº«ä¸æ˜¯å¯ä»¥å­˜å‚¨åˆ°IRModuleä¸­çš„è‡ªåŒ…å«å‡½æ•°ã€‚ç›¸åï¼Œå®ƒæ˜¯IRçš„ä¸€ä¸ªç‰‡æ®µï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶æ‹¼æ¥åœ¨ä¸€èµ·ä»¥æ„å»ºIRModuleã€‚



te/schedule provides a collection of scheduling primitives to control the function being generated. In the future, we might bring some of these scheduling components to the a tir::PrimFunc itself.

te / scheduleæä¾›äº†ä¸€ç»„è°ƒåº¦åŸè¯­ï¼Œä»¥æ§åˆ¶æ‰€ç”Ÿæˆçš„åŠŸèƒ½ã€‚å°†æ¥ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šå°†å…¶ä¸­çš„ä¸€äº›è°ƒåº¦ç»„ä»¶å¼•å…¥tir :: PrimFuncæœ¬èº«ã€‚

- [InferBound Pass](https://tvm.apache.org/docs/dev/inferbound.html)
- [Hybrid Frontend Developer Guide](https://tvm.apache.org/docs/dev/hybrid_script.html)

### 2.9 tvm/topi

While possible to construct operators directly via TIR or tensor expressions (TE) for each use case it is tedious to do so. topi (Tensor operator inventory) provides a set of pre-defined operators (in TE or TIR) defined by numpy and found in common deep learning workloads. We also provide a collection of common schedule templates to obtain performant implementations across different target platforms.

å°½ç®¡å¯ä»¥é’ˆå¯¹æ¯ä¸ªç”¨ä¾‹ç›´æ¥é€šè¿‡TIRæˆ–å¼ é‡è¡¨è¾¾å¼ï¼ˆTEï¼‰æ„é€ è¿ç®—ç¬¦ï¼Œä½†è¿™æ ·åšå¾ˆéº»çƒ¦ã€‚ topiï¼ˆå¼ é‡è¿ç®—ç¬¦æ¸…å•ï¼‰æä¾›äº†ä¸€ç»„é¢„å®šä¹‰çš„è¿ç®—ç¬¦ï¼ˆåœ¨TEæˆ–TIRä¸­ï¼‰ï¼Œç”±numpyå®šä¹‰å¹¶åœ¨å¸¸è§çš„æ·±åº¦å­¦ä¹ å·¥ä½œè´Ÿè½½ä¸­æ‰¾åˆ°ã€‚æˆ‘ä»¬è¿˜æä¾›äº†ä¸€ç»„å…¬å…±æ—¶é—´è¡¨æ¨¡æ¿ï¼Œä»¥åœ¨ä¸åŒç›®æ ‡å¹³å°ä¸Šè·å¾—é«˜æ€§èƒ½çš„å®ç°ã€‚

### 2.10 tvm/relay

Relay is the high-level functional IR used to represent full models. Various optimizations are defined in relay.transform. The Relay compiler defines multiple dialects, and each dialect is designed to support specific styles of optimization. Notable ones include QNN(for importing pre-quantized models), VM(for lowering to dynamic virtual machine), memory(for memory optimization).

ç»§ç”µå™¨æ˜¯ç”¨äºè¡¨ç¤ºå®Œæ•´æ¨¡å‹çš„é«˜çº§åŠŸèƒ½æ€§IRã€‚åœ¨relay.transformä¸­å®šä¹‰äº†å„ç§ä¼˜åŒ–ã€‚Relayç¼–è¯‘å™¨å®šä¹‰äº†å¤šç§æ–¹è¨€ï¼Œæ¯ç§æ–¹è¨€æ—¨åœ¨æ”¯æŒç‰¹å®šçš„ä¼˜åŒ–æ ·å¼ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯QNNï¼ˆç”¨äºå¯¼å…¥é¢„é‡åŒ–æ¨¡å‹ï¼‰ï¼ŒVMï¼ˆç”¨äºé™çº§ä¸ºåŠ¨æ€è™šæ‹Ÿæœºï¼‰ï¼Œå†…å­˜ï¼ˆç”¨äºå†…å­˜ä¼˜åŒ–ï¼‰ã€‚

- [Introduction to Relay IR](https://tvm.apache.org/docs/dev/relay_intro.html)
- [Relay Operator Strategy](https://tvm.apache.org/docs/dev/relay_op_strategy.html)
- [Convert Layout Pass](https://tvm.apache.org/docs/dev/convert_layout.html)

### 2.11 tvm/autotvm

AutoTVM and AutoScheduler are both components which automate search based program optimization. This is rapidly evolving and primarily consists of:

AutoTVMå’ŒAutoScheduleréƒ½æ˜¯è‡ªåŠ¨è¿›è¡ŒåŸºäºæœç´¢çš„ç¨‹åºä¼˜åŒ–çš„ç»„ä»¶ã€‚è¿™æ˜¯è¿…é€Ÿå‘å±•çš„ï¼Œä¸»è¦åŒ…æ‹¬ï¼š

- Cost models and feature extraction.æˆæœ¬æ¨¡å‹å’Œç‰¹å¾æå–ã€‚
- A record format for storing program benchmark results for cost model construction.ä¸€ç§è®°å½•æ ¼å¼ï¼Œç”¨äºå­˜å‚¨è®¡åˆ’åŸºå‡†ç»“æœä»¥è¿›è¡Œæˆæœ¬æ¨¡å‹æ„å»ºã€‚
- A set of search policies over program transformations.ä¸€ç»„æœ‰å…³ç¨‹åºè½¬æ¢çš„æœç´¢ç­–ç•¥ã€‚

Automated program optimization is still an active research field. As a result, we have attempted to modularize the design so that researchers may quickly modify a component or apply their own algorithms via the Python bindings, and customize the search and plugin their algorithms from the Python binding.

è‡ªåŠ¨åŒ–ç¨‹åºä¼˜åŒ–ä»ç„¶æ˜¯æ´»è·ƒçš„ç ”ç©¶é¢†åŸŸã€‚ç»“æœï¼Œæˆ‘ä»¬è¯•å›¾å¯¹è®¾è®¡è¿›è¡Œæ¨¡å—åŒ–ï¼Œä»¥ä¾¿ç ”ç©¶äººå‘˜å¯ä»¥é€šè¿‡Pythonç»‘å®šå¿«é€Ÿä¿®æ”¹ç»„ä»¶æˆ–åº”ç”¨è‡ªå·±çš„ç®—æ³•ï¼Œå¹¶è‡ªå®šä¹‰æœç´¢å¹¶ä»Pythonç»‘å®šä¸­æ’å…¥å…¶ç®—æ³•ã€‚

- [Benchmark Performance Log Format](https://tvm.apache.org/docs/dev/benchmark.html)

### 2.12 Frontends

Frontends ingest models from different frameworks into the TVM stack. [`tvm.relay.frontend`](https://tvm.apache.org/docs/api/python/relay/frontend.html#module-tvm.relay.frontend) is the namespace for model ingestion APIs.

å‰ç«¯å°†æ¥è‡ªä¸åŒæ¡†æ¶çš„æ¨¡å‹å¸æ”¶åˆ°TVMå †æ ˆä¸­ã€‚ [`tvm.relay.frontend`](https://tvm.apache.org/docs/api/python/relay/frontend.html#module-tvm.relay.frontend)æ˜¯æ¨¡å‹æå–APIçš„åç§°ç©ºé—´ã€‚

- [TensorFlow Frontend](https://tvm.apache.org/docs/dev/frontend/tensorflow.html)

## Security

- [Security Guide](https://tvm.apache.org/docs/dev/security.html)