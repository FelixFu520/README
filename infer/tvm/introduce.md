# TVM Overview

âŒšï¸:2020å¹´11æœˆ30æ—¥

ğŸ“šå‚è€ƒ

-  [åŸæ–‡](https://chhzh123.github.io/blogs/2020-03-26-tvm-overview/)

- [CSDN](https://blog.csdn.net/sinat_31425585/article/details/89395680) [éœ€è¦ä¿®æ”¹ï¼Œè¿™ä¸ªæ˜¯ç¬¬ä¸€ä»£TVM]


---

> - æ·±åº¦å­¦ä¹ ç¼–è¯‘å™¨å­¦ä¹ ç¬”è®°å’Œå®è·µä½“ä¼š https://zhuanlan.zhihu.com/c_1169609848697663488
> - è“è‰²çš„å‘³é“ https://zhuanlan.zhihu.com/frozengene
> - TVMå®˜æ–¹ä¸“æ  https://zhuanlan.zhihu.com/tvmai

# 1. åˆè¯† TVM

æ—¶éš”ä¸€å¹´å†å›æ¥çœ‹TVMï¼Œè¯¸å¤šæ„Ÿå—ã€‚

åœ¨è¿™ä¸€å¹´æˆ‘åšå…¶ä»–æ–¹é¢å·¥ä½œçš„æ—¶é—´é‡Œï¼ŒæŸç§æ„ä¹‰ä¸Šå·²ç»é”™å¤±äº†åšæ·±åº¦å­¦ä¹ ç¼–è¯‘å™¨çš„è‰¯æœºã€‚TVMçš„å‘æ—©æ—©æŒ–å‡ºæ¥ï¼Œç¬¬ä¸€æ‰¹ç ”ç©¶çš„å­¦è€…å·²ç»åœ¨è‡ªåŠ¨è°ƒåº¦[FlexTensor ASPLOSâ€™20]ã€CPUæ¨æ–­[NeoCPU ATCâ€™19]ã€å¼‚æ„è®¡ç®—å¹³å°[HeteroCL FPGAâ€™19]ç­‰æ–¹é¢åšå‡ºäº†ä¸€å®šçš„æˆæœï¼Œæ‰€ä»¥ç°åœ¨å…¥å‘ä¼¼ä¹å·²ç»æœ‰ç‚¹æ™šäº†ã€‚ä½†çœ‹çœ‹TVMè¿™ä¸€å¹´çš„é£é€Ÿå‘å±•ï¼Œç°åœ¨v0.7ç‰ˆæœ¬å°†è¦å‘å¸ƒï¼ŒRelay IRçš„æå‡ºã€VTAç¼–è¯‘çš„æ›´å¤šæ”¯æŒã€å¼‚æ„å›¾åˆ’åˆ†ï¼Œå„å¤§[å‚å•†](https://tvm.apache.org/community)ï¼ˆäºšé©¬é€Šã€é˜¿é‡Œå·´å·´ã€åä¸ºã€Intelç­‰ï¼‰åŠé«˜æ ¡ï¼ˆUWã€Cornellã€UCBã€UCLAç­‰ï¼‰çš„æ”¯æŒï¼ŒApacheåŸºé‡‘ä¼šçš„åŠ æŒï¼Œ[Githubé¡¹ç›®](https://github.com/apache/incubator-tvm/)çš„æŒç»­æ›´æ–°ï¼Œå¼€æºç¤¾åŒºçš„ä¸æ–­å£®å¤§ï¼Œä¸¤æ¬¡[TVM Conference](https://tvmconf.org/)çš„ä¸¾åŠï¼Œéƒ½æ„å‘³ç€TVMè¿™ä¸ªå¹³å°çš„æ„ˆå‘æˆç†Ÿï¼Œè€Œä¸æ˜¯ä¸€ä¸ªdemo projectè½»æ˜“å°±ä¼šæ¶ˆäº¡ã€‚å› æ­¤ï¼Œä»è¿™ä¸ªè§’åº¦ä¸Šæ¥è¯´ï¼Œåœ¨é¡¹ç›®å‘å±•ä¸­æœŸå…¥å‘ä¾ç„¶å¯ä»¥æ¢ç´¢å‡ºå¾ˆå¤šå·¥ä½œï¼Œè€Œä¸”æ–‡æ¡£é€æ­¥å®Œå–„ä¹Ÿé¿å…äº†èµ°å¤§é‡å¼¯è·¯ã€‚æœªæ¥TVMå¾ˆæœ‰å¯èƒ½æˆä¸ºæ·±åº¦å­¦ä¹ æ—¶ä»£ä¸€ä¸ªä¸å¯è·å–çš„å·¥å…·ï¼Œä¸ç®¡æ˜¯å°†å…¶ä½œä¸ºä¸€ä¸ªå·¥å…·ä½¿ç”¨ï¼Œæˆ–æ˜¯å°†å…¶ä½œä¸ºç ”ç©¶å¯¹è±¡éƒ½æ˜¯ä¸é”™çš„é€‰æ‹©ã€‚



ç¬¬ä¸€ä»£çš„TVMä»¥NNVMä½œä¸ºå‰ç«¯ç¼–è¯‘å™¨ï¼Œå°†ä¸åŒæ¡†æ¶ç¼–å†™çš„æ¨¡å‹ä»¥ç»Ÿä¸€çš„æ ¼å¼æ˜ å°„åˆ°NNVMçš„è®¡ç®—å›¾ä¸Šï¼Œç„¶åå†å¯¹è®¡ç®—å›¾è¿›è¡Œä¼˜åŒ–è¿›å…¥åˆ°TVMï¼Œæœ€åç»ç”±TVMè¾“å‡ºåç«¯ä»£ç ï¼Œæ•´ä½“æµç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚



![NNVM](imgs/nnvm_compiler_stack.png)



è€Œç¬¬äºŒä»£çš„NNVMåˆ™æ˜¯å°†Relay IRä½œä¸ºå‰ç«¯ï¼Œæä¾›äº†æ›´ä¸ºç®€æ´çš„æ–‡æœ¬å½¢å¼ï¼Œå¼ºç±»å‹ç³»ç»Ÿï¼Œå¢æ·»äº†å¯¹æ§åˆ¶æµçš„æ”¯æŒï¼ŒåŒæ—¶æ”¯æŒè‡ªåŠ¨å¾®åˆ†(automatic differentiation, AD)åŠå¼‚æ„ç¼–è¯‘ï¼ˆéœ€è¦æ‰‹åŠ¨åˆ’åˆ†ï¼‰ã€‚



![img](imgs/tvm-overview.png)

æ€»ä½“ç¼–è¯‘æµç¨‹å¯è§[TVM-ä»£ç ç”Ÿæˆæµç¨‹](https://chhzh123.github.io/blogs/2020-03-26-tvm-flow/)ã€‚ç®€è€Œè¨€ä¹‹åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š

1. Relayå°†ä¸åŒæ¡†æ¶è¯»å…¥çš„æ¨¡å‹è½¬æ¢ä¸ºRelay IR
2. **è®¡ç®—å›¾å±‚é¢**(graph-level)çš„ä¼˜åŒ–ï¼ˆæ¯”å¦‚æ•´ä¸ªç¥ç»ç½‘ç»œï¼‰
3. ç”Ÿæˆä¼˜åŒ–åçš„è®¡ç®—å›¾é€å…¥TVM
4. **ç®—å­å±‚é¢**(operator-level)çš„ä¼˜åŒ–ï¼ˆæ¯”å¦‚ä¸€ä¸ªå·ç§¯ç®—å­ï¼‰
5. å¯¹æ¯ä¸€ç®—å­lowerï¼Œç”Ÿæˆåç«¯ä»£ç 

ä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ æ¡†æ¶å¦‚PyTorchå’ŒTensorFlowå¾€å¾€åªåœ¨è®¡ç®—å›¾å±‚é¢è¿›è¡Œä¼˜åŒ–ï¼Œå¾ˆéš¾åšåˆ°é€‚é…ä¸åŒç¡¬ä»¶çš„ä¼˜åŒ–ï¼ˆè®¡ç®—å›¾ç»“ç‚¹å¹¶æ²¡æœ‰å‘ŠçŸ¥è¯¥ç®—å­å¦‚ä½•å®æ–½ï¼‰ã€‚è€ŒTVMåˆ™æ˜¯æ›´æ·±å…¥åˆ°ç®—å­å±‚é¢ï¼Œå› æ­¤ä¼˜åŒ–ç²’åº¦æ›´ç»†ï¼Œå¯ä»¥é’ˆå¯¹ä¸åŒç¡¬ä»¶ç‰¹æ€§æ¥åšä¼˜åŒ–ï¼ˆæ¯”å¦‚è¯´åœ¨CPUä¸Šç”¨AVXåšå¹¶è¡Œï¼‰ï¼Œè¿™ä¹Ÿæ˜¯TVMè¿™ç§æ·±åº¦å­¦ä¹ **ç¼–è¯‘å™¨**èƒ½å¤Ÿèƒœè¿‡ä¹‹å‰æ·±åº¦å­¦ä¹ **æ¡†æ¶**çš„åŸå› ã€‚

å¦ä¸€æ–¹é¢ï¼Œåœ¨ç®—å­å±‚é¢TVMè¿˜å¼•å…¥äº†AutoTVMè¿›è¡Œè‡ªåŠ¨è°ƒå‚ï¼Œè™½ç„¶è¿˜æ²¡è¾¾åˆ°AutoScheduleçš„çº§åˆ«ï¼Œä½†æ˜¯å·²ç»èƒ½å¤Ÿå¾ˆå¥½åœ°é’ˆå¯¹ä¸åŒç¡¬ä»¶è¿›è¡Œscheduleçš„**å‚æ•°**è°ƒæ•´äº†ã€‚è¿™ä¹Ÿç›¸å½“äºTVMå°†MLå’ŒsystemåŒå‘æ‰“é€šï¼Œæ—¢æ˜¯system for MLï¼Œä¹Ÿç”¨åˆ°äº†ML for systemæ¥åšä¼˜åŒ–ï¼Œä¸¤è€…ç›¸è¾…ç›¸æˆï¼Œæœ€ç»ˆæ‰èƒ½è¾¾åˆ°è¿™ä¹ˆå¥½çš„æ•ˆæœã€‚

å†çœ‹ä¸€ä¸‹å¤©å¥‡åœ¨ç¬¬äºŒå±ŠTVM Confçš„æŠ¥å‘Š[*TVM: Where are we going*](https://tvmconf.org/slides/2019/tvmconf-keynote-dec19.pdf)ï¼Œå°±ä¼šå‘ç°TVMçš„å…¨æ ˆçœŸçš„ä¸æ˜¯å¼€ç©ç¬‘çš„ï¼Œåº”è¯¥æ•´ä¸ªUW CSç³»éƒ½æŠ•å…¥å…¶ä¸­äº†ï¼Œç°åœ¨ä»–ä»¬ç€æ‰‹åœ¨ä»¥ä¸‹å‡ ä¸ªç‚¹ï¼š

- Relayè™šæ‹Ÿæœºï¼šç”¨æ¥å¤„ç†åŠ¨æ€è®¡ç®—å›¾ï¼ˆå¦‚æœ‰é€’å½’å’Œå¾ªç¯çš„å›¾ç»“æ„ï¼‰
- Î¼TVMï¼šåœ¨è¾¹ç¼˜ç«¯æ— éœ€OSçš„è¿è¡Œæ—¶ç³»ç»Ÿï¼Œè‡ªåŠ¨ä¸AutoTVMè¿›è¡Œäº¤äº’

![img](imgs/mu-tvm.png)

- VTAï¼šç¬¬äºŒä»£å·²ç»ç”¨Chiselè¿›è¡Œå®ç°

![img](imgs/vta-stack.png)

- TSIMï¼šè‡ªç ”ç¡¬ä»¶æ¨¡æ‹Ÿå™¨

![img](imgs/tsim.png)

- å¤§ä¸€ç»Ÿè¿è¡Œæ—¶ï¼šä¸åŒè®¾å¤‡çš„runtimeéƒ½å¯ä»¥ç”¨Pythonç›´æ¥callï¼Œæ‰€ä»¥æ‰€æœ‰å·¥ä½œéƒ½å¯ä»¥åœ¨ç›´æ¥TVMå†…å®Œæˆï¼ˆä¼¼ä¹ç°åœ¨å·²ç»å®ç°äº†å¤§åŠï¼‰
- å¤§ä¸€ç»Ÿçš„IRï¼šç±»ä¼¼è°·æ­Œ[MLIR](https://mlir.llvm.org/)çš„å·¥ä½œï¼Œå¼¥åˆé«˜å±‚å’Œä½å±‚çš„IRè¡¨ç¤ºï¼Œä¹Ÿå³ç°åœ¨çš„Relay IRå’Œåº•å±‚TVMç”¨çš„Tensor Expression
- å…¨æ ˆè‡ªåŠ¨åŒ–ï¼šç°åœ¨AutoTVMåªæ˜¯åœ¨TVMç®—å­çš„å®ç°éƒ¨åˆ†å¯¹scheduleçš„å‚æ•°è¿›è¡Œæœç´¢ï¼Œå¸Œæœ›åšåˆ°å¯¹IRæœ¬èº«æˆ–è€…Scheduleæœ¬èº«ä¹Ÿè¿›è¡Œè‡ªåŠ¨åŒ–æœç´¢
- å…¶ä»–ï¼šé‡åŒ–ã€ä½ç²¾åº¦ã€è®­ç»ƒæ”¯æŒã€è‡ªåŠ¨å¾®åˆ†ç­‰

æ‰€ä»¥äº‹å®ä¸ŠTVMç»™äº†å¾ˆå¤šç§‘ç ”äººå‘˜å¤§é‡çš„ç ”ç©¶ç©ºé—´ï¼Œä¸ºæ·±åº¦å­¦ä¹ ç³»ç»Ÿæ¶æ„çš„ç ”ç©¶é“ºå¹³äº†é“è·¯ã€‚æœ€å…³é”®æ˜¯å®ƒçš„æ‰€æœ‰ä»£ç éƒ½æ˜¯å¼€æºçš„ï¼Œå› æ­¤ä»æºä»£ç å…¥æ‰‹ä¹Ÿå¯ä»¥ç€æ‰‹å¾ˆå¤šåº•å±‚çš„å·¥ä½œã€‚

## Related Posts

- [æ·±åº¦å­¦ä¹ é«˜å±‚æ¬¡ç»¼åˆ(HLS)ç³»ç»Ÿ ](https://chhzh123.github.io/blogs/2020-10-15-dnn-hls/)15 Oct 2020

- [Binary Neural Network (BNN) ](https://chhzh123.github.io/blogs/2020-05-01-bnn/)01 May 2020

- [TVM - Relay IR Pass ](https://chhzh123.github.io/blogs/2020-04-02-relay-ir-pass/)02 Apr 2020



## 2. TVM å®‰è£…

ä¸åŒç¯å¢ƒçš„å®‰è£…æ–¹æ³•å¯ä»¥å‚è€ƒtvmçš„å®˜ç½‘ï¼šhttps://docs.tvm.ai/install/index.html

å¯¹äºå®‰è£…ç¯å¢ƒï¼Œæˆ‘è¿˜æ˜¯å¼ºçƒˆæ¨èdockerçš„,ä¼šå°‘å¾ˆå¤šå‘ã€‚



## 3. TVM ä½¿ç”¨

TVMçš„ä½¿ç”¨å¯ä»¥é˜…è¯»ä¸€ä¸‹tvmæä¾›çš„tutorialsï¼šhttps://docs.tvm.ai/tutorials/

ä¸»è¦æ¨èä¸¤éƒ¨åˆ†ï¼š

- compile deep learning models
- auto tuning

å…¶å®ç®€å•çš„ä½¿ç”¨ä¸»è¦å°±æ˜¯è¿™ä¸¤å—å†…å®¹ï¼Œå¦‚æœä¸æƒ³ç»†ç ”ç©¶å…¶ä»£ç ï¼Œå¯ä»¥å°†å…¶å½“æˆä¸€ä¸ªå·¥å…·ä½¿ç”¨ï¼Œé€šè¿‡compile deep learning models,æ— è®ºä½ ä½¿ç”¨ä»€ä¹ˆæ ·çš„æ¡†æ¶ï¼Œéƒ½å¯ä»¥ç”Ÿæˆç»Ÿä¸€çš„æ¨¡å‹ï¼Œä¸€èˆ¬ä¼šç”Ÿæˆ3ä¸ªä¸œè¥¿å¦‚ä¸‹ï¼š



![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](imgs/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NodW5mZW5neWFueXVsb3Zl,size_16,color_FFFFFF,t_70.png)

è¿™é‡Œä¸€èˆ¬ä¼šåšä¸€äº›å±‚çš„èåˆç­‰æ“ä½œï¼Œé€Ÿåº¦ä¼šæœ‰ä¸€å®šçš„æå‡çš„ï¼Œä½†æ˜¯ä¸æ˜¯ç‰¹åˆ«å¤§ã€‚è¿™æ—¶å¦‚æœä½ éœ€è¦è¿›ä¸€æ­¥æé€Ÿå¯ä»¥è¯•è¯•**auto tuning**,è¿™éƒ¨åˆ†å¯ä»¥å‚è€ƒtutorialsä»¥åŠä¸‹é¢çš„ä¾‹å­ä»£ç ï¼Œauto-tuneçš„æ—¶é—´ä¸€èˆ¬æ¯”è¾ƒé•¿ï¼Œä½†æ˜¯æ•ˆæœè¿˜æ˜¯æ¯”è¾ƒæ˜¾è‘—çš„ï¼Œæœ¬åœ°æµ‹è¯•ï¼Œresnetåœ¨nvidia 1080tiä¸Šå¯ä»¥æé«˜3å€å·¦å³ã€‚

#### Demoä»£ç 

TVMçš„åŸç†å¾ˆå¤æ‚ä½†æ˜¯ä½¿ç”¨èµ·æ¥è¿˜æ˜¯æ¯”è¾ƒæ–¹ä¾¿çš„ï¼Œä¸‹é¢æ˜¯ä½¿ç”¨MXNetè¿›è¡ŒTVMè½¬æ¢çš„demoã€‚

**ä»£ç ä¸€ï¼šç”ŸæˆTVMæ¨¡å‹ã€‚**

```python
import tvm
from tvm import relay
from tvm.relay import testing
from tvm.contrib import graph_runtime
import mxnet as mx
from tvm.contrib.download import download_testdata
import numpy as np
import time

## load mxnet model
prefix = '/Models/resnetv1d-101'
epoch = 13
mx_sym, arg_params, aux_params = mx.model.load_checkpoint(prefix, epoch)
shape_dict = {'data': (1, 3, 224, 224)}

relay_func, relay_params = relay.frontend.from_mxnet(mx_sym, shape_dict,
        arg_params=arg_params, aux_params=aux_params)


target = 'cuda'
with relay.build_config(opt_level=3):
    graph, lib, params = relay.build(relay_func, target, params=relay_params)
# run forward



from PIL import Image
image = Image.open('test.jpg').resize((224, 224))
def transform_image(im):
    im = np.array(im).astype(np.float32)
    im = np.transpose(im, [2, 0, 1])
    im = im[np.newaxis, :]
    return im
x = transform_image(image)
# let's go
ctx = tvm.gpu(0)
dtype = 'float32'

m = graph_runtime.create(graph, lib, ctx)
## set input data
m.set_input('data', tvm.nd.array(x.astype(dtype)))
## set input params
m.set_input(**params)
t1 = time.time()
m.run()
t2 = time.time()
# get output
outputs = m.get_output(0)
top1 = np.argmax(outputs.asnumpy()[0])
print(outputs, str(t2-t1))

### evaluate inference time

ftimer = m.module.time_evaluator('run', ctx, number=1, repeat=100)
prof_res = np.array(ftimer().results) * 1000
print('time cost : mean:{}'.format(np.mean(prof_res)))





# save model

path_lib = '/Outputs/tvm/deploy_resnet101_v1d_lib.tar'
lib.export_library(path_lib)

with open('/Outputs/tvm/deploy_resnet101_v1d_graph.json', 'w') as f:
    f.write(graph)
with open('/Outputs/tvm/deploy_params', 'wb') as f:
    f.write(relay.save_param_dict(params))
    
    
# load model back

loaded_json = open('/Outputs/tvm/deploy_resnet101_v1d_graph.json').read()
loaded_lib = tvm.module.load(path_lib)
loaded_params = bytearray(open('/Outputs/tvm/deploy_params', 'rb').read())
module = graph_runtime.create(loaded_json, loaded_lib, ctx)
module.load_params(loaded_params)

tvm_data = tvm.nd.array(x.astype(dtype))
module.run(data=tvm_data)
outputs = module.get_output(0)
print(outputs)

```

**ä»£ç äºŒï¼šauto-tuning**

è¿™éƒ¨åˆ†è€—æ—¶è¾ƒé•¿ï¼Œä¸€ä¸ªresnet101æ¨¡å‹ï¼Œåœ¨1080tiä¸Šé¢å¯èƒ½è¦tune1åˆ°2å¤©çš„æ—¶é—´ã€‚

```python
import os

import numpy as np
import mxnet as mx
import tvm
from tvm import autotvm
from tvm import relay
import tvm.relay.testing
from tvm.autotvm.tuner import XGBTuner, GATuner, RandomTuner, GridSearchTuner
from tvm.contrib.util import tempdir
import tvm.contrib.graph_runtime as runtime
import argparse

def get_network(dtype, args):
    """Get the symbol definition and random weight of a network"""
    input_shape = (args.batch_size, 3, 224, 224)

    # if "resnet" in name:
    #     n_layer = int(name.split('-')[1])
    #     mod, params = relay.testing.resnet.get_workload(num_layers=n_layer, batch_size=batch_size, dtype=dtype)
    # elif "vgg" in name:
    #     n_layer = int(name.split('-')[1])
    #     mod, params = relay.testing.vgg.get_workload(num_layers=n_layer, batch_size=batch_size, dtype=dtype)
    # elif name == 'mobilenet':
    #     mod, params = relay.testing.mobilenet.get_workload(batch_size=batch_size, dtype=dtype)
    # elif name == 'squeezenet_v1.1':
    #     mod, params = relay.testing.squeezenet.get_workload(batch_size=batch_size, version='1.1', dtype=dtype)
    # elif name == 'inception_v3':
    #     input_shape = (1, 3, 299, 299)
    #     mod, params = relay.testing.inception_v3.get_workload(batch_size=batch_size, dtype=dtype)
    # elif name == 'mxnet':
        # an example for mxnet model
        # from mxnet.gluon.model_zoo.vision import get_model
        # block = get_model('resnet18_v1', pretrained=True)
    # else:
    #     raise ValueError("Unsupported network: " + name)

    prefix = '/Models/{}/{}'.format(args.version, args.model_name)
    epoch = args.model_index
    mx_sym, arg_params, aux_params = mx.model.load_checkpoint(prefix, epoch)

    mod, params = relay.frontend.from_mxnet(mx_sym, shape={'data': input_shape}, dtype=dtype, arg_params=arg_params,
                                            aux_params=aux_params)
    net = mod["main"]
    net = relay.Function(net.params, relay.nn.softmax(net.body), None, net.type_params, net.attrs)
    mod = relay.Module.from_expr(net)
    return mod, params, input_shape




# You can skip the implementation of this function for this tutorial.
def tune_tasks(tasks,
               measure_option,
               tuner='xgb',
               n_trial=1000,
               early_stopping=None,
               log_filename='tuning.log',
               use_transfer_learning=True,
               try_winograd=True):
    if try_winograd:
        for i in range(len(tasks)):
            try:  # try winograd template
                tsk = autotvm.task.create(tasks[i].name, tasks[i].args,
                                          tasks[i].target, tasks[i].target_host, 'winograd')
                input_channel = tsk.workload[1][1]
                if input_channel >= 64:
                    tasks[i] = tsk
            except Exception:
                pass

    # create tmp log file
    tmp_log_file = log_filename + ".tmp"
    if os.path.exists(tmp_log_file):
        os.remove(tmp_log_file)

    for i, tsk in enumerate(reversed(tasks)):
        prefix = "[Task %2d/%2d] " %(i+1, len(tasks))

        # create tuner
        if tuner == 'xgb' or tuner == 'xgb-rank':
            tuner_obj = XGBTuner(tsk, loss_type='rank')
        elif tuner == 'ga':
            tuner_obj = GATuner(tsk, pop_size=100)
        elif tuner == 'random':
            tuner_obj = RandomTuner(tsk)
        elif tuner == 'gridsearch':
            tuner_obj = GridSearchTuner(tsk)
        else:
            raise ValueError("Invalid tuner: " + tuner)

        if use_transfer_learning:
            if os.path.isfile(tmp_log_file):
                tuner_obj.load_history(autotvm.record.load_from_file(tmp_log_file))

        # do tuning
        n_trial = min(n_trial, len(tsk.config_space))
        tuner_obj.tune(n_trial=n_trial,
                       early_stopping=early_stopping,
                       measure_option=measure_option,
                       callbacks=[
                           autotvm.callback.progress_bar(n_trial, prefix=prefix),
                           autotvm.callback.log_to_file(tmp_log_file)])

    # pick best records to a cache file
    autotvm.record.pick_best(tmp_log_file, log_filename)
    os.remove(tmp_log_file)


def tune_and_evaluate(tuning_opt, target, log_file, dtype, args):
    # extract workloads from relay program
    print("Extract tasks...")
    mod, params, input_shape = get_network(dtype, args)
    tasks = autotvm.task.extract_from_program(mod["main"], target=target,
                                              params=params, ops=(relay.op.nn.conv2d,))

    # run tuning tasks
    print("Tuning...")
    tune_tasks(tasks, **tuning_opt)

    # compile kernels with history best records
    with autotvm.apply_history_best(log_file):
        print("Compile...")
        with relay.build_config(opt_level=3):
            graph, lib, params = relay.build_module.build(
                mod, target=target, params=params)

        # export library
        tmp = tempdir()
        filename = "/Outputs/tvm_autotuning/{}/{}_auto_tune_deploy_batch_{}_lib.tar".format(args.version,args.model_name, args.batch_size)
        lib.export_library(tmp.relpath(filename))

        with open('/Outputs/tvm_autotuning/{}/{}_auto_tune_deploy_batch_{}_graph.json'.format(args.version,args.model_name,args.batch_size) , 'w') as f:
            f.write(graph)
        with open('/Outputs/tvm_autotuning/{}/{}_auto_tune_deploy_batch_{}_params.params'.format(args.version,args.model_name,args.batch_size) , 'wb') as f:
            f.write(relay.save_param_dict(params))

        # load parameters
        ctx = tvm.context(str(target), 0)
        module = runtime.create(graph, lib, ctx)
        data_tvm = tvm.nd.array((np.random.uniform(size=input_shape)).astype(dtype))
        module.set_input('data', data_tvm)
        module.set_input(**params)

        # evaluate
        print("Evaluate inference time cost...")
        ftimer = module.module.time_evaluator("run", ctx, number=1, repeat=600)
        prof_res = np.array(ftimer().results) * 1000  # convert to millisecond
        print("Mean inference time (std dev): %.2f ms (%.2f ms)" %
              (np.mean(prof_res), np.std(prof_res)))

# We do not run the tuning in our webpage server since it takes too long.
# Uncomment the following line to run it by yourself.

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='score a model on a dataset')
    parser.add_argument('--version', type=str, default='porno')
    parser.add_argument('--model-name', type=str, default='resnetv1d-101-320x320')
    parser.add_argument('--model-index', type=int, default=16)
    parser.add_argument('--batch-size', type=int, default=1)
    parser.add_argument('--tag', type=str, default='')

    args = parser.parse_args()

    if not os.path.exists(os.path.join('/Outputs/tvm_autotuning/{}'.format(args.version))):
        os.mkdir(os.path.join('/Outputs/tvm_autotuning/{}'.format(args.version)))

    #### DEVICE CONFIG ####
    target = tvm.target.cuda()

    #### TUNING OPTION ####
    log_file = '/Outputs/tvm_autotuning/{}/{}_batch_{}.log'.format(args.version, args.model_name, args.batch_size)
    dtype = 'float32'

    tuning_option = {
        'log_filename': log_file,

        'tuner': 'xgb',
        'n_trial': 2000,
        'early_stopping': 600,

        'measure_option': autotvm.measure_option(
            builder=autotvm.LocalBuilder(timeout=10),
            runner=autotvm.LocalRunner(number=20, repeat=3, timeout=4, min_repeat_ms=150),
            # runner=autotvm.RPCRunner(
            #     '1080ti',  # change the device key to your key
            #     '0.0.0.0', 9190,
            #     number=20, repeat=3, timeout=4, min_repeat_ms=150)
        ),
    }


tune_and_evaluate(tuning_option, target, log_file, dtype, args)

```

#### TensorRT

è¿™é‡Œç®€å•ä»‹ç»ä¸€ä¸‹TensorRT,ä¹Ÿæ˜¯æ¨¡å‹åŠ é€Ÿçš„åˆ©å™¨ï¼Œå¹¶ä¸”tvmå’ŒtensorRTåšçš„å¯¹ä¸æ¨¡å‹å›¾çš„ä¼˜åŒ–éƒ½å·®ä¸å¤šï¼Œå¯ä»¥å‚è€ƒã€‚

TensorRTæ˜¯Nvidiaå‡ºå“çš„ç”¨äºå°†ä¸åŒæ¡†æ¶è®­ç»ƒçš„æ¨¡å‹éƒ¨ç½²åˆ°GPUçš„åŠ é€Ÿå¼•æ“ï¼Œå¯ä»¥è‡ªåŠ¨å°†ä¸åŒæ¡†æ¶çš„æ¨¡å‹è½¬æ¢ä¸ºTensorRTæ¨¡å‹ï¼Œå¹¶è¿›è¡Œæ¨¡å‹åŠ é€Ÿã€‚

TensorRTè¿›è¡Œæ¨¡å‹åŠ é€Ÿä¸»è¦æœ‰ä¸¤ç‚¹ï¼š

- TensorRTæ”¯æŒint8ä»¥åŠFP16è®¡ç®—
- TensorRTå¯¹ç½‘ç»œè¿›è¡Œé‡æ„ä»¥åŠä¼˜åŒ–:

> å»æ‰ç½‘ç»œä¸­çš„æ— ç”¨å±‚

> ç½‘ç»œç»“æ„çš„å‚ç›´æ•´åˆ

> ç½‘ç»œç»“æ„çš„æ°´å¹³èåˆ

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](imgs/as.png)

åŸå§‹ç½‘ç»œ



![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](imgs/asdf.png)

çºµå‘èåˆ



![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](imgs/asdfs.png)

æ¨ªå‘èåˆ

#### å‚è€ƒèµ„æ–™

[TVMå®˜ç½‘ï¼š https://tvm.ai/](https://tvm.ai/)

[TVMè®ºæ–‡ï¼šarxiv: https://arxiv.org/abs/1802.04799](https://arxiv.org/abs/1802.04799)

[tensorRTåŠ é€Ÿå‚è€ƒæ–‡çŒ®ï¼šhttps://blog.csdn.net/xh_hit/article/details/79769599](https://blog.csdn.net/xh_hit/article/details/79769599)

[Nvidiaå‚è€ƒæ–‡çŒ®ï¼šhttps://devblogs.nvidia.com/production-deep-learning-nvidia-gpu-inference-engine/](https://devblogs.nvidia.com/production-deep-learning-nvidia-gpu-inference-engine/)

