# TVMæ¨¡å‹éƒ¨ç½²

âŒšï¸:2020å¹´11æœˆ30æ—¥

ğŸ“šå‚è€ƒ

---



* [æ­å»ºTVMå¼€å‘ç¯å¢ƒ](#develop-env)
  * [ç¼–è¯‘LLVM](#build-llvm)
  * [ç¼–è¯‘TVM](#build-tvm)
* [æ¨¡å‹ç¼–è¯‘å‚æ•°è®²è§£](#build-parmas)
  * target
  * layout
  * ç¼–è¯‘å™¨
  * ç¤ºä¾‹ä»£ç 
* [ä¸åŒæ¨¡å‹çš„ç¼–è¯‘](#build)
  * [ç¼–è¯‘Tensorflowæ¨¡å‹](#build-tensorflow)
  * [ç¼–è¯‘ONNXæ¨¡å‹](#build-onnx)
  * [ç¼–è¯‘MXNetæ¨¡å‹](#build-mxnet)
  
* [æ¨¡å‹éƒ¨ç½²](#deploy)
  * [éƒ¨ç½²åˆ°PC CPU](#deploy-pc-cpu)
  * [éƒ¨ç½²åˆ°ARM CPU](#deploy-arm-cpu)
  * [éƒ¨ç½²åˆ°Android CPU](#deploy-android-cpu)
  
* [å…¶ä»–](#others)

TVMç›®å‰æœ‰ä¸ªç¼ºé™·å°±æ˜¯ä¸æ”¯æŒdynamic input shapesï¼Œæœ‰dynamic input shapes éœ€æ±‚çš„é€‰æ‹©å…¶ä»–æ¡†æ¶ã€‚ï¼ˆè¯»è€…çœ‹åˆ°è¿™é‡Œæ—¶å¯èƒ½å·²ç»æ”¯æŒäº†,è¿™é‡Œåªæ˜¯æé†’ä¸€ä¸‹ã€‚ï¼‰

TVMåŒ…å«ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼š
1ã€TVMç¼–è¯‘å™¨ï¼Œç”¨æ¥åšç¼–è¯‘å’Œä¼˜åŒ–ã€‚
2ã€TVMè¿è¡Œç¯å¢ƒï¼Œç”¨æ¥åœ¨ç›®æ ‡è®¾å¤‡ä¸Šè¿è¡Œã€‚
è¿˜å¯ä»¥ä½¿ç”¨RPCè¿›è¡Œè¿œç¨‹æµ‹è¯•å’Œä¼˜åŒ–ã€‚

<span id="develop-env">
<b>ä¸€ã€æ­å»ºTVMå¼€å‘ç¯å¢ƒ</b>
</span>

è¿™éƒ¨åˆ†è®²å¦‚ä½•åœ¨PCæ­å»ºTVMçš„å¼€å‘ç¯å¢ƒï¼Œå°±æ˜¯TVMç¼–è¯‘å™¨ã€‚

æœ€å°ç¼–è¯‘ä¾èµ–ï¼š
æ”¯æŒC++ 11çš„c++ç¼–è¯‘å™¨ï¼ˆg++4.8æˆ–æ›´é«˜ç‰ˆæœ¬ï¼‰
CMake 3.5 æˆ–æ›´é«˜ç‰ˆæœ¬
å¼ºçƒˆå»ºè®®ä½¿ç”¨LLVMç¼–è¯‘ä»¥æ‰“å¼€æ‰€æœ‰ç‰¹æ€§
å¦‚æœåªç”¨CUDA/OpenCLï¼Œå¯ä»¥ä¸ä¾èµ–LLVM
å¦‚æœä½¿ç”¨NNVMç¼–è¯‘å™¨ï¼Œéœ€è¦LLVM


éœ€è¦ç¼–è¯‘çš„ä¸»è¦æœ‰ä¸¤éƒ¨åˆ†LLVMå’ŒTVMã€‚è¿™é‡Œå±•ç¤ºå¦‚ä½•åœ¨Linuxä¸Šç¼–è¯‘ï¼ŒWindowsçš„ç¼–è¯‘æ–¹å¼å¯èƒ½ç¨æœ‰å·®å¼‚ã€‚
TVMMå¹¶ä¸æ˜¯ä¸€å®šä¾èµ–LLVMï¼Œä½†æ˜¯ç”±äºå¤§å¤šæ•°CPUä¸Šçš„éƒ¨ç½²éƒ½ä¾èµ–LLVMï¼Œæ‰€ä»¥è¿™é‡Œä¼šæ·»åŠ LLVMã€‚

<span id="build-llvm">
<b>1ã€ç¼–è¯‘LLVM</b>
</span>

  ä»å®˜ç½‘[LLVM Download Page](http://releases.llvm.org/download.html)ä¸‹è½½æºç 
  è§£å‹æºç ï¼Œåœ¨å’Œæºç å¹³çº§çš„ç›®å½•ä¸‹å»ºç›®å½•llvm-buildï¼ˆä¸å…è®¸åœ¨æºç å†…ç¼–è¯‘ï¼‰
  è¿›å…¥ç›®å½•llvm-build
  æ‰§è¡Œå‘½ä»¤

  ```
  cmake ../llvm-8.0.0.src
  ```
  å¦‚æœæŠ¥é”™è¯´gccç‰ˆæœ¬è¿‡ä½ï¼Œéœ€è¦åŠ ä¸Šå‚æ•°` DLLVM_TEMPORARILY_ALLOW_OLD_TOOLCHAIN=ON`
  ```
  cmake -DLLVM_TEMPORARILY_ALLOW_OLD_TOOLCHAIN=ON ../llvm-8.0.0.src
  ```
  è¿™ä¸€æ­¥æ‰§è¡Œå®Œæˆåä¼šç”ŸæˆMakefileæ–‡ä»¶ï¼Œæ‰§è¡Œå‘½ä»¤
  ```
  make
  ```
  ç¼–è¯‘å®Œæˆ.

<span id="build-tvm">
<b>2ã€ç¼–è¯‘TVM</b>
</span>

  cloneä»£ç  `git clone --recursive https://github.com/dmlc/tvm`
  è¿›å…¥æºç ç›®å½•æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
  ```
  mkdir build
  cp cmake/config.cmake build
  ```
  ç¼–è¾‘build/config.cmakeå®šåˆ¶ç¼–è¯‘é€‰é¡¹ï¼š
  (1). åœ¨macOS,å¯¹äºä¸€äº›ç‰ˆæœ¬å‘¢çš„Xcode,éœ€è¦åœ¨LDFLAGSä¸­æ·»åŠ  `-lc++abi`ï¼Œå¦åˆ™ä¼šæœ‰é“¾æ¥é”™è¯¯ã€‚
  (2). ä¿®æ”¹ `set(USE_CUDA OFF)` ä¸º `set(USE_CUDA ON)` æ‰“å¼€CUDAåç«¯ã€‚å…¶ä»–åç«¯å’Œåº“ï¼ˆOpenCLã€RCOMã€METALã€VULKANç­‰ï¼‰ä¹Ÿæ˜¯å¦‚æ­¤ã€‚
  æ ¹æ®æŸäº›é€‰é¡¹TVMä¼šä¾èµ–LLVMã€‚æœ‰äº›CPUå¹³å°çš„ç¼–è¯‘ä¼šéœ€è¦LLVMã€‚
  (1).å¦‚æœä¾èµ–LLVMï¼Œéœ€è¦ 4.0 æˆ–è€…æ›´é«˜ç‰ˆæœ¬ã€‚è®°ä½é»˜è®¤çš„LLVMç‰ˆæœ¬å¯èƒ½ä½äº4.0ã€‚
  (2).å› ä¸ºæºç ç¼–è¯‘LLVMä¼šèŠ±è´¹å¾ˆå¤šæ—¶é—´ï¼Œå¯ä»¥ä»[LLVM Download Page](http://releases.llvm.org/download.html)ä¸‹è½½é¢„ç¼–è¯‘ç‰ˆæœ¬ã€‚
    (a).è§£å‹åˆ°æŒ‡å®šç›®å½•ï¼Œä¿®æ”¹`build/config.cmake`æ·»åŠ `set(USE_LLVM /path/to/your/llvm/bin/llvm-config)`
    (b).ä¹Ÿå¯ä»¥ç›´æ¥è®¾ç½®`set(USE_LLVM ON)`è®©cmakeæœç´¢å¯ç”¨çš„LLVMã€‚
  (3).ä¹Ÿå¯ä»¥ä½¿ç”¨[LLVM Nightly Ubuntu Build](https://apt.llvm.org/),æ³¨æ„aptåŒ…è¦åœ¨`llvm-config`åé¢è·Ÿä¸Šç‰ˆæœ¬å·ã€‚æ¯”å¦‚ï¼Œ`set(LLVM_CONFIG llvm-config-4.0)`
  æ¥ä¸‹æ¥æ‰§è¡Œ
  ```
  cd build
  cmake ..
  make all
  ```
  å®‰è£…PythonåŒ…ï¼š
  pythonåŒ…ä½äºtvm/pythonï¼Œç›´æ¥è®¾ç½®ç¯å¢ƒå˜é‡ï¼š
  ```
  export TVM_HOME=/path/to/tvm
  export PYTHONPATH=$TVM_HOME/python:$TVM_HOME/topi/python:$TVM_HOME/nnvm/python:${PYTHONPATH}
  ```

<span id="build-parmas">
<b>ä¸€ã€æ¨¡å‹ç¼–è¯‘å‚æ•°è®²è§£</b>
</span>

è¿™ä¸€éƒ¨åˆ†è®²è§£æ¨¡å‹ç¼–è¯‘æ—¶çš„ä¸€äº›å…³é”®å‚æ•°ï¼Œç„¶åå±•ç¤ºå¦‚ä½•å°†Tensorlowæ¨¡å‹ç¼–è¯‘ç”ŸæˆåŠ¨æ€åº“ï¼Œå†åŠ è½½åŠ¨æ€åº“æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚

* 1.æŒ‡å®šç¼–è¯‘å‚æ•°target
  è¿™ä¸ªå‚æ•°åœ¨ç¼–è¯‘ç”Ÿæˆobjectæ–‡ä»¶(.oæ–‡ä»¶)æ—¶ç”¨åˆ°ï¼Œè¿™ä¸ªæ–‡ä»¶ä¼šæ”¾åˆ°/tmpç›®å½•ä¸‹ï¼Œå¦‚æœæƒ³è¦å¾—åˆ°è¿™ä¸ªæ–‡ä»¶ï¼Œå¯ä»¥æ‰§è¡Œ`lib.save(path) `ã€‚
  target ä¸­çš„å­—ç¬¦ä¸²é€‰é¡¹ï¼š
  llvm, å¤§å¤šæ•°CPU éƒ½éœ€è¦LLVM
  target, ç›®æ ‡ç¡¬ä»¶å¹³å°ï¼Œæ¯”å¦‚ -target=armv7l-linux-gnueabi
  mfloat-abi, è½¯ç¡¬æµ®ç‚¹é€‰é¡¹ï¼Œè¿™é‡Œåªæ”¯æŒsoft,hardä¸¤ä¸ªé€‰é¡¹ï¼Œé»˜è®¤hardã€‚Androidå·²ç»å…³é—­äº†VFPæ”¯æŒï¼Œæ‰€ä»¥éœ€è¦åœ¨è¿™é‡ŒæŒ‡å®šsoft
  å…¶ä»–é€‰é¡¹è¯¦è§TVMä»£ç ï¼Œåœ¨ llvm_common.cc ParseLLVMTargetOptionså‡½æ•°ã€‚ 

  ç¤ºä¾‹ï¼š
  ```
  target = "llvm" #éƒ¨ç½²åˆ°PC CPUä¸Š
  target = "llvm -target=armv7l-linux-gnueabi -mfloat-abi=soft" #éƒ¨ç½²åˆ°arm cpuä¸Šï¼Œå¹¶é€‰æ‹©è½¯æµ®ç‚¹
  target = tvm.target.create('llvm -device=arm_cpu -target=arm-linux-androideabi -mattr=+neon -mfloat-abi=soft') #éƒ¨ç½²åˆ°Android CPU
  ```
* 2.æŒ‡å®šlayout
  æœ‰äº›å¹³å°åªæ”¯æŒ"NCHW"é€šé“é¡ºåº, é»˜è®¤é¡ºåºæ˜¯"NHWC"ã€‚
* 3.æŒ‡å®šç¼–è¯‘å™¨
  ```
  lib.export_library(libpath, cc="/data/proj/FaceLandmark/tvm/gcc-linaro-4.8-2015.06-x86_64_arm-linux-gnueabi/bin/arm-linux-gnueabi-g++")
  ```
  é€šè¿‡å‚æ•°ccæŒ‡å®šç¼–è¯‘å™¨ï¼Œé»˜è®¤æ˜¯gccï¼Œå¦‚æœtargetæ˜¯PCåˆ™ä½¿ç”¨é»˜è®¤ç¼–è¯‘å™¨å³å¯ã€‚
  arm linux gcc äº¤å‰ç¼–è¯‘å™¨å¯ä»¥åœ¨linaroä¸‹è½½ï¼Œæ–°ç‰ˆæœ¬åœ°å€ï¼šhttps://releases.linaro.org/components/toolchain/binaries/  ï¼Œè€ç‰ˆæœ¬åœ°å€ï¼šhttp://releases.linaro.org/archive/14.04/components/toolchain/gcc-linaro/
  å¦‚æœè¦éƒ¨ç½²åˆ°Androidå¹³å°ï¼Œéœ€è¦ä½¿ç”¨NDKç¼–è¯‘ã€‚éœ€è¦å…ˆä¸‹è½½NDKï¼Œä¸‹è½½åœ°å€https://developer.android.com/ndk/downloads ,ç„¶åè®¾ç½®NDK C++ç¼–è¯‘å™¨åˆ°ç¯å¢ƒå˜é‡TVM_NDK_CC ä¸­ã€‚æ–°ç‰ˆæœ¬çš„NDKç¼–è¯‘å™¨å·²æ›´æ¢ä¸ºclang,ç¤ºä¾‹ï¼š
  ```
  export TVM_NDK_CC=YOUR_NDK_PATH/android-ndk-r19c/toolchains/llvm/prebuilt/linux-x86_64/bin/armv7a-linux-androideabi24-clang++
  ```
  ç¼–è¯‘ä»£ç ï¼š
  ```
  lib.export_library(libpath, ndk.create_shared, options=["-shared", "-fPIC"])
  ```

* 4.å®Œæ•´çš„ç¼–è¯‘ã€åŠ è½½ã€æ¨ç†ç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼š
  ```
  import tensorflow  as tf
  import tvm.relay.testing.tf as tf_testing
  from tvm.contrib import graph_runtime
  import tvm
  import tvm.relay as relay
  import cv2
  import os
  import numpy as np
  
  img_dir = "../test/lala"
  img_name = "img_414.jpg"
  img_path = os.path.join(img_dir, img_name)
  img = cv2.imread(img_path)
  img = (img-127.5)/128
  x = img[np.newaxis, :]
  x = np.array(x).astype("float32")
  print(x.shape)
  target = "llvm"
  # target = "llvm -target=armv7l-linux-gnueabihf"
  # target =  tvm.target.arm_cpu("pynq") 
  # target = "llvm -target=armv7l-linux-gnueabi -mfloat-abi=soft"
  # target = tvm.target.create('llvm')
  
  layout="NCHW"
  ctx = tvm.cpu(0)
  
  tf_model = "../graph/pnet_frozen_model.pb"
  
  with tf.gfile.FastGFile(tf_model, 'rb') as f:
      graph_def = tf.GraphDef()
      graph_def.ParseFromString(f.read())
      graph = tf.import_graph_def(graph_def, name='')
      # Call the utility to import the graph definition into default graph.
      # graph_def = tf_testing.ProcessGraphDefParam(graph_def)
      # # Add shapes to the graph.
      # with tf.Session() as sess:
      #     graph_def = tf_testing.AddShapesToGraphDef(sess, 'softmax')
  
  shape_dict = {"input_image": x.shape}
  outputs=["cls_prob", "bbox_pred", "landmark_pred"]
  sym, params = relay.frontend.from_tensorflow(graph_def, layout=layout, shape=shape_dict, outputs=outputs)
  
  with relay.build_config(opt_level=3):
      graph, lib, params = relay.build(sym, target=target, params=params)   
   dtype = 'float32'
  
    libpath = "./libpnet.so"
  
  
    # lib.save("lib.o")
  
    lib.export_library(libpath)
    graph_json_path = "pnet.json"
    with open(graph_json_path, 'w') as f:
        f.write(graph)
  
    param_path = "pnet.params"
    with open(param_path, 'wb') as f:
        f.write(relay.save_param_dict(params))
  
  
    loaded_json = open(graph_json_path).read()
    loaded_lib = tvm.module.load(libpath)
    loaded_params = bytearray(open(param_path, "rb").read())
  
    m = graph_runtime.create(loaded_json, loaded_lib, ctx)
    m.load_params(loaded_params)
  
    # set inputs
  
    m.set_input('input_image', tvm.nd.array(x.astype(dtype)))
  
    # execute
  
    m.run()
  
    # get outputs
  
    print("get_num_outputs: ",m.get_num_outputs())
  
    # cls_prob = m.get_output(0, tvm.nd.empty(((220, 151, 2)), 'float32'))
  
    cls_prob = m.get_output(0).asnumpy()
    bbox_pred = m.get_output(1).asnumpy()
    landmark_pred = m.get_output(2).asnumpy()
  
    print(cls_prob.shape)
    print(bbox_pred.shape)
    print(landmark_pred.shape)
  ```

 <span id="build">
<b>äºŒã€æ¨¡å‹ç¼–è¯‘</b>
</span>

è¿™éƒ¨åˆ†æè¿°å¦‚ä½•å°†è®­ç»ƒå¥½çš„æ¨¡å‹ç¼–è¯‘ä¸ºTVMæ¨¡å‹ï¼ŒTVMæ¨¡å‹ç”±ä¸‰ä¸ªæ–‡ä»¶ç»„æˆ deploy.soã€deploy.jsonã€deploy.paramsã€‚

<span id="build-tensorflow">
<b>1ã€ç¼–è¯‘Tensorflowæ¨¡å‹</b>
</span>

Tensorflowæ¨¡å‹è¦é¦–å…ˆè½¬æ¢ä¸ºfrozen modelæˆ–è€… tf-liteæ¨¡å‹æ‰èƒ½è¿›è¡Œç¼–è¯‘ï¼Œè¿™é‡Œåªå±•ç¤ºå¦‚ä½•è½¬æ¢ä¸ºfrozen modelã€‚è½¬æ¢ä¸ºfrozen modelçš„æ–¹æ³•ä¹Ÿä¸å”¯ä¸€ï¼Œè¿™é‡Œåªå±•ç¤ºæœ€ç®€å•çš„æ–¹æ³•ã€‚
è½¬æ¢ä»£ç ï¼š

    import numpy as np
    import tensorflow as tf
    import sys
    sys.path.append("../")
    from train_models.MTCNN_config import config
    from train_models.fast_mtcnn import P_Net_predict
    
    graph = tf.Graph()
    with graph.as_default():
    input_image = tf.placeholder(tf.float32, shape=[1, 12, 12, 3] , name="input_image")
    
    cls_prob, bbox_pred, _ = P_Net_predict(input_image)
    
    model_path = "../checkpoint/MTCNN_model/PNet_landmark-Adam/PNet-2"
    model_dict = '/'.join(model_path.split('/')[:-1])
    
    sess = tf.Session()
    saver = tf.train.Saver()
    
    ckpt = tf.train.get_checkpoint_state(model_dict)
    readstate = ckpt and ckpt.model_checkpoint_path
    assert  readstate, "the params dictionary is not valid"
    print ("restore models' param")
    saver.restore(sess, model_path)
    
    frozen_graphdef = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def, ["input_image","cls_prob","bbox_pred","landmark_pred"]) 
    with open('pnet_frozen_model.pb', 'wb') as f:
          f.write(frozen_graphdef.SerializeToString())
    #tflite_model = tf.contrib.lite.toco_convert(frozen_graphdef, [image_reshape], [cls_prob, bbox, landmark])
    #open("model.tflite", "wb").write(tflite_model)
```
ç„¶åå°±å¯ä»¥å°†frozen model ç¼–è¯‘ä¸ºTVM modelã€‚
ç¼–è¯‘ä»£ç ï¼š
```
```
import tensorflow  as tf
import tvm.relay.testing.tf as tf_testing
from tvm.contrib import graph_runtime
import tvm
import tvm.relay as relay
import cv2
import os
import numpy as np

img_dir = "../test/lala"
img_name = "img_414.jpg"
img_path = os.path.join(img_dir, img_name)
img = cv2.imread(img_path)
img = (img-127.5)/128
x = img[np.newaxis, :]
x = np.array(x).astype("float32")
print(x.shape)
target = "llvm"

# target = "llvm -target=armv7l-linux-gnueabihf"

# target =  tvm.target.arm_cpu("pynq") 

# target = "llvm -target=armv7l-linux-gnueabi -mfloat-abi=soft"

# target = tvm.target.create('llvm')

layout="NCHW"
ctx = tvm.cpu(0)

tf_model = "../graph/pnet_frozen_model.pb"

with tf.gfile.FastGFile(tf_model, 'rb') as f:
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(f.read())
    graph = tf.import_graph_def(graph_def, name='')
    # Call the utility to import the graph definition into default graph.
    # graph_def = tf_testing.ProcessGraphDefParam(graph_def)
    # # Add shapes to the graph.
    # with tf.Session() as sess:
    #     graph_def = tf_testing.AddShapesToGraphDef(sess, 'softmax')

shape_dict = {"input_image": x.shape}
outputs=["cls_prob", "bbox_pred", "landmark_pred"]
sym, params = relay.frontend.from_tensorflow(graph_def, layout=layout, shape=shape_dict, outputs=outputs)

with relay.build_config(opt_level=3):
    graph, lib, params = relay.build(sym, target=target, params=params)         


dtype = 'float32'

libpath = "./libpnet.so"

# lib.save("lib.o")

lib.export_library(libpath)
graph_json_path = "pnet.json"
with open(graph_json_path, 'w') as f:
    f.write(graph)

param_path = "pnet.params"
with open(param_path, 'wb') as f:
    f.write(relay.save_param_dict(params))
```

<span id="build-onnx">
<b>2ã€ç¼–è¯‘ONNXæ¨¡å‹</b>
</span>

å‚è€ƒï¼šhttps://docs.tvm.ai/tutorials/frontend/from_onnx.html
<span id="build-mxnet">
<b>3ã€ç¼–è¯‘MXNetæ¨¡å‹</b>
</span>

å‚è€ƒï¼šhttps://docs.tvm.ai/tutorials/frontend/from_mxnet.html
<span id="deploy">
<b>ä¸‰ã€æ¨¡å‹éƒ¨ç½²</b>
</span>

è¿™éƒ¨åˆ†è®²TVMæ¨¡å‹åœ¨ä¸åŒå¹³å°çš„éƒ¨ç½²ã€‚

<span id="deploy-pc-cpu">
<b>1ã€éƒ¨ç½²åˆ°PC CPU</b>
</span>

è¦å°†TVMæ¨¡å‹éƒ¨ç½²åˆ°PC CPUä¸Šéœ€è¦åœ¨ç¼–è¯‘æ¨¡å‹æ—¶æŒ‡å®š `target=llvm`ã€‚
Python ç¤ºä¾‹ä»£ç ï¼š

```

from tvm.contrib import graph_runtime
import tvm
import cv2
import os
import numpy as np
loaded_json = open(graph_json_path).read()
loaded_lib = tvm.module.load(libpath)
loaded_params = bytearray(open(param_path, "rb").read())

m = graph_runtime.create(loaded_json, loaded_lib, ctx)
m.load_params(loaded_params)

# set inputs

m.set_input('input_image', tvm.nd.array(x.astype(dtype)))

# execute

m.run()

get outputs

print("get_num_outputs: ",m.get_num_outputs())

cls_prob = m.get_output(0, tvm.nd.empty(((220, 151, 2)), 'float32'))

cls_prob = m.get_output(0).asnumpy()
bbox_pred = m.get_output(1).asnumpy()
landmark_pred = m.get_output(2).asnumpy()

print(cls_prob.shape)
print(bbox_pred.shape)
print(landmark_pred.shape)
```
<span id="deploy-arm-cpu">
<b>2ã€éƒ¨ç½²åˆ°ARM CPU</b>
</span>

éƒ¨ç½²åˆ°ARM CPUéœ€è¦åœ¨ç¼–è¯‘æ¨¡å‹æ—¶åœ¨targtä¸­æŒ‡å®šç›®æ ‡å¹³å°ï¼Œæ¯”å¦‚ 
`target = "llvm -target=armv7l-linux-gnueabi -mfloat-abi=soft"` ï¼ŒåŒæ—¶è¿˜è¦æŒ‡å®šäº¤å‰ç¼–è¯‘å™¨ã€‚
åœ¨ç›®æ ‡å¹³å°ä¸Šéœ€è¦æ­å»ºè¿è¡Œç¯å¢ƒï¼ŒåŒ…æ‹¬tvm_runtimå’Œllvmã€‚
llvmå¯ä»¥ä½¿ç”¨ç¼–è¯‘å¥½çš„åº“ï¼Œä¸‹è½½åœ°å€:http://releases.llvm.org/download.html
tvm_runtimeéœ€è¦åœ¨ARMä¸Šç¼–è¯‘:

```
git clone --recursive https://github.com/dmlc/tvm
cd tvm
mkdir build
cp cmake/config.cmake build   # è¿™é‡Œä¿®æ”¹config.cmakeä½¿å…¶æ”¯æŒllvm,é…ç½®ä¸Šllvmçš„è·¯å¾„
cd build
cmake ..
make runtime

```
æ­å»ºå¥½ç¯å¢ƒä¹‹åå¯ä»¥è¿è¡Œä»£ç äº†ã€‚
C++ä»£ç ç¤ºä¾‹(æ¥è‡ªå®˜æ–¹ï¼šhttps://docs.tvm.ai/deploy/nnvm.html)ï¼š

    #include <dlpack/dlpack.h>
    #include <tvm/runtime/module.h>
    #include <tvm/runtime/registry.h>
    #include <tvm/runtime/packed_func.h>
    
    #include <fstream>
    #include <iterator>
    #include <algorithm>
    
    int main()
    {
        // tvm module for compiled functions
        tvm::runtime::Module mod_syslib = tvm::runtime::Module::LoadFromFile("deploy.so");
    // json graph
    std::ifstream json_in("deploy.json", std::ios::in);
    std::string json_data((std::istreambuf_iterator<char>(json_in)), std::istreambuf_iterator<char>());
    json_in.close();
    
    // parameters in binary
    std::ifstream params_in("deploy.params", std::ios::binary);
    std::string params_data((std::istreambuf_iterator<char>(params_in)), std::istreambuf_iterator<char>());
    params_in.close();
    
    // parameters need to be TVMByteArray type to indicate the binary data
    TVMByteArray params_arr;
    params_arr.data = params_data.c_str();
    params_arr.size = params_data.length();
    
    int dtype_code = kDLFloat;
    int dtype_bits = 32;
    int dtype_lanes = 1;
    int device_type = kDLCPU;
    int device_id = 0;
    
    // get global function module for graph runtime
    tvm::runtime::Module mod = (*tvm::runtime::Registry::Get("tvm.graph_runtime.create"))(json_data, mod_syslib, device_type, device_id);
    
    DLTensor* x;
    int in_ndim = 4;
    int64_t in_shape[4] = {1, 3, 224, 224};
    TVMArrayAlloc(in_shape, in_ndim, dtype_code, dtype_bits, dtype_lanes, device_type, device_id, &x);
    // load image data saved in binary
    std::ifstream data_fin("cat.bin", std::ios::binary);
    data_fin.read(static_cast<char*>(x->data), 3 * 224 * 224 * 4);
    
    // get the function from the module(set input data)
    tvm::runtime::PackedFunc set_input = mod.GetFunction("set_input");
    set_input("data", x);
    
    // get the function from the module(load patameters)
    tvm::runtime::PackedFunc load_params = mod.GetFunction("load_params");
    load_params(params_arr);
    
    // get the function from the module(run it)
    tvm::runtime::PackedFunc run = mod.GetFunction("run");
    run();
    
    DLTensor* y;
    int out_ndim = 2;
    int64_t out_shape[2] = {1, 1000, };
    TVMArrayAlloc(out_shape, out_ndim, dtype_code, dtype_bits, dtype_lanes, device_type, device_id, &y);
    
    // get the function from the module(get output data)
    tvm::runtime::PackedFunc get_output = mod.GetFunction("get_output");
    get_output(0, y);
    
    // get the maximum position in output vector
    auto y_iter = static_cast<float*>(y->data);
    auto max_iter = std::max_element(y_iter, y_iter + 1000);
    auto max_index = std::distance(y_iter, max_iter);
    std::cout << "The maximum position in output vector is: " << max_index << std::endl;
    
    TVMArrayFree(x);
    TVMArrayFree(y);
    
    return 0;
}

<span id="deploy-android-cpu">
<b>2ã€éƒ¨ç½²åˆ°Android CPU</b>
</span>

éƒ¨ç½²åˆ°Androidç«¯éœ€è¦åœ¨ç¼–è¯‘æ—¶æŒ‡å®štargetä¸ºandroideabiï¼Œè½¯æµ®ç‚¹ï¼Œæ¯”å¦‚: 
```target = tvm.target.create('llvm -device=arm_cpu -target=arm-linux-androideabi -mattr=+neon -mfloat-abi=soft')```
åŒæ—¶ç¼–è¯‘æ—¶ä¸èƒ½ä½¿ç”¨arm linuxäº¤å‰ç¼–è¯‘å™¨ï¼Œè¦ä½¿ç”¨ndkã€‚è¯¦è§[æ¨¡å‹ç¼–è¯‘å‚æ•°è®²è§£](#build-parmas)ã€‚
ä¸‹é¢è®²çš„æ˜¯ç”¨TVM C++ APIéƒ¨ç½²ï¼Œç”¨Java APIéƒ¨ç½²å‚è€ƒ [TVM4J](https://github.com/dmlc/tvm/blob/master/apps/android_deploy/README.md#build-and-installation)
ä½¿ç”¨NDKç¼–è¯‘å‡ºçš„deploy.soä¼šä¾èµ–libc++_shared.soï¼Œè¿™ä¸ªæ–‡ä»¶åœ¨NDKç¼–è¯‘å™¨ä¸­ï¼Œæ‰¾åˆ°è¿™ä¸ªæ–‡ä»¶å’Œdeploy.soä¸€èµ·æ”¾åˆ°Android ä»£ç çš„jniLibsç›®å½•ä¸­ã€‚deploy.paramså’Œdeploy.jsonä½œä¸ºèµ„æºæ–‡ä»¶æ”¾åˆ°assetsæ–‡ä»¶å¤¹ä¸­ä½¿ç”¨æ—¶copyåˆ°SDCardä¸­ä½¿ç”¨ã€‚
è¿è¡Œæ—¶ç¯å¢ƒåŒæ ·åŒ…æ‹¬llvmå’Œtvm_runtimã€‚ç”±äºæ–°ç‰ˆæœ¬çš„Andrid Studioå·²ç»ä½¿ç”¨clang+llvmç¼–è¯‘å™¨ï¼Œæ‰€ä»¥Androidä¸­ä¼šè‡ªå¸¦llvmç¯å¢ƒï¼Œæˆ‘ä»¬åªéœ€è¦ç¼–è¯‘å‡ºtvm_runtimeã€‚
ç¼–è¯‘tvm_runtimeåªéœ€è¦å°†TVMæºç æ”¾å…¥Android Studioçš„jniä»£ç ç›®å½•ï¼Œåœ¨CMakeLists.txtä¸­åŠ å…¥ç¼–è¯‘tvm/apps/howto_deploy/tvm_runtime_pack.ccdçš„å‘½ä»¤å³å¯ç¼–è¯‘å¾—åˆ°tvm_runtimeã€‚

ç¼–è¯‘å‘½ä»¤ï¼š
`
add_library(tvm_runtime SHARED  src/main/cpp/tvm/apps/howto_deploy/tvm_runtime_pack.cc)
`

ç¼–è¯‘åå¯ä»¥åœ¨build/intermediates/cmakeï¼ˆåé¢è¿˜æœ‰å­ç›®å½•ï¼Œæ‰¾ä¸€ä¸‹ï¼‰ç›®å½•ä¸‹æ‰¾åˆ° libtvm_rumtime.soï¼Œå°†libtvm_rumtime.soå’Œdeploy.soæ”¾åˆ°ä¸€èµ·ã€‚
åˆ°è¿™é‡Œç¯å¢ƒä¹Ÿéƒ¨ç½²å¥½äº†ï¼Œæ¨ç†ä»£ç å¯å‚è€ƒ[éƒ¨ç½²åˆ°ARM CPU](#deploy-arm-cpu)ä¸­çš„C++ä»£ç ã€‚


<span id="others">
<b>å››ã€å…¶ä»–</b>
</span>

* 1ã€çº¿ç¨‹å®‰å…¨
å…³äºtvm_runtimeçš„çº¿ç¨‹å®‰å…¨ï¼š[https://discuss.tvm.ai/t/is-tvmruntime-thread-safe/84](https://discuss.tvm.ai/t/is-tvmruntime-thread-safe/84)