# Pytorchä¸­çš„æŸå¤±å‡½æ•°

âŒšï¸: 2020å¹´8æœˆ2æ—¥

ğŸ“šå‚è€ƒ

- [Pytorch-æŸå¤±å‡½æ•°](https://www.jianshu.com/p/7dbb9667e9e5)

---

åœ¨æ·±åº¦å­¦ä¹ ä¸­è¦ç”¨åˆ°å„ç§å„æ ·çš„æŸå¤±å‡½æ•°ï¼ˆloss functionï¼‰ï¼Œè¿™äº›æŸå¤±å‡½æ•°å¯çœ‹ä½œæ˜¯ä¸€ç§ç‰¹æ®Šçš„ layer ï¼ŒPyTorchä¹Ÿå°†è¿™äº›æŸå¤±å‡½æ•°å®ç°ä¸º nn.Module çš„å­ç±»ã€‚ç„¶è€Œåœ¨å®é™…ä½¿ç”¨ä¸­é€šå¸¸å°†è¿™äº› loss function ä¸“é—¨æå–å‡ºæ¥ï¼Œå’Œä¸»æ¨¡å‹äº’ç›¸ç‹¬ç«‹ã€‚

æˆ‘ä»¬æ‰€è¯´çš„ä¼˜åŒ–ï¼Œå³ä¼˜åŒ–ç½‘ç»œæƒå€¼ä½¿å¾—æŸå¤±å‡½æ•°å€¼å˜å°ã€‚ä½†æ˜¯ï¼ŒæŸå¤±å‡½æ•°å€¼å˜å°æ˜¯å¦èƒ½ä»£è¡¨æ¨¡å‹çš„åˆ†ç±»/å›å½’ç²¾åº¦å˜é«˜å‘¢ï¼Ÿé‚£ä¹ˆå¤šç§æŸå¤±å‡½æ•°ï¼Œåº”è¯¥å¦‚ä½•é€‰æ‹©å‘¢ï¼Ÿè¦è§£ç­”è¿™äº›å°±é¦–å…ˆè¦äº†è§£Pytorchä¸­çš„æŸå¤±å‡½æ•°éƒ½æœ‰å“ªäº›å’Œä»–ä»¬çš„æœºåˆ¶ï¼Œæ¥çœ‹ä¸€ä¸‹å§ã€‚

> 1.L1loss
>
> 2.MSELoss
>
> 3.CrossEntropyLoss
>
> 4.NLLLoss
>
> 5.PoissonNLLLoss
>
> 6.KLDivLoss
>
> 7.BCELoss
>
> 8.BCEWithLogitsLoss
>
> 9.MarginRankingLoss
>
> 10.HingeEmbeddingLoss
>
> 11.MultiLabelMarginLoss
>
> 12.SmoothL1Loss
>
> 13.SoftMarginLoss
>
> 14.MultiLabelSoftMarginLoss
>
> 15.CosineEmbeddingLoss
>
> 16.MultiMarginLoss
>
> 17.TripletMarginLoss
>
> 18.CTCLoss

**å€¼å¾—æ³¨æ„çš„æ˜¯**ï¼Œå¾ˆå¤šçš„ loss å‡½æ•°éƒ½æœ‰ size_average å’Œ reduce ä¸¤ä¸ªå¸ƒå°”ç±»å‹çš„å‚æ•°ï¼Œéœ€è¦è§£é‡Šä¸€ä¸‹ã€‚å› ä¸ºä¸€èˆ¬æŸå¤±å‡½æ•°éƒ½æ˜¯ç›´æ¥è®¡ç®— batch çš„æ•°æ®ï¼Œ**å› æ­¤è¿”å›çš„ loss ç»“æœéƒ½æ˜¯ç»´åº¦ä¸º (batch_size, ) çš„å¼ é‡**ã€‚

- å¦‚æœ reduce = Falseï¼Œé‚£ä¹ˆ size_average å‚æ•°å¤±æ•ˆï¼Œç›´æ¥è¿”å›å¼ é‡å½¢å¼çš„ lossï¼›ï¼ˆä¸ä¸‹æ–‡ä¸­çš„`size_average=None, reduce=None, reduction='none'`ä¸€è‡´ï¼‰
- å¦‚æœ reduce = Trueï¼Œé‚£ä¹ˆ loss è¿”å›çš„æ˜¯æ ‡é‡;
  1)å¦‚æœ size_average = Trueï¼Œè¿”å› loss.mean(); ï¼ˆä¸ä¸‹æ–‡ä¸­çš„`size_average=None, reduce=None, reduction='mean'`ä¸€è‡´ï¼‰
  2)å¦‚æœ size_average = Falseï¼Œè¿”å› loss.sum();ï¼ˆä¸ä¸‹æ–‡ä¸­çš„`size_average=None, reduce=None, reduction='sum'`ä¸€è‡´ï¼‰

## 1ã€L1èŒƒæ•°æŸå¤± L1Loss



```java
class torch.nn.L1Loss(size_average=None, reduce=None, reduction='mean')
```

- `reduction`æœ‰ä¸‰ä¸ªå–å€¼ï¼Œåˆ†åˆ«ä¸º`'none'`ã€`'mean'`ã€`'sum'`ï¼›
  `reduction`å–`'none'`ï¼Œåˆ™è¿”å› ä¸é¢„æµ‹å€¼æˆ–è€…çœŸå®å€¼å½¢çŠ¶ä¸€è‡´çš„ å¼ é‡ï¼›
  `reduction`ä¸å–`'mean'`æˆ–è€…`'sum'`ï¼Œåˆ™è¿”å› ä¸€ä¸ªæ ‡é‡å€¼ã€‚
- `size_average`å’Œ`reduce`éƒ½å–`None`ï¼Œä¸ä½œæ›´æ”¹ï¼Œè¿™æ˜¯æ¨èçš„æ–¹å¼ï¼›
  `size_average`å’Œ`reduce`ä¸¤è€…ä¸­æœ‰ä¸€ä¸ªä¸ä¸º`None`ï¼Œåˆ™ä¼šé‡å†™`reduction`ã€‚
- **æ€»ä¹‹ï¼Œæ¨èçš„æ–¹å¼æ˜¯ï¼š1ï¼‰`size_average`å’Œ`reduce`éƒ½å–`None`ï¼›2ï¼‰åŒæ—¶ï¼Œæ›´æ”¹`reduction`çš„å€¼ä»¥è¾¾åˆ°ä¸åŒçš„ç›®æ ‡**

### 1.1 åŠŸèƒ½

è®¡ç®—é¢„æµ‹å€¼ `x` and çœŸå®å€¼ `y`ä¹‹é—´çš„**å¹³å‡ç»å¯¹å€¼è¯¯å·®(MAE)**(mean absolute error).

### 1.2 å…¬å¼

1ï¼‰å½“`reduction = 'none'`:

![](imgs/21.png)

å…¶ä¸­ï¼Œ![N](https://math.jianshu.com/math?formula=N)ä¸ºæ‰¹é‡å¤§å°batch sizeã€‚



2ï¼‰å½“`reduction = 'mean'`æˆ–è€…`reduction = 'sum'`ï¼š

![](imgs/25.png)

![\ell(x, y) = \begin{cases} \operatorname{mean}(L), & \text{if reduction} = \text{'mean';}\\ \operatorname{sum}(L), & \text{if reduction} = \text{'sum'.} \end{cases}](https://math.jianshu.com/math?formula=%5Cell(x%2C%20y)%20%3D%20%5Cbegin%7Bcases%7D%20%5Coperatorname%7Bmean%7D(L)%2C%20%26%20%5Ctext%7Bif%20reduction%7D%20%3D%20%5Ctext%7B%27mean%27%3B%7D%5C%5C%20%5Coperatorname%7Bsum%7D(L)%2C%20%26%20%5Ctext%7Bif%20reduction%7D%20%3D%20%5Ctext%7B%27sum%27.%7D%20%5Cend%7Bcases%7D)

3ï¼‰é»˜è®¤æƒ…å†µï¼ˆå³`reduction='mean'`ï¼‰:

![](imgs/22.png)



### 1.3 ä»£ç 



```python
input_ = torch.empty(2, 3, dtype=torch.float).random_(0, 4)
target = torch.empty(2, 3, dtype=torch.float).random_(0, 4)
print(input_); print(target);
print(input_.size(), target.size())

print('=== mean ===')
loss_fn = torch.nn.L1Loss(reduce=None, size_average=None, reduction='mean')
loss = loss_fn(input_, target)
print(loss)
print(loss.size())

print('=== sum ===')
loss_fn = torch.nn.L1Loss(reduce=None, size_average=None, reduction='sum')
loss = loss_fn(input_, target)
print(loss)
print(loss.size())

print('=== none ===')
loss_fn = torch.nn.L1Loss(reduce=None, size_average=None, reduction='none')
loss = loss_fn(input_, target)
print(loss)
print(loss.size())
```

è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š

```csharp
tensor([[3., 0., 3.],
        [3., 0., 3.]])
tensor([[2., 0., 3.],
        [2., 0., 0.]])
torch.Size([2, 3]) torch.Size([2, 3])
=== mean ===
tensor(0.8333)
torch.Size([])
=== sum ===
tensor(5.)
torch.Size([])
=== none ===
tensor([[1., 0., 0.],
        [1., 0., 3.]])
torch.Size([2, 3])
```

- meanå’Œsumæ–¹å¼ä¸‹ï¼Œlosså€¼ä¸ºä¸€ä¸ªæ ‡é‡ï¼›
- noneæ–¹å¼ä¸‹ï¼Œlosså€¼ä¸ºå¼ é‡ï¼Œå½¢çŠ¶ä¸input_æˆ–è€…targetä¸€è‡´ï¼›
- ä»¥meanä¸ºä¾‹è®¡ç®—ï¼Œ(|3-2| + |0-0| + |3-3| + |3-2| + |0-0| + |3-0|) / 6 = 5 / 6 = 0.8333



æ€»ç»“ï¼šL1lossè¦æ±‚inputå’Œtargetå½¢çŠ¶ä¸€è‡´ã€‚

## 2ã€å‡æ–¹è¯¯å·®æŸå¤± MSELoss

```java
class torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')
```

- `reduction`æœ‰ä¸‰ä¸ªå–å€¼ï¼Œåˆ†åˆ«ä¸º`'none'`ã€`'mean'`ã€`'sum'`ï¼›
  `reduction`å–`'none'`ï¼Œåˆ™è¿”å› ä¸é¢„æµ‹å€¼æˆ–è€…çœŸå®å€¼å½¢çŠ¶ä¸€è‡´çš„ å¼ é‡ï¼›
  `reduction`ä¸å–`'mean'`æˆ–è€…`'sum'`ï¼Œåˆ™è¿”å› ä¸€ä¸ªæ ‡é‡å€¼ã€‚
- `size_average`å’Œ`reduce`éƒ½å–`None`ï¼Œä¸ä½œæ›´æ”¹ï¼Œè¿™æ˜¯æ¨èçš„æ–¹å¼ï¼›
  `size_average`å’Œ`reduce`ä¸¤è€…ä¸­æœ‰ä¸€ä¸ªä¸ä¸º`None`ï¼Œåˆ™ä¼šé‡å†™`reduction`ã€‚
- **æ€»ä¹‹ï¼Œæ¨èçš„æ–¹å¼æ˜¯ï¼š1ï¼‰`size_average`å’Œ`reduce`éƒ½å–`None`ï¼›2ï¼‰åŒæ—¶ï¼Œæ›´æ”¹`reduction`çš„å€¼ä»¥è¾¾åˆ°ä¸åŒçš„ç›®æ ‡**

### 2.1 åŠŸèƒ½

è®¡ç®—é¢„æµ‹å€¼ `x` and çœŸå®å€¼ `y`ä¹‹é—´çš„**å‡æ–¹è¯¯å·®(MSE)**(mean squared error (squared L2 norm)).

### 2.2 å…¬å¼

1ï¼‰å½“`reduction = 'none'`:

![](imgs/24.png)

å…¶ä¸­ï¼Œ![N](https://math.jianshu.com/math?formula=N)ä¸ºæ‰¹é‡å¤§å°batch sizeã€‚



2ï¼‰å½“`reduction = 'mean'`æˆ–è€…`reduction = 'sum'`ï¼š

![](imgs/26.png)

![](imgs/27.png)



3ï¼‰é»˜è®¤æƒ…å†µï¼ˆå³`reduction='mean'`ï¼‰:

![](imgs/28.png)



### 2.3 ä»£ç 



```python
input_ = torch.empty(2, 3, dtype=torch.float).random_(0, 4)
target = torch.empty(2, 3, dtype=torch.float).random_(0, 4)
print(input_); print(target);
print(input_.size(), target.size())

print('=== mean ===')
loss_fn = torch.nn.MSELoss(reduce=None, size_average=None, reduction='mean')
loss = loss_fn(input_, target)
print(loss)
print(loss.size())

print('=== sum ===')
loss_fn = torch.nn.MSELoss(reduce=None, size_average=None, reduction='sum')
loss = loss_fn(input_, target)
print(loss)
print(loss.size())

print('=== none ===')
loss_fn = torch.nn.MSELoss(reduce=None, size_average=None, reduction='none')
loss = loss_fn(input_, target)
print(loss)
print(loss.size())
```

è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š



```csharp
tensor([[2., 0., 3.],
        [3., 0., 1.]])
tensor([[2., 0., 0.],
        [0., 3., 0.]])
torch.Size([2, 3]) torch.Size([2, 3])
=== mean ===
tensor(4.6667)
torch.Size([])
=== sum ===
tensor(28.)
torch.Size([])
=== none ===
tensor([[0., 0., 9.],
        [9., 9., 1.]])
torch.Size([2, 3])
```

- meanå’Œsumæ–¹å¼ä¸‹ï¼Œlosså€¼ä¸ºä¸€ä¸ªæ ‡é‡ï¼›
- noneæ–¹å¼ä¸‹ï¼Œlosså€¼ä¸ºå¼ é‡ï¼Œå½¢çŠ¶ä¸input_æˆ–è€…targetä¸€è‡´ï¼›



æ€»ç»“ï¼šinputä¸targetå½¢çŠ¶ä¸€è‡´

## 3ã€äº¤å‰ç†µæŸå¤± CrossEntropyLoss



```python
class torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')
```

### 3.1 åŠŸèƒ½

è¯¥æ–¹æ³•å°†`nn.LogSoftmax()`å’Œ`nn.NLLLoss()`è¿›è¡Œäº†ç»“åˆã€‚ä¸¥æ ¼æ„ä¹‰ä¸Šçš„äº¤å‰ç†µæŸå¤±å‡½æ•°åº”è¯¥æ˜¯nn.NLLLoss()ã€‚**åœ¨ç±»åˆ«åˆ†å¸ƒä¸å¹³è¡¡çš„æ•°æ®é›†ä¸­å°¤å…¶æœ‰ç”¨**ã€‚

å…¶ä¸­ï¼Œ`nn.LogSoftmax()`å…¬å¼å¦‚ä¸‹ï¼š

![](imgs/29.png)

`nn.NLLLoss()`å…¬å¼:

![](imgs/30.png)

è¯¦ç»†è§ä¸‹æ–‡ğŸ‘‡ã€‚



### 3.2 å…¬å¼

1ï¼‰å½“ä¸æŒ‡æ˜æƒé‡æ—¶ï¼š

![](imgs/31.png)



2ï¼‰å½“æŒ‡æ˜æƒé‡æ—¶ï¼š

![](imgs/32.png)



### 3.3 ä»£ç 

```python
import torch
import torch.nn as nn
import numpy as np
import math

print('\n\n--------------------------------------------------- CrossEntropy loss: base')
loss_f = nn.CrossEntropyLoss(weight=None, size_average=True, reduce=False)

# ç”Ÿæˆç½‘ç»œè¾“å‡º ä»¥åŠ ç›®æ ‡è¾“å‡º
output = torch.ones(2, 3, requires_grad=True) * 0.5		# å‡è®¾ä¸€ä¸ªä¸‰åˆ†ç±»ä»»åŠ¡ï¼Œbatchsize=2ï¼Œå‡è®¾æ¯ä¸ªç¥ç»å…ƒè¾“å‡ºéƒ½ä¸º0.5
target = torch.from_numpy(np.array([0, 1])).type(torch.LongTensor)

print("---output---")
print(output)
print("---target---")
print(target)

loss = loss_f(output, target)
print('loss: ', loss)
print('ç”±äºreduce=Falseï¼Œæ‰€ä»¥å¯ä»¥çœ‹åˆ°æ¯ä¸€ä¸ªæ ·æœ¬çš„lossï¼Œè¾“å‡ºä¸º[1.0986, 1.0986]')



print('\n\n---------------------------------------------------  æ‰‹åŠ¨è®¡ç®—')
# ç†Ÿæ‚‰è®¡ç®—å…¬å¼ï¼Œæ‰‹åŠ¨è®¡ç®—ç¬¬ä¸€ä¸ªæ ·æœ¬
output = output[0].detach().numpy() # [0.5 0.5 0.5]
output_1 = output[0]              # 0.5 		ç¬¬ä¸€ä¸ªæ ·æœ¬çš„è¾“å‡ºå€¼
target_1 = target[0].numpy()			# 0

# ç¬¬ä¸€é¡¹
x_class = output[target_1]
# ç¬¬äºŒé¡¹
exp = math.e
sigma_exp_x = pow(exp, output[0]) + pow(exp, output[1]) + pow(exp, output[2])
log_sigma_exp_x = math.log(sigma_exp_x)
# ä¸¤é¡¹ç›¸åŠ 
loss_1 = -x_class + log_sigma_exp_x

print('ç¬¬ä¸€ä¸ªæ ·æœ¬çš„lossï¼š', loss_1)




print('\n\n--------------------------------------------------- CrossEntropy loss: weight')
weight = torch.from_numpy(np.array([0.6, 0.2, 0.2])).float()
loss_f = nn.CrossEntropyLoss(weight=weight, size_average=True, reduce=False)
output = torch.ones(2, 3, requires_grad=True) * 0.5  # å‡è®¾ä¸€ä¸ªä¸‰åˆ†ç±»ä»»åŠ¡ï¼Œbatchsizeä¸º2ä¸ªï¼Œå‡è®¾æ¯ä¸ªç¥ç»å…ƒè¾“å‡ºéƒ½ä¸º0.5
target = torch.from_numpy(np.array([0, 1])).type(torch.LongTensor)
loss = loss_f(output, target)

print('loss: ', loss)  #
print('åŸå§‹losså€¼ä¸º1.0986, ç¬¬ä¸€ä¸ªæ ·æœ¬æ˜¯ç¬¬0ç±»ï¼Œweight=0.6,æ‰€ä»¥è¾“å‡ºä¸º1.0986*0.6 =', 1.0986*0.6)




print('\n\n--------------------------------------------------- CrossEntropy loss: ignore_index')
loss_f_1 = nn.CrossEntropyLoss(weight=None, size_average=False, reduce=False, ignore_index=1)
loss_f_2 = nn.CrossEntropyLoss(weight=None, size_average=False, reduce=False, ignore_index=2)
output = torch.ones(3, 3, requires_grad=True) * 0.5  # å‡è®¾ä¸€ä¸ªä¸‰åˆ†ç±»ä»»åŠ¡ï¼Œbatchsizeä¸º2ä¸ªï¼Œå‡è®¾æ¯ä¸ªç¥ç»å…ƒè¾“å‡ºéƒ½ä¸º0.5
target = torch.from_numpy(np.array([0, 1, 2])).type(torch.LongTensor)

loss_1 = loss_f_1(output, target)
loss_2 = loss_f_2(output, target)


print('ignore_index = 1: ', loss_1)     # ç±»åˆ«ä¸º1çš„æ ·æœ¬çš„lossä¸º0
print('ignore_index = 2: ', loss_2)     # ç±»åˆ«ä¸º2çš„æ ·æœ¬çš„lossä¸º0
```

è¾“å‡ºç»“æœï¼š

```xml
--------------------------------------------------- CrossEntropy loss: base---output---tensor([[0.5000, 0.5000, 0.5000],        [0.5000, 0.5000, 0.5000]], grad_fn=<MulBackward0>)---target---tensor([0, 1])loss:  tensor([1.0986, 1.0986], grad_fn=<NllLossBackward>)ç”±äºreduce=Falseï¼Œæ‰€ä»¥å¯ä»¥çœ‹åˆ°æ¯ä¸€ä¸ªæ ·æœ¬çš„lossï¼Œè¾“å‡ºä¸º[1.0986, 1.0986]---------------------------------------------------  æ‰‹åŠ¨è®¡ç®—ç¬¬ä¸€ä¸ªæ ·æœ¬çš„lossï¼š 1.0986122886681098--------------------------------------------------- CrossEntropy loss: weightloss:  tensor([0.6592, 0.2197], grad_fn=<NllLossBackward>)åŸå§‹losså€¼ä¸º1.0986, ç¬¬ä¸€ä¸ªæ ·æœ¬æ˜¯ç¬¬0ç±»ï¼Œweight=0.6,æ‰€ä»¥è¾“å‡ºä¸º1.0986*0.6 = 0.65916--------------------------------------------------- CrossEntropy loss: ignore_indexignore_index = 1:  tensor([1.0986, 0.0000, 1.0986], grad_fn=<NllLossBackward>)ignore_index = 2:  tensor([1.0986, 1.0986, 0.0000], grad_fn=<NllLossBackward>)
```

### 3.4 å…¶ä»–

äº¤å‰ç†µæŸå¤±(cross-entropy Loss) åˆç§°ä¸ºå¯¹æ•°ä¼¼ç„¶æŸå¤±(Log-likelihood Loss)ã€å¯¹æ•°æŸå¤±ï¼›äºŒåˆ†ç±»æ—¶è¿˜å¯ç§°ä¹‹ä¸ºé€»è¾‘æ–¯è°›å›å½’æŸå¤±(Logistic Loss)ã€‚pytrochè¿™é‡Œä¸æ˜¯ä¸¥æ ¼æ„ä¹‰ä¸Šçš„äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œè€Œæ˜¯å…ˆå°†inputç»è¿‡softmaxæ¿€æ´»å‡½æ•°ï¼Œå°†å‘é‡â€œå½’ä¸€åŒ–â€æˆæ¦‚ç‡å½¢å¼ï¼Œç„¶åå†ä¸targetè®¡ç®—ä¸¥æ ¼æ„ä¹‰ä¸Šäº¤å‰ç†µæŸå¤±ã€‚ **åœ¨å¤šåˆ†ç±»ä»»åŠ¡ä¸­ï¼Œç»å¸¸é‡‡ç”¨softmaxæ¿€æ´»å‡½æ•°+äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œå› ä¸ºäº¤å‰ç†µæè¿°äº†ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒçš„å·®å¼‚ï¼Œç„¶è€Œç¥ç»ç½‘ç»œè¾“å‡ºçš„æ˜¯å‘é‡ï¼Œå¹¶ä¸æ˜¯æ¦‚ç‡åˆ†å¸ƒçš„å½¢å¼ã€‚æ‰€ä»¥éœ€è¦softmaxæ¿€æ´»å‡½æ•°å°†ä¸€ä¸ªå‘é‡è¿›è¡Œâ€œå½’ä¸€åŒ–â€æˆæ¦‚ç‡åˆ†å¸ƒçš„å½¢å¼ï¼Œå†é‡‡ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°è®¡ç®—lossã€‚** å†å›é¡¾PyTorchçš„CrossEntropyLoss()ï¼Œå®˜æ–¹æ–‡æ¡£ä¸­æåˆ°æ—¶å°†nn.LogSoftmax()å’Œ nn.NLLLoss()è¿›è¡Œäº†ç»“åˆï¼Œnn.LogSoftmax() ç›¸å½“äºæ¿€æ´»å‡½æ•° ï¼Œ nn.NLLLoss()æ˜¯æŸå¤±å‡½æ•°ï¼Œå°†å…¶ç»“åˆï¼Œå®Œæ•´çš„æ˜¯å¦å¯ä»¥å«åšsoftmax+äº¤å‰ç†µæŸå¤±å‡½æ•°å‘¢ï¼Ÿ

æ€»ç»“ï¼šinput ï¼ˆNï¼ŒCï¼‰ï¼Œtargetï¼ˆNï¼‰ï¼Œoutputï¼ˆNï¼‰ï¼Œ

## 4ã€è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤± NLLLoss

NLLLossçš„å…¨ç§°æ˜¯Negative Log Likelihood Loss,ä¸­æ–‡åç§°æ˜¯æœ€å¤§ä¼¼ç„¶æˆ–è€…logä¼¼ç„¶ä»£ä»·å‡½æ•°ã€‚ CrossEntropyLossæ˜¯äº¤å‰ç†µä»£ä»·å‡½æ•°ã€‚

```python
class torch.nn.NLLLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')
```

- `size_average`ã€`reduce`ã€`reduction`è¿™ä¸ªä¸‰ä¸ªå‚æ•°ä¸å†èµ˜è¿°ï¼Œä¸L1Lossã€MSELossä¸€è‡´ï¼›
- `weight`: å¦‚æœæä¾›ï¼Œåˆ™åº”è¯¥æ˜¯ ç”±æ¯ä¸ªç±»åˆ«çš„æƒé‡ç»„æˆçš„ä¸€ç»´å¼ é‡ï¼Œä¾‹å¦‚æœ‰![Aã€Bã€C](https://math.jianshu.com/math?formula=A%E3%80%81B%E3%80%81C)ä¸‰ä¸ªç±»åˆ«ï¼Œåˆ™weightå¯ä¸ºweight=[0.6, 0.3, 0.1]ï¼Œå…¶ä¸­weightçš„é•¿åº¦ä¸ç±»åˆ«æ•°ç›¸ç­‰ï¼›å¦‚æœæ²¡æœ‰æä¾›ï¼Œæ‰€æœ‰æƒé‡ç½®ä¸º1ï¼›
- `ignore_index`ï¼šå¿½ç•¥æŸä¸€ç±»åˆ«ï¼Œä¸è®¡ç®—å…¶lossï¼Œå…¶lossä¼šä¸º0ï¼Œå¹¶ä¸”ï¼Œåœ¨é‡‡ç”¨size_averageæ—¶ï¼Œä¸ä¼šè®¡ç®—é‚£ä¸€ç±»çš„lossï¼Œé™¤çš„æ—¶å€™çš„åˆ†æ¯ä¹Ÿä¸ä¼šç»Ÿè®¡é‚£ä¸€ç±»çš„æ ·æœ¬ï¼›
- NLLLossçš„è¾“å…¥æ˜¯**æ¯ä¸€ç±»åˆ«çš„log-probabilitiesï¼Œè¯¥log-probabilitieså¯ä»¥é€šè¿‡`LogSoftmax`ç½‘ç»œå±‚è·å¾—**ï¼Œå¦‚æœä¸æƒ³è®©ç½‘ç»œçš„æœ€åä¸€å±‚æ˜¯ log_softmax å±‚çš„è¯ï¼Œå°±å¯ä»¥é‡‡ç”¨ CrossEntropyLoss å®Œå…¨ä»£æ›¿æ­¤å‡½æ•°ï¼Œ**å› ä¸ºCrossEntropyLoss ä¸­å°±æœ‰è¿™äº›æ­¥éª¤**ã€‚

### 4.1 åŠŸèƒ½

åœ¨**ç±»åˆ«åˆ†å¸ƒä¸å¹³è¡¡çš„æ•°æ®é›†**ä¸­å°¤å…¶æœ‰ç”¨ã€‚

### 4.2 å…¬å¼

1ï¼‰å½“`reduction = 'none'`:

![](imgs/390.png)

å…¶ä¸­ï¼Œ![N](https://math.jianshu.com/math?formula=N)ä¸ºæ‰¹é‡å¤§å°batch sizeï¼›![](imgs/33.png)è¡¨ç¤ºä¸€ä¸ªæ‰¹é‡ä¸­çš„ç¬¬$n$ä¸ªæ ·æœ¬ï¼›$l_n$è¡¨ç¤ºç¬¬$n$ä¸ªæ ·æœ¬çš„æŸå¤±å€¼ï¼›$y_n$è¡¨ç¤ºç¬¬$n$ä¸ªæ ·æœ¬çš„çœŸå®æ ‡ç­¾ï¼ˆä¾‹å¦‚æ˜¯ç±»åˆ«2ï¼Œåˆ™$y_n=2$ï¼›$w_{y_n}$è¡¨ç¤ºç±»åˆ«$y_n$çš„æƒé‡ã€‚    

2ï¼‰å½“`reduction = 'mean'`æˆ–è€…`reduction = 'sum'`ï¼š

![](imgs/34.png)

### 4.3 shape

- Input: `(N, C)` where `C = number of classes` or `(N, C, d_1, d_2, ..., d_K)` with $K \geq 1$in the case of `K`- dimensional loss.

- Target: `(N)` where each value is $0 \leq \text{targets}[i] \leq C-1$  or  `(N, d_1, d_2, ..., d_K)` with $K \geq 1$in the case of `K`- dimensional loss.

- Output: scalar.

  If `reduction` is `'none'`, then the same size as the target: `(N)` or `(N, d_1, d_2, ..., d_K)` with$ K \geq 1$in the case of `K`- dimensional loss.

- **ä¼šå‘ç°Target ä¸ Outputçš„å½¢çŠ¶ä¸€è‡´ï¼Œä½†Inputè¦æ¯”å‰ä¸¤è€…å¤šä¸€ä¸ªç±»åˆ«é€šé“`(C)`**

#### 4.4 ç†è§£$f(x, class) = -x[class]$

ä»¥ä¸‰åˆ†ç±»ä»»åŠ¡ä¸ºä¾‹ï¼Œç±»åˆ«æ ‡å·ä¸º0ã€1ã€2ã€‚NLLLoss çš„è¾“å…¥input=[-1.233, 2.657, 0.534]ï¼Œ çœŸå®æ ‡ç­¾ä¸º2ï¼ˆclass=2ï¼‰ï¼Œåˆ™$loss = f(x, class) = -x[class] = -input[2]$= -0.534ï¼ˆæœ‰ç‚¹åˆ—è¡¨åˆ‡ç‰‡çš„æ„æ€ï¼‰ã€‚

### 4.5 ä¸¾ä¾‹

1ï¼‰ å½“`reduction = 'none'`ï¼Œ

- ç†è®ºè®¡ç®—

ä½¿ç”¨å¦‚ä¸‹å…¬å¼ï¼š

![](imgs/35.png)



ä»¥ä¸‰åˆ†ç±»ä»»åŠ¡ä¸ºä¾‹ï¼Œç±»åˆ«æ ‡å·ä¸º0ã€1ã€2ã€‚NLLLoss çš„è¾“å…¥inputä¸º[[0.6, 0.2, 0.2], [0.4, 1.2, 0.4]]ï¼ˆå³ä¸¤ä¸ªæ ·æœ¬åˆ†åˆ«åœ¨ä¸‰ä¸ªç±»åˆ«çš„é¢„æµ‹æ¦‚ç‡å€¼ï¼‰ï¼ŒçœŸå®æ ‡ç­¾target= [0, 1]ï¼Œ**ä¸æŒ‡å®šæƒé‡ï¼ˆåˆ™æƒé‡å…¨ä¸º1ï¼‰**ã€‚

è¿™ä¸ªæ‰¹é‡æœ‰ä¸¤ä¸ªæ ·æœ¬ï¼ˆ![N=2](https://math.jianshu.com/math?formula=N%3D2)ï¼‰ï¼Œæ‰€ä»¥$L = \{l_1, l_2\}^\top$ï¼›

çœŸå®æ ‡ç­¾åˆ—è¡¨ä¸ºtarget= [0, 1]ï¼Œå³ç¬¬ä¸€ä¸ªæ ·æœ¬çš„çœŸå®æ ‡ç­¾ä¸ºç±»åˆ«0ï¼Œç¬¬äºŒä¸ªæ ·æœ¬çš„çœŸå®æ ‡ç­¾ä¸ºç±»åˆ«1ï¼Œæ‰€ä»¥$y_1 = 0$ã€![y_2= 1](https://math.jianshu.com/math?formula=y_2%3D%201)ï¼›

æƒé‡æ²¡æœ‰æŒ‡å®šï¼ˆåˆ™weight = [1, 1, 1]ï¼‰ï¼Œæ‰€ä»¥![w_{y_1} = w_0 = \text {weight}[0] = 1](https://math.jianshu.com/math?formula=w_%7By_1%7D%20%3D%20w_0%20%3D%20%5Ctext%20%7Bweight%7D%5B0%5D%20%3D%201)ï¼ŒåŒç†ï¼Œ![w_{y_2} = w_1 = \text {weight}[1] = 1](https://math.jianshu.com/math?formula=w_%7By_2%7D%20%3D%20w_1%20%3D%20%5Ctext%20%7Bweight%7D%5B1%5D%20%3D%201)ï¼›

![x_{1, y_1}](https://math.jianshu.com/math?formula=x_%7B1%2C%20y_1%7D) è¡¨ç¤ºç¬¬ä¸€ä¸ªæ ·æœ¬ä¸­![y_1](https://math.jianshu.com/math?formula=y_1)å¯¹åº”çš„å€¼ï¼Œå³![x_{1, y_1}= [0.6, 0.2, 0.2][y_1]=[0.6, 0.2, 0.2][0]=0.6](https://math.jianshu.com/math?formula=x_%7B1%2C%20y_1%7D%3D%20%5B0.6%2C%200.2%2C%200.2%5D%5By_1%5D%3D%5B0.6%2C%200.2%2C%200.2%5D%5B0%5D%3D0.6)ï¼ŒåŒç†ï¼Œ![x_{2, y_2}=1.2](https://math.jianshu.com/math?formula=x_%7B2%2C%20y_2%7D%3D1.2);

æ‰€ä»¥ ![l_1 = - w_{y_1} x_{1,y_1} = - 1 * 0.6 = -0.6](https://math.jianshu.com/math?formula=l_1%20%3D%20-%20w_%7By_1%7D%20x_%7B1%2Cy_1%7D%20%3D%20-%201%20*%200.6%20%3D%20-0.6)ï¼ŒåŒç†ï¼Œ![l_2 = -1 * 1.2 = -1.2](https://math.jianshu.com/math?formula=l_2%20%3D%20-1%20*%201.2%20%3D%20-1.2)

å› æ­¤ï¼Œ![\ell(x, y) = L = \{-0.6, -1.2\} ^\top](https://math.jianshu.com/math?formula=%5Cell(x%2C%20y)%20%3D%20L%20%3D%20%5C%7B-0.6%2C%20-1.2%5C%7D%20%5E%5Ctop)ã€‚

- ä»£ç å®ç°ï¼š



```python
import torch
import torch.nn as nn
import numpy as np

# ----------------------------------- log likelihood loss
# ç”Ÿæˆç½‘ç»œè¾“å‡º ä»¥åŠ ç›®æ ‡è¾“å‡º
output = torch.from_numpy(np.array([[0.6, 0.2, 0.2], [0.4, 1.2, 0.4]])).float()  
output.requires_grad = True
target = torch.from_numpy(np.array([0, 1])).type(torch.LongTensor)

loss_f = nn.NLLLoss(weight=None, size_average=None, reduce=None, reduction='none')
loss = loss_f(output, target)

print('\nloss: \n', loss)
print('\nç¬¬ä¸€ä¸ªæ ·æœ¬æ˜¯0ç±»ï¼Œloss = -(1*0.6)={}'.format(loss[0]))
print('\nç¬¬äºŒä¸ªæ ·æœ¬æ˜¯1ç±»ï¼Œloss = -(1*1.2)={}'.format(loss[1]))
```

è¾“å‡ºç»“æœï¼š



```jsx
loss: 
 tensor([-0.6000, -1.2000], grad_fn=<NllLossBackward>)
ç¬¬ä¸€ä¸ªæ ·æœ¬æ˜¯0ç±»ï¼Œloss = -(1*0.6)=-0.6000000238418579
ç¬¬äºŒä¸ªæ ·æœ¬æ˜¯1ç±»ï¼Œloss = -(1*1.2)=-1.2000000476837158
```

2ï¼‰å½“`reduction = 'mean'`ï¼Œä¸”å¸¦ä¸Šæƒé‡

- ç†è®ºè®¡ç®—

![l_n = - w_{y_n} x_{n,y_n}, \quad w_{c} = \text{weight}[c] \cdot \mathbb{1}\{c \not= \text{ignore_index}\},](https://math.jianshu.com/math?formula=l_n%20%3D%20-%20w_%7By_n%7D%20x_%7Bn%2Cy_n%7D%2C%20%5Cquad%20w_%7Bc%7D%20%3D%20%5Ctext%7Bweight%7D%5Bc%5D%20%5Ccdot%20%5Cmathbb%7B1%7D%5C%7Bc%20%5Cnot%3D%20%5Ctext%7Bignore_index%7D%5C%7D%2C)

![\ell(x, y) = \begin{cases} \sum_{n=1}^N \frac{1}{\sum_{n=1}^N w_{y_n}} l_n, & \text{if reduction} = \text{'mean';}\\ \sum_{n=1}^N l_n, & \text{if reduction} = \text{'sum'.} \end{cases}](https://math.jianshu.com/math?formula=%5Cell(x%2C%20y)%20%3D%20%5Cbegin%7Bcases%7D%20%5Csum_%7Bn%3D1%7D%5EN%20%5Cfrac%7B1%7D%7B%5Csum_%7Bn%3D1%7D%5EN%20w_%7By_n%7D%7D%20l_n%2C%20%26%20%5Ctext%7Bif%20reduction%7D%20%3D%20%5Ctext%7B%27mean%27%3B%7D%5C%5C%20%5Csum_%7Bn%3D1%7D%5EN%20l_n%2C%20%26%20%5Ctext%7Bif%20reduction%7D%20%3D%20%5Ctext%7B%27sum%27.%7D%20%5Cend%7Bcases%7D)

ä»¥ä¸‰åˆ†ç±»ä»»åŠ¡ä¸ºä¾‹ï¼Œç±»åˆ«æ ‡å·ä¸º0ã€1ã€2ã€‚NLLLoss çš„è¾“å…¥inputä¸º[[0.6, 0.2, 0.2], [0.4, 1.2, 0.4]]ï¼ˆå³ä¸¤ä¸ªæ ·æœ¬åˆ†åˆ«åœ¨ä¸‰ä¸ªç±»åˆ«çš„é¢„æµ‹æ¦‚ç‡å€¼ï¼‰ï¼ŒçœŸå®æ ‡ç­¾target= [0, 1]ï¼Œ**æŒ‡å®šæƒå€¼weight = [0.6, 0.2, 0.2]**ã€‚

åˆ™æœ‰ï¼Œ
![l_1 = - 0.6 * 0.6 = -0.36](https://math.jianshu.com/math?formula=l_1%20%3D%20-%200.6%20*%200.6%20%3D%20-0.36)
![l_2 = - 0.2 * 1.2 = -0.24](https://math.jianshu.com/math?formula=l_2%20%3D%20-%200.2%20*%201.2%20%3D%20-0.24)ï¼›

å› ä¸º`reduction = 'mean'`ï¼Œæ‰€ä»¥![\ell(x, y)=\frac{1}{0.6 + 0.2} * (-0.36) + \frac{1}{0.6 + 0.2} * (-0.24) = -0.75](https://math.jianshu.com/math?formula=%5Cell(x%2C%20y)%3D%5Cfrac%7B1%7D%7B0.6%20%2B%200.2%7D%20*%20(-0.36)%20%2B%20%5Cfrac%7B1%7D%7B0.6%20%2B%200.2%7D%20*%20(-0.24)%20%3D%20-0.75)ã€‚

- ä»£ç å®ç°

```python
import torch
import torch.nn as nn
import numpy as np

# ----------------------------------- log likelihood loss
# å„ç±»åˆ«æƒé‡
weight = torch.from_numpy(np.array([0.6, 0.2, 0.2])).float()

# ç”Ÿæˆç½‘ç»œè¾“å‡º ä»¥åŠ ç›®æ ‡è¾“å‡º
output = torch.from_numpy(np.array([[0.6, 0.2, 0.2], [0.4, 1.2, 0.4]])).float()  
output.requires_grad = True
target = torch.from_numpy(np.array([0, 1])).type(torch.LongTensor)

loss_f = nn.NLLLoss(weight=weight, size_average=None, reduce=None, reduction='mean')
loss = loss_f(output, target)

print('\nloss: \n', loss)
```

è¾“å‡ºç»“æœï¼š

```jsx
loss:  tensor(-0.7500, grad_fn=<NllLossBackward>)
```

## 5ã€ç›®æ ‡å€¼ä¸ºæ³Šæ¾åˆ†å¸ƒçš„è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±PoissonNLLLoss



```java
class torch.nn.PoissonNLLLoss(log_input=True, full=False, size_average=None, eps=1e-08, reduce=None, reduction='mean')
```

- `log_input (bool, optional):` å¦‚æœä¸ºTrueï¼Œä½¿ç”¨![\exp(\text{input}) - \text{target}*\text{input}](https://math.jianshu.com/math?formula=%5Cexp(%5Ctext%7Binput%7D)%20-%20%5Ctext%7Btarget%7D*%5Ctext%7Binput%7D)ï¼›å¦‚æœä¸ºFalseï¼Œä½¿ç”¨![\text{input} - \text{target}*\log(\text{input}+\text{eps})](https://math.jianshu.com/math?formula=%5Ctext%7Binput%7D%20-%20%5Ctext%7Btarget%7D*%5Clog(%5Ctext%7Binput%7D%2B%5Ctext%7Beps%7D))
- `full (bool, optional):` æ˜¯å¦è®¡ç®—å…¨éƒ¨çš„lossã€‚ä¾‹å¦‚ï¼Œå½“é‡‡ç”¨æ–¯ç‰¹æ—å…¬å¼è¿‘ä¼¼é˜¶ä¹˜é¡¹æ—¶ï¼Œé˜¶ä¹˜é¡¹è¿‘ä¼¼ä¸º![\text{target}*\log(\text{target}) - \text{target} + 0.5 * \log(2\pi\text{target})](https://math.jianshu.com/math?formula=%5Ctext%7Btarget%7D*%5Clog(%5Ctext%7Btarget%7D)%20-%20%5Ctext%7Btarget%7D%20%2B%200.5%20*%20%5Clog(2%5Cpi%5Ctext%7Btarget%7D))

![](imgs/36.png)

- `eps(float)`ï¼š å½“log_input = Falseæ—¶ï¼Œç”¨æ¥é˜²æ­¢è®¡ç®—log(0)ï¼Œè€Œå¢åŠ çš„ä¸€ä¸ªä¿®æ­£é¡¹ï¼Œå³ loss(input,target)=input - target * log(input+eps)ã€‚é»˜è®¤ä¸º![1e^{-8}](https://math.jianshu.com/math?formula=1e%5E%7B-8%7D)

### 5.1 åŠŸèƒ½

ç›®æ ‡å€¼ä¸ºæ³Šæ¾åˆ†å¸ƒçš„è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±

### 5.2 å…¬å¼

![\text{target} \sim \mathrm{Poisson}(\text{input})](https://math.jianshu.com/math?formula=%5Ctext%7Btarget%7D%20%5Csim%20%5Cmathrm%7BPoisson%7D(%5Ctext%7Binput%7D))

![\text{loss}(\text{input}, \text{target})= \text{input} - \text{target} * \log(\text{input})+ \log(\text{target!})](https://math.jianshu.com/math?formula=%5Ctext%7Bloss%7D(%5Ctext%7Binput%7D%2C%20%5Ctext%7Btarget%7D)%3D%20%5Ctext%7Binput%7D%20-%20%5Ctext%7Btarget%7D%20*%20%5Clog(%5Ctext%7Binput%7D)%2B%20%5Clog(%5Ctext%7Btarget!%7D))

å…¶ä¸­ï¼Œä¸Šå¼çš„æœ€åä¸€é¡¹èƒ½è¢«çœç•¥æˆ–è€…ä½¿ç”¨Stirling formulaè¿‘ä¼¼ã€‚å½“targetçš„å€¼å¤§äº1æ—¶ï¼Œä½¿ç”¨è¯¥è¿‘ä¼¼ï¼›å½“å°äºæˆ–ç­‰äº1æ—¶ï¼Œå°†è¯¥æœ€åä¸€é¡¹åŠ åˆ°æŸå¤±ä¸­ï¼Œä¸è¿‘ä¼¼ã€‚
![\text{target}*\log(\text{target}) - \text{target} + 0.5 * \log(2\pi\text{target})](https://math.jianshu.com/math?formula=%5Ctext%7Btarget%7D*%5Clog(%5Ctext%7Btarget%7D)%20-%20%5Ctext%7Btarget%7D%20%2B%200.5%20*%20%5Clog(2%5Cpi%5Ctext%7Btarget%7D))

### 5.3 ä»£ç 



```python
import torch
import torch.nn as nn
import numpy as np
# ----------------------------------- Poisson NLLLoss# ç”Ÿæˆç½‘ç»œè¾“å‡º ä»¥åŠ ç›®æ ‡è¾“å‡º
log_input = torch.randn(5, 2, requires_grad=True)
target = torch.randn(5, 2)
loss_f = nn.PoissonNLLLoss()
loss = loss_f(log_input, target)
print('\nloss: \n', loss)
```

è¾“å‡ºç»“æœï¼š



```jsx
loss:  tensor(1.1533, grad_fn=<MeanBackward0>)
```

## 6ã€KL æ•£åº¦æŸå¤± KLDivLoss



```java
class torch.nn.KLDivLoss(size_average=None, reduce=None, reduction='mean')
```

æ³¨æ„ï¼š

- `reduction`çš„é€‰é¡¹å¢åŠ äº†`batchmean`ï¼›
- `reduction` = `'mean'` doesn't return the true kl divergence valueï¼›é™¤ lossæ€»ä¸ªæ•°ï¼›
- `reduction` = `'batchmean'` which aligns with KL math definition.ï¼ˆç›®å‰æ˜¯è¿™æ ·çš„ï¼Œåç»­ç‰ˆæœ¬å¯èƒ½ä¼šæ”¹è¿›ï¼‰ï¼›é™¤ batch size ã€‚

### 6.1 åŠŸèƒ½

è®¡ç®—inputå’Œtargetä¹‹é—´çš„KLæ•£åº¦( Kullbackâ€“Leibler divergence) ã€‚ KL æ•£åº¦ï¼Œåˆå«åšç›¸å¯¹ç†µï¼Œç®—çš„æ˜¯ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„è·ç¦»ï¼Œè¶Šç›¸ä¼¼åˆ™è¶Šæ¥è¿‘é›¶ã€‚KLæ•£åº¦æ˜¯è¿ç»­åˆ†å¸ƒçš„æœ‰ç”¨è·ç¦»åº¦é‡ï¼Œåœ¨å¯¹è¿ç»­è¾“å‡ºåˆ†å¸ƒçš„ç©ºé—´è¿›è¡Œç›´æ¥å›å½’æ—¶é€šå¸¸å¾ˆæœ‰ç”¨ã€‚

### 6.2 å…¬å¼

1ï¼‰å½“`reduction = 'none'`
![l(x,y) = L = \{ l_1,\dots,l_N \}, \quad l_n = y_n \cdot \left( \log y_n - x_n \right)](https://math.jianshu.com/math?formula=l(x%2Cy)%20%3D%20L%20%3D%20%5C%7B%20l_1%2C%5Cdots%2Cl_N%20%5C%7D%2C%20%5Cquad%20l_n%20%3D%20y_n%20%5Ccdot%20%5Cleft(%20%5Clog%20y_n%20-%20x_n%20%5Cright))

2ï¼‰å½“`reduction`ä¸ä¸º `'none'`ï¼Œé»˜è®¤ä¸º`'mean'`

![L = \{ l_1,\dots,l_N \}, \quad l_n = y_n \cdot \left( \log y_n - x_n \right)](https://math.jianshu.com/math?formula=L%20%3D%20%5C%7B%20l_1%2C%5Cdots%2Cl_N%20%5C%7D%2C%20%5Cquad%20l_n%20%3D%20y_n%20%5Ccdot%20%5Cleft(%20%5Clog%20y_n%20-%20x_n%20%5Cright))

![\ell(x, y) = \begin{cases} \operatorname{mean}(L), & \text{if reduction} = \text{'mean';} \\ \operatorname{sum}(L), & \text{if reduction} = \text{'sum'}. \\ \end{cases}](https://math.jianshu.com/math?formula=%5Cell(x%2C%20y)%20%3D%20%5Cbegin%7Bcases%7D%20%5Coperatorname%7Bmean%7D(L)%2C%20%26%20%5Ctext%7Bif%20reduction%7D%20%3D%20%5Ctext%7B%27mean%27%3B%7D%20%5C%5C%20%5Coperatorname%7Bsum%7D(L)%2C%20%26%20%5Ctext%7Bif%20reduction%7D%20%3D%20%5Ctext%7B%27sum%27%7D.%20%5C%5C%20%5Cend%7Bcases%7D)

### 6.3 ä»£ç 



```python
import torch
import torch.nn as nn
import numpy as np

# -----------------------------------  KLDiv loss
loss_f = nn.KLDivLoss(size_average=False, reduce=False)
loss_f_mean = nn.KLDivLoss(size_average=True, reduce=True)
loss_f_mean_1 = nn.KLDivLoss(reduction='mean')
loss_f_mean_2 = nn.KLDivLoss(reduction='batchmean')

# ç”Ÿæˆç½‘ç»œè¾“å‡º ä»¥åŠ ç›®æ ‡è¾“å‡º
output = torch.from_numpy(np.array([[0.1132, 0.5477, 0.3390], [0.1132, 0.5477, 0.3390]])).float()
output.requires_grad = True
target = torch.from_numpy(np.array([[0.8541, 0.0511, 0.0947], [0.1132, 0.5477, 0.3390]])).float()

loss_1 = loss_f(output, target)
loss_mean = loss_f_mean(output, target)
loss_mean_1 = loss_f_mean_1(output, target)
loss_mean_2 = loss_f_mean_2(output, target)

print('\nloss: ', loss_1)
print('\nloss_mean: ', loss_mean)
print('\nloss_mean_1: ', loss_mean_1)  # é™¤ æ€»æŸå¤±ä¸ªæ•°
print('\nloss_mean_2: ', loss_mean_2)  # è¿™æ˜¯çœŸæ­£æ•°å­¦ä¸ŠKLæ•£åº¦çš„å®šä¹‰ï¼Œé™¤ batch size
print(torch.sum(loss_1) / 6)  # æ‰€ä»¥ ä¸ loss_mean_1ç›¸ç­‰
print(torch.sum(loss_1) / 2)  # æ‰€ä»¥ ä¸ loss_mean_2ç›¸ç­‰

# ç†Ÿæ‚‰è®¡ç®—å…¬å¼ï¼Œæ‰‹åŠ¨è®¡ç®—æ ·æœ¬çš„ç¬¬ä¸€ä¸ªå…ƒç´ çš„lossï¼Œæ³¨æ„è¿™é‡Œåªæœ‰ä¸€ä¸ªæ ·æœ¬ï¼Œæ˜¯ element-wiseè®¡ç®—çš„

output = output[0].detach().numpy()
output_1 = output[0]           # ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ç¬¬ä¸€ä¸ªå…ƒç´ 
target_1 = target[0][0].numpy()

loss_1 = target_1 * (np.log(target_1) - output_1)

print('\nç¬¬ä¸€ä¸ªæ ·æœ¬ç¬¬ä¸€ä¸ªå…ƒç´ çš„lossï¼š', loss_1)
```

è¾“å‡ºç»“æœï¼š



```xml
loss:  tensor([[-0.2314, -0.1800, -0.2553],
        [-0.2594, -0.6297, -0.4816]], grad_fn=<KlDivBackward>)
loss_mean:  tensor(-0.3396, grad_fn=<KlDivBackward>)
loss_mean_1:  tensor(-0.3396, grad_fn=<KlDivBackward>)
loss_mean_2:  tensor(-1.0187, grad_fn=<DivBackward0>)
tensor(-0.3396, grad_fn=<DivBackward0>)
tensor(-1.0187, grad_fn=<DivBackward0>)
ç¬¬ä¸€ä¸ªæ ·æœ¬ç¬¬ä¸€ä¸ªå…ƒç´ çš„lossï¼š -0.23138165
```

## 7ã€äºŒè¿›åˆ¶äº¤å‰ç†µæŸå¤± BCELoss



```rust
class torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='mean')
```

### 7.1 åŠŸèƒ½

äºŒåˆ†ç±»ä»»åŠ¡æ—¶çš„äº¤å‰ç†µè®¡ç®—å‡½æ•°ã€‚æ­¤å‡½æ•°å¯ä»¥è®¤ä¸ºæ˜¯ `nn.CrossEntropyLoss` å‡½æ•°çš„ç‰¹ä¾‹ã€‚**å…¶åˆ†ç±»é™å®šä¸ºäºŒåˆ†ç±»**ï¼Œ![y](https://math.jianshu.com/math?formula=y)å¿…é¡»æ˜¯{0,1}ã€‚è¿˜éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œinput åº”è¯¥ä¸ºæ¦‚ç‡åˆ†å¸ƒçš„å½¢å¼ï¼Œè¿™æ ·æ‰ç¬¦åˆäº¤å‰ç†µçš„åº”ç”¨ã€‚æ‰€ä»¥åœ¨ BCELoss ä¹‹å‰ï¼Œinputä¸€èˆ¬ä¸º sigmoid æ¿€æ´»å±‚çš„è¾“å‡ºï¼Œå®˜æ–¹ä¾‹å­ä¹Ÿæ˜¯è¿™æ ·ç»™çš„ã€‚**è¯¥æŸå¤±å‡½æ•°åœ¨è‡ªç¼–ç å™¨ä¸­å¸¸ç”¨**ã€‚

### 7.2 å…¬å¼

1ï¼‰å½“`reduction = 'none'`
![\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right],](https://math.jianshu.com/math?formula=%5Cell(x%2C%20y)%20%3D%20L%20%3D%20%5C%7Bl_1%2C%5Cdots%2Cl_N%5C%7D%5E%5Ctop%2C%20%5Cquad%20l_n%20%3D%20-%20w_n%20%5Cleft%5B%20y_n%20%5Ccdot%20%5Clog%20x_n%20%2B%20(1%20-%20y_n)%20%5Ccdot%20%5Clog%20(1%20-%20x_n)%20%5Cright%5D%2C)
2ï¼‰å½“`reduction` ä¸ä¸º `'none'`ï¼Œé»˜è®¤ä¸º`'mean'`
![L = \{l_1,\dots,l_N\}^\top, \quad l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right],](https://math.jianshu.com/math?formula=L%20%3D%20%5C%7Bl_1%2C%5Cdots%2Cl_N%5C%7D%5E%5Ctop%2C%20%5Cquad%20l_n%20%3D%20-%20w_n%20%5Cleft%5B%20y_n%20%5Ccdot%20%5Clog%20x_n%20%2B%20(1%20-%20y_n)%20%5Ccdot%20%5Clog%20(1%20-%20x_n)%20%5Cright%5D%2C)

![\ell(x, y) = \begin{cases} \operatorname{mean}(L), & \text{if reduction} = \text{'mean';}\\ \operatorname{sum}(L), & \text{if reduction} = \text{'sum'.} \end{cases}](https://math.jianshu.com/math?formula=%5Cell(x%2C%20y)%20%3D%20%5Cbegin%7Bcases%7D%20%5Coperatorname%7Bmean%7D(L)%2C%20%26%20%5Ctext%7Bif%20reduction%7D%20%3D%20%5Ctext%7B%27mean%27%3B%7D%5C%5C%20%5Coperatorname%7Bsum%7D(L)%2C%20%26%20%5Ctext%7Bif%20reduction%7D%20%3D%20%5Ctext%7B%27sum%27.%7D%20%5Cend%7Bcases%7D)

### 7.3 ä»£ç 



```python
import torch.nn.functional as F
loss_fn = torch.nn.BCELoss(reduce=False, size_average=False)
input = torch.autograd.Variable(torch.randn(3, 4))
target = torch.autograd.Variable(torch.FloatTensor(3, 4).random_(2))
loss = loss_fn(F.sigmoid(input), target)
print(input); print(target); print(loss)
```

è¾“å‡ºç»“æœ



```css
tensor([[-1.8626, -0.1685,  1.3190,  0.4265],
        [ 0.3094, -1.2203, -0.4972, -0.4424],
        [-0.1279,  0.4547,  0.7306,  0.0625]])
tensor([[0., 0., 1., 1.],
        [0., 0., 1., 0.],
        [1., 0., 1., 1.]])
tensor([[0.1443, 0.6125, 0.2370, 0.5025],
        [0.8597, 0.2586, 0.9723, 0.4962],
        [0.7591, 0.9461, 0.3931, 0.6624]])
```

## 8ã€BCEWithLogitsLoss



```rust
class torch.nn.BCEWithLogitsLoss(weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None)
```

### 8.1 åŠŸèƒ½

- å°†`Sigmoid`å±‚å’Œ`BCELoss`ç»„åˆæˆä¸€ä¸ªå±‚ï¼›
- This version is **more numerically stable** than using a plain `Sigmoid`
  followed by a `BCELoss`ï¼›
- å°†ä¸¤ä¸ªæ“ä½œç»„åˆæˆä¸€ä¸ªå±‚ï¼Œ we take advantage of the **log-sum-exp trick for numerical stability.**ã€‚

### 8.2 å…¬å¼

1ï¼‰å½“`reduction = 'none'`

![](imgs/37.png)



2ï¼‰å½“`reduction` ä¸ä¸º `'none'`ï¼Œé»˜è®¤ä¸º`'mean'`

![](imgs/38.png)



### 8.3 ä»£ç 



```php
loss = nn.BCEWithLogitsLoss(reduction='none')
input = torch.randn(3, requires_grad=True)
target = torch.empty(3).random_(2)

output = loss(input, target)
print(input); print(target); print(output)
```

è¾“å‡ºç»“æœï¼š



```jsx
tensor([ 0.1099,  1.3278, -0.2820], requires_grad=True)
tensor([0., 0., 0.])
tensor([0.7496, 1.5629, 0.5620],
       grad_fn=<BinaryCrossEntropyWithLogitsBackward>)
```

## 9ã€MarginRankingLoss



```java
class torch.nn.MarginRankingLoss(margin=0.0, size_average=None, reduce=None, reduction='mean')
```

è®¡ç®—ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œå½“ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„è·ç¦»å¤§äºmarginï¼Œåˆ™lossä¸ºæ­£ï¼Œå°äºmarginï¼Œlossä¸º0

### 9.1 å…¬å¼

![\text{loss}(x1, x2, y) = \max(0, -y * (x1 - x2) + \text{margin})](https://math.jianshu.com/math?formula=%5Ctext%7Bloss%7D(x1%2C%20x2%2C%20y)%20%3D%20%5Cmax(0%2C%20-y%20*%20(x1%20-%20x2)%20%2B%20%5Ctext%7Bmargin%7D))

- inputs :`x1`, `x2`, two 1D mini-batch `Tensors`ï¼›
- `y`: a label 1D mini-batch tensor (containing 1 or -1)ï¼›
- `margin`ï¼šé»˜è®¤ä¸º0ï¼›
- å½“![y=1](https://math.jianshu.com/math?formula=y%3D1)æ—¶ï¼Œ![x1](https://math.jianshu.com/math?formula=x1)è¦æ¯”![x2](https://math.jianshu.com/math?formula=x2)å¤§ï¼Œä¸”![x1 - x2 > margin](https://math.jianshu.com/math?formula=x1%20-%20x2%20%3E%20margin)ï¼Œæ‰ä¸ä¼šæœ‰æŸå¤±ï¼›
- å½“![y=-1](https://math.jianshu.com/math?formula=y%3D-1)æ—¶ï¼Œ![x2](https://math.jianshu.com/math?formula=x2)è¦æ¯”![x1](https://math.jianshu.com/math?formula=x1)å¤§ï¼Œä¸”![x2 - x1 > margin](https://math.jianshu.com/math?formula=x2%20-%20x1%20%3E%20margin)ï¼Œæ‰ä¸ä¼šæœ‰æŸå¤±ã€‚

### 9.2 ä»£ç 



```php
loss = nn.MarginRankingLoss(reduction='none')
input1 = torch.randn(3, requires_grad=True)
input2 = torch.randn(3, requires_grad=True) + 0.5
target = torch.empty(3).random_(2)

output = loss(input1, input2, target)
print(input1); print(input2); print(target); print(output)
```

è¾“å‡ºç»“æœï¼š



```xml
tensor([ 0.2112, -0.0281,  0.5583], requires_grad=True)
tensor([ 1.8994, -0.6425,  0.9355], grad_fn=<AddBackward0>)
tensor([1., 0., 1.])
tensor([1.6882, 0.0000, 0.3772], grad_fn=<ClampMinBackward>)
```

## 10ã€MultiMarginLoss

å¤šåˆ†ç±»ï¼ˆmulti-classï¼‰çš„ Hinge æŸå¤±ï¼Œ

![loss(\text{x}, y) = \frac{1}{N} \sum_{i=0, i \neq y}^{n} [\text{max}(0, (\text{margin} - (\text{x}_{y} - \text{x}_i))^{p})]](https://math.jianshu.com/math?formula=loss(%5Ctext%7Bx%7D%2C%20y)%20%3D%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi%3D0%2C%20i%20%5Cneq%20y%7D%5E%7Bn%7D%20%5B%5Ctext%7Bmax%7D(0%2C%20(%5Ctext%7Bmargin%7D%20-%20(%5Ctext%7Bx%7D_%7By%7D%20-%20%5Ctext%7Bx%7D_i))%5E%7Bp%7D)%5D)
å…¶ä¸­ï¼Œ![0 \leq y \leq N -1](https://math.jianshu.com/math?formula=0%20%5Cleq%20y%20%5Cleq%20N%20-1)è¡¨ç¤ºæ ‡ç­¾ï¼Œ![p](https://math.jianshu.com/math?formula=p)é»˜è®¤å–1ï¼Œmarginé»˜è®¤å–1ï¼Œä¹Ÿå¯ä»¥å–åˆ«çš„å€¼ã€‚

æ³¨æ„ï¼š

- ![\text{x}](https://math.jianshu.com/math?formula=%5Ctext%7Bx%7D)ä¸ºå‘é‡ï¼Œ![y](https://math.jianshu.com/math?formula=y)ä¸ºæ ‡é‡å€¼ã€‚

ä»£ç ï¼š



```bash
loss = nn.MultiMarginLoss()
x = torch.FloatTensor([[0.1, 0.2, 0.4, 0.8]])
y = torch.LongTensor([3])
# 0.25 * ((1 - 0.8 + 0.1) + (1 - 0.8 + 0.2) + (1 - 0.8 + 0.4)) = 0.325
loss(x, y)
```

## 11ã€MultiLabelMarginLoss



```java
class torch.nn.MultiLabelMarginLoss(size_average=None, reduce=None, reduction='mean')
```

å¤šç±»åˆ«ï¼ˆmulti-classï¼‰å¤šåˆ†ç±»ï¼ˆmulti-classificationï¼‰çš„ Hinge æŸå¤±ï¼Œæ˜¯ä¸Šé¢ MultiMarginLoss åœ¨å¤šç±»åˆ«ä¸Šçš„æ‹“å±•ã€‚åŒæ—¶é™å®š p = 1ï¼Œmargin = 1.

è¿™ä¸ªæ¥å£æœ‰ç‚¹å‘ï¼Œæ˜¯ç›´æ¥ä» Torch é‚£é‡ŒæŠ„è¿‡æ¥çš„ï¼Œè§ [MultiLabelMarginCriterion](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Ftorch%2Fnn%2Fblob%2Fmaster%2Fdoc%2Fcriterion.md%23nn.MultiLabelMarginCriterion) çš„æè¿°ã€‚è€Œ Lua çš„ä¸‹æ ‡å’Œ Python ä¸ä¸€æ ·ï¼Œå‰è€…çš„æ•°ç»„ä¸‹æ ‡æ˜¯ä» 1 å¼€å§‹çš„ï¼Œæ‰€ä»¥ç”¨ 0 è¡¨ç¤ºå ä½ç¬¦ã€‚æœ‰å‡ ä¸ªå‘éœ€è¦æ³¨æ„:
![loss(\text{x}, \text{y}) = \frac{1}{N} \sum_{i=1, i \neq \text{y}_i}^{n} \sum_{j=1}^{\text{y}_j \neq 0}[\text{max}(0, 1 - (\text{x}_{\text{y}_j} - \text{x}_i))]](https://math.jianshu.com/math?formula=loss(%5Ctext%7Bx%7D%2C%20%5Ctext%7By%7D)%20%3D%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi%3D1%2C%20i%20%5Cneq%20%5Ctext%7By%7D_i%7D%5E%7Bn%7D%20%5Csum_%7Bj%3D1%7D%5E%7B%5Ctext%7By%7D_j%20%5Cneq%200%7D%5B%5Ctext%7Bmax%7D(0%2C%201%20-%20(%5Ctext%7Bx%7D_%7B%5Ctext%7By%7D_j%7D%20-%20%5Ctext%7Bx%7D_i))%5D)

- è¿™é‡Œçš„ ![\text{x,y}](https://math.jianshu.com/math?formula=%5Ctext%7Bx%2Cy%7D) éƒ½æ˜¯å¤§å°ä¸º N çš„å‘é‡ï¼Œå¦‚æœ ![\text{y}](https://math.jianshu.com/math?formula=%5Ctext%7By%7D) ä¸æ˜¯å‘é‡è€Œæ˜¯æ ‡é‡ï¼Œåé¢çš„ ![\sum_{j=1}](https://math.jianshu.com/math?formula=%5Csum_%7Bj%3D1%7D)å°±æ²¡æœ‰äº†ï¼Œå› æ­¤å°±é€€åŒ–æˆä¸Šé¢çš„MultiMarginLossï¼›
- é™åˆ¶![\text{y}](https://math.jianshu.com/math?formula=%5Ctext%7By%7D)çš„å¤§å°ä¸º Nï¼Œæ˜¯ä¸ºäº†å¤„ç†å¤šæ ‡ç­¾ä¸­æ ‡ç­¾ä¸ªæ•°ä¸åŒçš„æƒ…å†µï¼Œç”¨ 0 è¡¨ç¤ºå ä½ï¼Œè¯¥ä½ç½®å’Œåé¢çš„æ•°å­—éƒ½ä¼šè¢«è®¤ä¸ºä¸æ˜¯æ­£ç¡®çš„ç±»ã€‚å¦‚![\text{y}=[5,3,0,0,4]](https://math.jianshu.com/math?formula=%5Ctext%7By%7D%3D%5B5%2C3%2C0%2C0%2C4%5D) é‚£ä¹ˆå°±ä¼šè¢«è®¤ä¸ºæ˜¯å±äºç±»åˆ« 5 å’Œ 3ï¼Œè€Œ 4 å› ä¸ºåœ¨é›¶åé¢ï¼Œå› æ­¤ä¼šè¢«å¿½ç•¥ã€‚
- ä¸Šé¢çš„å…¬å¼å’Œè¯´æ˜åªæ˜¯ä¸ºäº†å’Œæ–‡æ¡£ä¿æŒä¸€è‡´ï¼Œå…¶å®åœ¨è°ƒç”¨æ¥å£çš„æ—¶å€™ï¼Œç”¨çš„æ˜¯ -1 åšå ä½ç¬¦ï¼Œè€Œ 0 æ˜¯ç¬¬ä¸€ä¸ªç±»åˆ«ã€‚

### 11.1 å…¬å¼è§£æ

![loss(\text{x}, \text{y}) = \frac{1}{N} \sum_{i=0, i \neq \text{y}_{i}}^{n-1} \sum_{j=0}^{\text{y}_j \neq -1}[\text{max}(0, 1 - (\text{x}_{\text{y}_j} - \text{x}_i))]](https://math.jianshu.com/math?formula=loss(%5Ctext%7Bx%7D%2C%20%5Ctext%7By%7D)%20%3D%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi%3D0%2C%20i%20%5Cneq%20%5Ctext%7By%7D_%7Bi%7D%7D%5E%7Bn-1%7D%20%5Csum_%7Bj%3D0%7D%5E%7B%5Ctext%7By%7D_j%20%5Cneq%20-1%7D%5B%5Ctext%7Bmax%7D(0%2C%201%20-%20(%5Ctext%7Bx%7D_%7B%5Ctext%7By%7D_j%7D%20-%20%5Ctext%7Bx%7D_i))%5D)

- **![\text{x,y}](https://math.jianshu.com/math?formula=%5Ctext%7Bx%2Cy%7D)å½¢çŠ¶ç›¸åŒçš„å‘é‡**ï¼Œä¸ºäº†ç»´æŒä¸€è‡´ï¼Œä½¿ç”¨-1å¡«å……![\text{y}](https://math.jianshu.com/math?formula=%5Ctext%7By%7D)ï¼›
- å¯¹äºçœŸå®æ ‡ç­¾![\text{y}](https://math.jianshu.com/math?formula=%5Ctext%7By%7D)ï¼Œä¸è€ƒè™‘![\text{y}](https://math.jianshu.com/math?formula=%5Ctext%7By%7D)ä¸­-1ä¹‹åçš„å€¼ï¼›
- ![j](https://math.jianshu.com/math?formula=j)æ˜¯![\text{y}](https://math.jianshu.com/math?formula=%5Ctext%7By%7D)çš„ç´¢å¼•ï¼Œä»0å¼€å§‹ç›´åˆ°![\text{y}_j \neq -1](https://math.jianshu.com/math?formula=%5Ctext%7By%7D_j%20%5Cneq%20-1)ï¼Œä¹Ÿå°±æ˜¯å–![\text{y}](https://math.jianshu.com/math?formula=%5Ctext%7By%7D)ä¸­-1ä¹‹å‰çš„å€¼ã€‚ä¾‹å¦‚![\text{y} = [3,0,-1,1]](https://math.jianshu.com/math?formula=%5Ctext%7By%7D%20%3D%20%5B3%2C0%2C-1%2C1%5D)ï¼Œåˆ™![j](https://math.jianshu.com/math?formula=j)å¯å–0ã€1ï¼›![\text{y}_j](https://math.jianshu.com/math?formula=%5Ctext%7By%7D_j)å¯å–3ã€0;
- ![i](https://math.jianshu.com/math?formula=i)æ˜¯![\text{x}](https://math.jianshu.com/math?formula=%5Ctext%7Bx%7D)çš„ç´¢å¼•ï¼Œ![i](https://math.jianshu.com/math?formula=i)çš„å–å€¼ä¸º{0, 1, ..., n-1}ä¸­ä¸ç­‰äº![\text{y}_j](https://math.jianshu.com/math?formula=%5Ctext%7By%7D_j)çš„å€¼ã€‚

ä»£ç ï¼š



```bash
loss = nn.MultiLabelMarginLoss()
x = torch.FloatTensor([[0.1, 0.2, 0.4, 0.8]])
# for target y, only consider labels 3 and 0, not after label -1
y = torch.LongTensor([[3, 0, -1, 1]])
# 0.25 * ((1-(0.1-0.2)) + (1-(0.1-0.4)) + (1-(0.8-0.2)) + (1-(0.8-0.4)))
loss(x, y)
```

ä»£ç ä¸­å…¬å¼ï¼š

![](imgs/39.png)



## 12ã€SoftMarginLoss



```java
class nn.SoftMarginLoss(size_average=None, reduce=None, reduction='mean')
```

å¤šæ ‡ç­¾äºŒåˆ†ç±»é—®é¢˜ï¼Œè¿™ N é¡¹éƒ½æ˜¯äºŒåˆ†ç±»é—®é¢˜ï¼Œå…¶å®å°±æ˜¯æŠŠ N ä¸ªäºŒåˆ†ç±»çš„ loss åŠ èµ·æ¥ï¼ŒåŒ–ç®€ä¸€ä¸‹ã€‚å…¶ä¸­ ![\text{y}](https://math.jianshu.com/math?formula=%5Ctext%7By%7D) åªèƒ½å– 1,âˆ’1 ä¸¤ç§ï¼Œä»£è¡¨æ­£ç±»å’Œè´Ÿç±»ã€‚å’Œä¸‹é¢çš„å…¶å®æ˜¯ç­‰ä»·çš„ï¼Œåªæ˜¯ ![\text{y}](https://math.jianshu.com/math?formula=%5Ctext%7By%7D) çš„å½¢å¼ä¸åŒã€‚

![](imgs/40.png)



## 13ã€MultiLabelSoftMarginLoss



```rust
class nn.MultiLabelSoftMarginLoss(    weight=None,    size_average=None,    reduce=None,    reduction='mean',)
```

æ ¹æ®æœ€å¤§ç†µçš„å¤šæ ‡ç­¾ one-versue-all æŸå¤±ã€‚



![](imgs/41.png)



## 14ã€CosineEmbeddingLoss



```java
class nn.CosineEmbeddingLoss(    margin=0.0,    size_average=None,    reduce=None,    reduction='mean',)
```

ä½™å¼¦ç›¸ä¼¼åº¦çš„æŸå¤±ï¼Œç›®çš„æ˜¯è®©ä¸¤ä¸ªå‘é‡å°½é‡ç›¸è¿‘ã€‚æ³¨æ„è¿™ä¸¤ä¸ªå‘é‡éƒ½æ˜¯æœ‰æ¢¯åº¦çš„ã€‚

![](imgs/42.png)

margin å¯ä»¥å– [âˆ’1,1]ï¼Œä½†æ˜¯æ¯”è¾ƒå»ºè®®å– 0-0.5 è¾ƒå¥½ã€‚

## 15ã€HingeEmbeddingLoss



```java
class nn.HingeEmbeddingLoss(    margin=1.0,    size_average=None,    reduce=None,    reduction='mean',)
```

This is usually used for measuring whether two inputs are similar or
dissimilar, e.g. using the L1 pairwise distance as `x`, and is typically
used for learning nonlinear embeddings or semi-supervised learning.

![l_n = \begin{cases} x_n, & \text{if}\; y_n = 1,\\ \max \{0, \Delta - x_n\}, & \text{if}\; y_n = -1, \\ \end{cases}](https://math.jianshu.com/math?formula=l_n%20%3D%20%5Cbegin%7Bcases%7D%20x_n%2C%20%26%20%5Ctext%7Bif%7D%5C%3B%20y_n%20%3D%201%2C%5C%5C%20%5Cmax%20%5C%7B0%2C%20%5CDelta%20-%20x_n%5C%7D%2C%20%26%20%5Ctext%7Bif%7D%5C%3B%20y_n%20%3D%20-1%2C%20%5C%5C%20%5Cend%7Bcases%7D)

![L = \{l_1,\dots,l_N\}^\top](https://math.jianshu.com/math?formula=L%20%3D%20%5C%7Bl_1%2C%5Cdots%2Cl_N%5C%7D%5E%5Ctop)

![\ell(x, y) = \begin{cases} \operatorname{mean}(L), & \text{if reduction} = \text{'mean';}\\ \operatorname{sum}(L), & \text{if reduction} = \text{'sum'.} \end{cases}](https://math.jianshu.com/math?formula=%5Cell(x%2C%20y)%20%3D%20%5Cbegin%7Bcases%7D%20%5Coperatorname%7Bmean%7D(L)%2C%20%26%20%5Ctext%7Bif%20reduction%7D%20%3D%20%5Ctext%7B%27mean%27%3B%7D%5C%5C%20%5Coperatorname%7Bsum%7D(L)%2C%20%26%20%5Ctext%7Bif%20reduction%7D%20%3D%20%5Ctext%7B%27sum%27.%7D%20%5Cend%7Bcases%7D)

## 16ã€TripleMarginLoss



```java
class nn.TripletMarginLoss(
    margin=1.0,
    p=2.0,
    eps=1e-06,
    swap=False,
    size_average=None,
    reduce=None,
    reduction='mean',
)
```

![L(a, p, n) = \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\}](https://math.jianshu.com/math?formula=L(a%2C%20p%2C%20n)%20%3D%20%5Cmax%20%5C%7Bd(a_i%2C%20p_i)%20-%20d(a_i%2C%20n_i)%20%2B%20%7B%5Crm%20margin%7D%2C%200%5C%7D)

![d(x_i, y_i) = \left\lVert {\bf x}_i - {\bf y}_i \right\rVert_p](https://math.jianshu.com/math?formula=d(x_i%2C%20y_i)%20%3D%20%5Cleft%5ClVert%20%7B%5Cbf%20x%7D_i%20-%20%7B%5Cbf%20y%7D_i%20%5Cright%5CrVert_p)



```php
triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)
anchor = torch.randn(100, 128, requires_grad=True)
positive = torch.randn(100, 128, requires_grad=True)
negative = torch.randn(100, 128, requires_grad=True)
output = triplet_loss(anchor, positive, negative)
output
```

## 17ã€SmoothL1Loss



```java
class nn.SmoothL1Loss(size_average=None, reduce=None, reduction='mean')
```

![\text{loss}(x, y) = \frac{1}{n} \sum_{i} z_{i}= \begin{cases} 0.5 (x_i - y_i)^2, & \text{if } |x_i - y_i| < 1 \\ |x_i - y_i| - 0.5, & \text{otherwise } \\ \end{cases}](https://math.jianshu.com/math?formula=%5Ctext%7Bloss%7D(x%2C%20y)%20%3D%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi%7D%20z_%7Bi%7D%3D%20%5Cbegin%7Bcases%7D%200.5%20(x_i%20-%20y_i)%5E2%2C%20%26%20%5Ctext%7Bif%20%7D%20%7Cx_i%20-%20y_i%7C%20%3C%201%20%5C%5C%20%7Cx_i%20-%20y_i%7C%20-%200.5%2C%20%26%20%5Ctext%7Botherwise%20%7D%20%5C%5C%20%5Cend%7Bcases%7D)

## 18ã€CTCLoss



```java
class nn.CTCLoss(blank=0, reduction='mean', zero_infinity=False)
```

è¿˜ä¸è¦æ¸…æ¥šã€‚ã€‚ã€‚



```csharp
T = 50      # Input sequence length
C = 20      # Number of classes (including blank)
N = 16      # Batch size
S = 30      # Target sequence length of longest target in batch
S_min = 10  # Minimum target length, for demonstration purposes

# Initialize random batch of input vectors, for *size = (T,N,C)
input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()

# Initialize random batch of targets (0 = blank, 1:C = classes)
target = torch.randint(low=1, high=C, size=(N, S), dtype=torch.long)

input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)
target_lengths = torch.randint(low=S_min, high=S, size=(N,), dtype=torch.long)
ctc_loss = nn.CTCLoss()
loss = ctc_loss(input, target, input_lengths, target_lengths)
loss.backward()
```

