# èŠ‚çº¦æ˜¾å­˜

âŒšï¸: 2020å¹´8æœˆ1æ—¥

ğŸ“šå‚è€ƒ

- [æµ…è°ˆæ·±åº¦å­¦ä¹ :å¦‚ä½•è®¡ç®—æ¨¡å‹ä»¥åŠä¸­é—´å˜é‡çš„æ˜¾å­˜å ç”¨å¤§å°](https://oldpan.me/archives/how-to-calculate-gpu-memory)
- [å¦‚ä½•åœ¨Pytorchä¸­ç²¾ç»†åŒ–åˆ©ç”¨æ˜¾å­˜](https://oldpan.me/archives/how-to-use-memory-pytorch)
- [ç¥ç»ç½‘ç»œ-GPUå’Œæ˜¾å­˜åˆ†æPPT](https://docs.google.com/presentation/d/e/2PACX-1vQVHMzd5MKrAbsYtCCsWDJ4eo9AUGGsC1tHtOY0agRfUbK0a9YCySvgNejuOLokB6tHbj0tLuohCaNP/pub?start=false&loop=false&delayms=3000&slide=id.p)
- [çŸ¥ä¹QA](https://www.zhihu.com/question/274635237)

---

> ä¸€èˆ¬å‘¢ï¼Œç¥ç»ç½‘ç»œæ˜¾å­˜çš„å ç”¨å¯ä»¥ç®€å•åˆ†ä¸ºè¿™**ä¸‰éƒ¨åˆ†**ï¼š
>
> 1. ç½‘ç»œæ¨¡å‹è‡ªèº«å‚æ•°å ç”¨çš„æ˜¾å­˜ã€‚
> 2. æ¨¡å‹è®¡ç®—æ—¶ï¼ˆåŒ…æ‹¬forward/backward/optimizerï¼‰æ‰€äº§ç”Ÿçš„ä¸­é—´å˜é‡æˆ–å‚æ•°ä¹Ÿæœ‰å ç”¨æ˜¾å­˜ã€‚
> 3. ç¼–ç¨‹æ¡†æ¶è‡ªèº«ä¸€äº›é¢å¤–çš„å¼€é”€ã€‚

## 1. å¦‚ä½•è®¡ç®—æ¨¡å‹ä»¥åŠä¸­é—´å˜é‡çš„æ˜¾å­˜å ç”¨å¤§å°

äº²ï¼Œæ˜¾å­˜ç‚¸äº†ï¼Œä½ çš„æ˜¾å¡å¿«å†’çƒŸäº†ï¼

```python
torch.FatalError: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1524590031827/work/aten/src/THC/generic/THCStorage.cu:58
```

æƒ³å¿…è¿™æ˜¯æ‰€æœ‰ç‚¼ä¸¹å¸ˆä»¬æœ€ä¸æƒ³çœ‹åˆ°çš„é”™è¯¯ï¼Œæ²¡æœ‰ä¹‹ä¸€ã€‚

`OUT OF MEMORY`ï¼Œæ˜¾ç„¶æ˜¯æ˜¾å­˜è£…ä¸ä¸‹ä½ é‚£ä¹ˆå¤šçš„æ¨¡å‹æƒé‡è¿˜æœ‰ä¸­é—´å˜é‡ï¼Œç„¶åç¨‹åºå¥”æºƒäº†ã€‚æ€ä¹ˆåŠï¼Œå…¶å®åŠæ³•æœ‰å¾ˆå¤šï¼ŒåŠæ—¶æ¸…ç©ºä¸­é—´å˜é‡ï¼Œä¼˜åŒ–ä»£ç ï¼Œå‡å°‘batchï¼Œç­‰ç­‰ç­‰ç­‰ï¼Œéƒ½èƒ½å¤Ÿå‡å°‘æ˜¾å­˜æº¢å‡ºçš„é£é™©ã€‚

è¦è¯´çš„æ˜¯ä¸Šé¢è¿™ä¸€åˆ‡ä¼˜åŒ–æ“ä½œçš„åŸºç¡€ï¼Œå¦‚ä½•å»è®¡ç®—æˆ‘ä»¬æ‰€ä½¿ç”¨çš„æ˜¾å­˜ã€‚å­¦ä¼šå¦‚ä½•è®¡ç®—å‡ºæ¥æˆ‘ä»¬è®¾è®¡çš„æ¨¡å‹ä»¥åŠä¸­é—´å˜é‡æ‰€å æ˜¾å­˜çš„å¤§å°ï¼Œæƒ³å¿…çŸ¥é“äº†è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯¹è‡ªå·±æ˜¾å­˜ä¹Ÿå°±ä¼šå¾—å¿ƒåº”æ‰‹äº†ã€‚

### 1.1 å¦‚ä½•è®¡ç®—

é¦–å…ˆæˆ‘ä»¬åº”è¯¥äº†è§£ä¸€ä¸‹åŸºæœ¬çš„æ•°æ®é‡ä¿¡æ¯ï¼š

- 1 G = 1000 MB
- 1 M = 1000 KB
- 1 K = 1000 Byte
- 1 B = 8 bit

ç„¶åæˆ‘ä»¬è¯´ä¸€ä¸‹æˆ‘ä»¬å¹³å¸¸ä½¿ç”¨çš„å‘é‡æ‰€å çš„ç©ºé—´å¤§å°ï¼Œä»¥Pytorchå®˜æ–¹çš„æ•°æ®æ ¼å¼ä¸ºä¾‹ï¼ˆæ‰€æœ‰çš„æ·±åº¦å­¦ä¹ æ¡†æ¶æ•°æ®æ ¼å¼éƒ½éµå¾ªåŒä¸€ä¸ªæ ‡å‡†ï¼‰ï¼š

![ã€Šæµ…è°ˆæ·±åº¦å­¦ä¹ :å¦‚ä½•è®¡ç®—æ¨¡å‹ä»¥åŠä¸­é—´å˜é‡çš„æ˜¾å­˜å ç”¨å¤§å°ã€‹](imgs\4.png)

æˆ‘ä»¬åªéœ€è¦çœ‹å·¦è¾¹çš„ä¿¡æ¯ï¼Œåœ¨å¹³å¸¸çš„è®­ç»ƒä¸­ï¼Œæˆ‘ä»¬ç»å¸¸ä½¿ç”¨çš„ä¸€èˆ¬æ˜¯è¿™ä¸¤ç§ç±»å‹ï¼š

- float32 å•ç²¾åº¦æµ®ç‚¹å‹
- int32 æ•´å‹

ä¸€èˆ¬ä¸€ä¸ª8-bitçš„æ•´å‹å˜é‡æ‰€å çš„ç©ºé—´ä¸º`1B`ä¹Ÿå°±æ˜¯`8bit`ã€‚è€Œ32ä½çš„floatåˆ™å `4B`ä¹Ÿå°±æ˜¯`32bit`ã€‚è€ŒåŒç²¾åº¦æµ®ç‚¹å‹doubleå’Œé•¿æ•´å‹longåœ¨å¹³å¸¸çš„è®­ç»ƒä¸­æˆ‘ä»¬ä¸€èˆ¬ä¸ä¼šä½¿ç”¨ã€‚

psï¼šæ¶ˆè´¹çº§æ˜¾å¡å¯¹å•ç²¾åº¦è®¡ç®—æœ‰ä¼˜åŒ–ï¼ŒæœåŠ¡å™¨çº§åˆ«æ˜¾å¡å¯¹åŒç²¾åº¦è®¡ç®—æœ‰ä¼˜åŒ–ã€‚

ä¹Ÿå°±æ˜¯è¯´ï¼Œå‡è®¾æœ‰ä¸€å¹…RGBä¸‰é€šé“çœŸå½©è‰²å›¾ç‰‡ï¼Œé•¿å®½åˆ†åˆ«ä¸º500 x 500ï¼Œæ•°æ®ç±»å‹ä¸ºå•ç²¾åº¦æµ®ç‚¹å‹ï¼Œé‚£ä¹ˆè¿™å¼ å›¾æ‰€å çš„æ˜¾å­˜çš„å¤§å°ä¸ºï¼š500 x 500 x 3 x 4B = 3Mã€‚

è€Œä¸€ä¸ª(256,3,100,100)-(N,C,H,W)çš„FloatTensoræ‰€å çš„ç©ºé—´ä¸º256 x 3 x 100 x 100 x 4B = 31M

ä¸å¤šæ˜¯å§ï¼Œæ²¡å…³ç³»ï¼Œå¥½æˆæ‰åˆšåˆšå¼€å§‹ã€‚

### 1.2 æ˜¾å­˜å»å“ªå„¿äº†

çœ‹èµ·æ¥ä¸€å¼ å›¾ç‰‡(3x256x256)å’Œå·ç§¯å±‚(256x100x100)æ‰€å çš„ç©ºé—´å¹¶ä¸å¤§ï¼Œé‚£ä¸ºä»€ä¹ˆæˆ‘ä»¬çš„æ˜¾å­˜ä¾æ—§è¿˜æ˜¯ç”¨çš„æ¯”è¾ƒå¤šï¼ŒåŸå› å¾ˆç®€å•ï¼Œ**å ç”¨æ˜¾å­˜æ¯”è¾ƒå¤šç©ºé—´çš„å¹¶ä¸æ˜¯æˆ‘ä»¬è¾“å…¥å›¾åƒ**ï¼Œè€Œæ˜¯**ç¥ç»ç½‘ç»œä¸­çš„ä¸­é—´å˜é‡**ä»¥åŠ**ä½¿ç”¨optimizerç®—æ³•æ—¶äº§ç”Ÿçš„å·¨é‡çš„ä¸­é—´å‚æ•°**ã€‚

æˆ‘ä»¬é¦–å…ˆæ¥ç®€å•è®¡ç®—ä¸€ä¸‹Vgg16è¿™ä¸ªnetéœ€è¦å ç”¨çš„æ˜¾å­˜ï¼š

é€šå¸¸ä¸€ä¸ªæ¨¡å‹å ç”¨çš„æ˜¾å­˜ä¹Ÿå°±æ˜¯ä¸¤éƒ¨åˆ†ï¼š

- æ¨¡å‹è‡ªèº«çš„å‚æ•°(params)

- æ¨¡å‹è®¡ç®—äº§ç”Ÿçš„ä¸­é—´å˜é‡(memory)

  

![ã€Šæµ…è°ˆæ·±åº¦å­¦ä¹ :å¦‚ä½•è®¡ç®—æ¨¡å‹ä»¥åŠä¸­é—´å˜é‡çš„æ˜¾å­˜å ç”¨å¤§å°ã€‹](imgs\5.png)

å›¾ç‰‡æ¥è‡ªcs231nï¼Œè¿™æ˜¯ä¸€ä¸ªå…¸å‹çš„sequential-netï¼Œè‡ªä¸Šè€Œä¸‹å¾ˆé¡ºç•…ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘ä»¬è¾“å…¥çš„æ˜¯ä¸€å¼ 224x224x3çš„ä¸‰é€šé“å›¾åƒï¼Œå¯ä»¥çœ‹åˆ°ä¸€å¼ å›¾åƒåªå ç”¨`150x4k`ï¼Œä½†ä¸Šé¢æ ‡æ³¨çš„æ˜¯`150k`ï¼Œè¿™æ˜¯å› ä¸ºä¸Šå›¾ä¸­åœ¨è®¡ç®—çš„æ—¶å€™é»˜è®¤çš„æ•°æ®æ ¼å¼æ˜¯8-bitè€Œä¸æ˜¯32-bitï¼Œæ‰€ä»¥æœ€åçš„ç»“æœè¦ä¹˜ä¸Šä¸€ä¸ª4ã€‚

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œ**å·¦è¾¹çš„memoryå€¼ä»£è¡¨ï¼šå›¾åƒè¾“å…¥è¿›å»ï¼Œå›¾ç‰‡ä»¥åŠæ‰€äº§ç”Ÿçš„ä¸­é—´å·ç§¯å±‚æ‰€å çš„ç©ºé—´**ã€‚æˆ‘ä»¬éƒ½çŸ¥é“ï¼Œè¿™äº›å½¢å½¢è‰²è‰²çš„æ·±å±‚å·ç§¯å±‚ä¹Ÿå°±æ˜¯æ·±åº¦ç¥ç»ç½‘ç»œè¿›è¡Œâ€œæ€è€ƒâ€çš„è¿‡ç¨‹ï¼š



![ã€Šæµ…è°ˆæ·±åº¦å­¦ä¹ :å¦‚ä½•è®¡ç®—æ¨¡å‹ä»¥åŠä¸­é—´å˜é‡çš„æ˜¾å­˜å ç”¨å¤§å°ã€‹](imgs\6.png)

å›¾ç‰‡ä»3é€šé“å˜ä¸º64 â€“> 128 â€“> 256 â€“> 512 â€¦. è¿™äº›éƒ½æ˜¯å·ç§¯å±‚ï¼Œè€Œæˆ‘ä»¬çš„æ˜¾å­˜ä¹Ÿä¸»è¦æ˜¯ä»–ä»¬å ç”¨äº†ã€‚

è¿˜æœ‰ä¸Šé¢**å³è¾¹çš„paramsï¼Œè¿™äº›æ˜¯ç¥ç»ç½‘ç»œçš„æƒé‡å¤§å°**ï¼Œå¯ä»¥çœ‹åˆ°ç¬¬ä¸€å±‚å·ç§¯æ˜¯3Ã—3ï¼Œè€Œè¾“å…¥å›¾åƒçš„é€šé“æ˜¯3ï¼Œè¾“å‡ºé€šé“æ˜¯64ï¼Œæ‰€ä»¥å¾ˆæ˜¾ç„¶ï¼Œç¬¬ä¸€ä¸ªå·ç§¯å±‚æƒé‡æ‰€å çš„ç©ºé—´æ˜¯ (3 x 3 x 3) x 64ã€‚

å¦å¤–è¿˜æœ‰ä¸€ä¸ªéœ€è¦æ³¨æ„çš„æ˜¯ä¸­é—´å˜é‡åœ¨backwardçš„æ—¶å€™ä¼šç¿»å€ï¼

ä¸ºä»€ä¹ˆï¼Œä¸¾ä¸ªä¾‹å­ï¼Œä¸‹é¢æ˜¯ä¸€ä¸ªè®¡ç®—å›¾ï¼Œè¾“å…¥`x`ï¼Œç»è¿‡ä¸­é—´ç»“æœ`z`ï¼Œç„¶åå¾—åˆ°æœ€ç»ˆå˜é‡`L`ï¼š



![ã€Šæµ…è°ˆæ·±åº¦å­¦ä¹ :å¦‚ä½•è®¡ç®—æ¨¡å‹ä»¥åŠä¸­é—´å˜é‡çš„æ˜¾å­˜å ç”¨å¤§å°ã€‹](imgs\7.png)

æˆ‘ä»¬åœ¨backwardçš„æ—¶å€™éœ€è¦ä¿å­˜ä¸‹æ¥çš„ä¸­é—´å€¼ã€‚è¾“å‡ºæ˜¯`L`ï¼Œç„¶åè¾“å…¥`x`ï¼Œæˆ‘ä»¬åœ¨backwardçš„æ—¶å€™è¦æ±‚`L`å¯¹`x`çš„æ¢¯åº¦ï¼Œè¿™ä¸ªæ—¶å€™å°±éœ€è¦åœ¨è®¡ç®—é“¾`L`å’Œ`x`ä¸­é—´çš„`z`ï¼š

![ã€Šæµ…è°ˆæ·±åº¦å­¦ä¹ :å¦‚ä½•è®¡ç®—æ¨¡å‹ä»¥åŠä¸­é—´å˜é‡çš„æ˜¾å­˜å ç”¨å¤§å°ã€‹](imgs\8.png)

`dz/dx`è¿™ä¸ªä¸­é—´å€¼å½“ç„¶è¦ä¿ç•™ä¸‹æ¥ä»¥ç”¨äºè®¡ç®—ï¼Œæ‰€ä»¥ç²—ç•¥ä¼°è®¡ï¼Œ`backward`çš„æ—¶å€™ä¸­é—´å˜é‡çš„å ç”¨äº†æ˜¯`forward`çš„ä¸¤å€ï¼

### 1.3 ä¼˜åŒ–å™¨å’ŒåŠ¨é‡

è¦æ³¨æ„ï¼Œä¼˜åŒ–å™¨ä¹Ÿä¼šå ç”¨æˆ‘ä»¬çš„æ˜¾å­˜ï¼

ä¸ºä»€ä¹ˆ,çœ‹è¿™ä¸ªå¼å­:

![ã€Šæµ…è°ˆæ·±åº¦å­¦ä¹ :å¦‚ä½•è®¡ç®—æ¨¡å‹ä»¥åŠä¸­é—´å˜é‡çš„æ˜¾å­˜å ç”¨å¤§å°ã€‹](imgs\9.png)



ä¸Šå¼æ˜¯å…¸å‹çš„SGDéšæœºä¸‹é™æ³•çš„æ€»ä½“å…¬å¼ï¼Œæƒé‡`W`åœ¨è¿›è¡Œæ›´æ–°çš„æ—¶å€™ï¼Œä¼šäº§ç”Ÿä¿å­˜ä¸­é—´å˜é‡![ã€Šæµ…è°ˆæ·±åº¦å­¦ä¹ :å¦‚ä½•è®¡ç®—æ¨¡å‹ä»¥åŠä¸­é—´å˜é‡çš„æ˜¾å­˜å ç”¨å¤§å°ã€‹](https://image.oldpan.me/TIM%E6%88%AA%E5%9B%BE20180607104328.jpg)ï¼Œä¹Ÿå°±æ˜¯åœ¨ä¼˜åŒ–çš„æ—¶å€™ï¼Œæ¨¡å‹ä¸­çš„paramså‚æ•°æ‰€å ç”¨çš„æ˜¾å­˜é‡ä¼šç¿»å€ã€‚

å½“ç„¶è¿™åªæ˜¯SGDä¼˜åŒ–å™¨ï¼Œå…¶ä»–å¤æ‚çš„ä¼˜åŒ–å™¨å¦‚æœåœ¨è®¡ç®—æ—¶éœ€è¦çš„ä¸­é—´å˜é‡å¤šçš„æ—¶å€™ï¼Œå°±ä¼šå ç”¨æ›´å¤šçš„å†…å­˜ã€‚

### 1.4 æ¨¡å‹ä¸­å“ªäº›å±‚ä¼šå ç”¨æ˜¾å­˜

æœ‰å‚æ•°çš„å±‚å³ä¼šå ç”¨æ˜¾å­˜çš„å±‚ã€‚æˆ‘ä»¬ä¸€èˆ¬çš„å·ç§¯å±‚éƒ½ä¼šå ç”¨æ˜¾å­˜ï¼Œè€Œæˆ‘ä»¬ç»å¸¸ä½¿ç”¨çš„æ¿€æ´»å±‚Reluæ²¡æœ‰å‚æ•°å°±ä¸ä¼šå ç”¨äº†ã€‚

å ç”¨æ˜¾å­˜çš„å±‚ä¸€èˆ¬æ˜¯ï¼š

- å·ç§¯å±‚ï¼Œé€šå¸¸çš„conv2d
- å…¨è¿æ¥å±‚ï¼Œä¹Ÿå°±æ˜¯Linearå±‚
- BatchNormå±‚
- Embeddingå±‚

è€Œä¸å ç”¨æ˜¾å­˜çš„åˆ™æ˜¯ï¼š

- åˆšæ‰è¯´åˆ°çš„æ¿€æ´»å±‚Reluç­‰
- æ± åŒ–å±‚
- Dropoutå±‚

å…·ä½“è®¡ç®—æ–¹å¼ï¼š

- Conv2d(Cin, Cout, K): å‚æ•°æ•°ç›®ï¼šCin Ã— Cout Ã— K Ã— K
- Linear(M->N): å‚æ•°æ•°ç›®ï¼šMÃ—N
- BatchNorm(N): å‚æ•°æ•°ç›®ï¼š 2N
- Embedding(N,W): å‚æ•°æ•°ç›®ï¼š N Ã— W

### 1.5 é¢å¤–çš„æ˜¾å­˜

æ€»ç»“ä¸€ä¸‹ï¼Œæˆ‘ä»¬åœ¨æ€»ä½“çš„è®­ç»ƒä¸­ï¼Œå ç”¨æ˜¾å­˜å¤§æ¦‚åˆ†ä»¥ä¸‹å‡ ç±»ï¼š

- æ¨¡å‹ä¸­çš„å‚æ•°(å·ç§¯å±‚æˆ–å…¶ä»–æœ‰å‚æ•°çš„å±‚)
- æ¨¡å‹åœ¨è®¡ç®—æ—¶äº§ç”Ÿçš„ä¸­é—´å‚æ•°(ä¹Ÿå°±æ˜¯è¾“å…¥å›¾åƒåœ¨è®¡ç®—æ—¶æ¯ä¸€å±‚äº§ç”Ÿçš„è¾“å…¥å’Œè¾“å‡º)
- backwardçš„æ—¶å€™äº§ç”Ÿçš„é¢å¤–çš„ä¸­é—´å‚æ•°
- ä¼˜åŒ–å™¨åœ¨ä¼˜åŒ–æ—¶äº§ç”Ÿçš„é¢å¤–çš„æ¨¡å‹å‚æ•°

ä½†å…¶å®ï¼Œæˆ‘ä»¬å ç”¨çš„æ˜¾å­˜ç©ºé—´ä¸ºä»€ä¹ˆæ¯”æˆ‘ä»¬ç†è®ºè®¡ç®—çš„è¿˜è¦å¤§ï¼ŒåŸå› å¤§æ¦‚æ˜¯å› ä¸ºæ·±åº¦å­¦ä¹ æ¡†æ¶ä¸€äº›é¢å¤–çš„å¼€é”€å§ï¼Œä¸è¿‡å¦‚æœé€šè¿‡ä¸Šé¢å…¬å¼ï¼Œç†è®ºè®¡ç®—å‡ºæ¥çš„æ˜¾å­˜å’Œå®é™…ä¸ä¼šå·®å¤ªå¤šçš„ã€‚



## 2. å¦‚ä½•åœ¨Pytorchä¸­ç²¾ç»†åŒ–åˆ©ç”¨æ˜¾å­˜

åœ¨ä¸ŠèŠ‚ä¸­æˆ‘ä»¬å¯¹å¦‚ä½•è®¡ç®—å„ç§å˜é‡æ‰€å æ˜¾å­˜å¤§å°è¿›è¡Œäº†ä¸€äº›æ¢ç´¢ã€‚è€Œè¿™èŠ‚æˆ‘ä»¬ç€é‡è®²è§£å¦‚ä½•åˆ©ç”¨Pytorchæ·±åº¦å­¦ä¹ æ¡†æ¶çš„ä¸€äº›ç‰¹æ€§ï¼Œå»æŸ¥çœ‹æˆ‘ä»¬å½“å‰ä½¿ç”¨çš„å˜é‡æ‰€å ç”¨çš„æ˜¾å­˜å¤§å°ï¼Œä»¥åŠä¸€äº›ä¼˜åŒ–å·¥ä½œã€‚ä»¥ä¸‹ä»£ç æ‰€ä½¿ç”¨çš„å¹³å°æ¡†æ¶ä¸ºPytorchã€‚

**ä¼˜åŒ–æ˜¾å­˜**

åœ¨Pytorchä¸­ä¼˜åŒ–æ˜¾å­˜æ˜¯æˆ‘ä»¬å¤„ç†å¤§é‡æ•°æ®æ—¶å¿…è¦çš„åšæ³•ï¼Œå› ä¸ºæˆ‘ä»¬å¹¶ä¸å¯èƒ½æ‹¥æœ‰æ— é™çš„æ˜¾å­˜ã€‚æ˜¾å­˜æ˜¯æœ‰é™çš„ï¼Œè€Œæ•°æ®æ˜¯æ— é™çš„ï¼Œæˆ‘ä»¬åªæœ‰ä¼˜åŒ–æ˜¾å­˜çš„ä½¿ç”¨é‡æ‰èƒ½å¤Ÿæœ€å¤§åŒ–åœ°åˆ©ç”¨æˆ‘ä»¬çš„æ•°æ®ï¼Œå®ç°å¤šç§å¤šæ ·çš„ç®—æ³•ã€‚

### 2.1 ä¼°æµ‹æ¨¡å‹æ‰€å çš„å†…å­˜

ä¸ŠèŠ‚ä¸­è¯´è¿‡ï¼Œä¸€ä¸ªæ¨¡å‹æ‰€å çš„æ˜¾å­˜æ— éæ˜¯è¿™ä¸¤ç§ï¼š

- æ¨¡å‹æƒé‡å‚æ•°
- æ¨¡å‹æ‰€å‚¨å­˜çš„ä¸­é—´å˜é‡

å…¶å®æƒé‡å‚æ•°ä¸€èˆ¬æ¥è¯´å¹¶ä¸ä¼šå ç”¨å¾ˆå¤šçš„æ˜¾å­˜ç©ºé—´ï¼Œ**ä¸»è¦å ç”¨æ˜¾å­˜ç©ºé—´çš„è¿˜æ˜¯è®¡ç®—æ—¶äº§ç”Ÿçš„ä¸­é—´å˜é‡**ï¼Œå½“æˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªmodelä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹ä»£ç ç®€å•è®¡ç®—å‡ºè¿™ä¸ªæ¨¡å‹æƒé‡å‚æ•°æ‰€å ç”¨çš„æ•°æ®é‡ï¼š

```python
import numpy as np

# modelæ˜¯æˆ‘ä»¬åœ¨pytorchå®šä¹‰çš„ç¥ç»ç½‘ç»œå±‚
# model.parameters()å–å‡ºè¿™ä¸ªmodelæ‰€æœ‰çš„æƒé‡å‚æ•°
para = sum([np.prod(list(p.size())) for p in model.parameters()])
```

å‡è®¾æˆ‘ä»¬æœ‰è¿™æ ·ä¸€ä¸ªmodelï¼š

```python
Sequential(
  (conv_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu_1): ReLU(inplace)
  (conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (relu_2): ReLU(inplace)
  (pool_2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv_3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)
```

ç„¶åæˆ‘ä»¬å¾—åˆ°çš„`para`æ˜¯`112576`ï¼Œä½†æ˜¯æˆ‘ä»¬è®¡ç®—å‡ºæ¥çš„ä»…ä»…æ˜¯æƒé‡å‚æ•°çš„â€œæ•°é‡â€ï¼Œå•ä½æ˜¯Bï¼Œæˆ‘ä»¬éœ€è¦è½¬åŒ–ä¸€ä¸‹ï¼š

```python
# ä¸‹é¢çš„type_sizeæ˜¯4ï¼Œå› ä¸ºæˆ‘ä»¬çš„å‚æ•°æ˜¯float32ä¹Ÿå°±æ˜¯4Bï¼Œ4ä¸ªå­—èŠ‚
 print('Model {} : params: {:4f}M'.format(model._get_name(), para * type_size / 1000 / 1000))
```

è¿™æ ·å°±å¯ä»¥æ‰“å°å‡ºï¼š

```powershell
Model Sequential : params: 0.450304M
```

ä½†æ˜¯æˆ‘ä»¬ä¹‹å‰è¯´è¿‡ä¸€ä¸ªç¥ç»ç½‘ç»œçš„æ¨¡å‹ï¼Œä¸ä»…ä»…æœ‰æƒé‡å‚æ•°è¿˜è¦è®¡ç®—ä¸­é—´å˜é‡çš„å¤§å°ã€‚æ€ä¹ˆå»è®¡ç®—ï¼Œæˆ‘ä»¬å¯ä»¥å‡è®¾ä¸€ä¸ª`è¾“å…¥å˜é‡`ï¼Œç„¶åå°†è¿™ä¸ªè¾“å…¥å˜é‡æŠ•å…¥è¿™ä¸ªæ¨¡å‹ä¸­ï¼Œç„¶åæˆ‘ä»¬ä¸»åŠ¨æå–è¿™äº›è®¡ç®—å‡ºæ¥çš„ä¸­é—´å˜é‡ï¼š

```python
# modelæ˜¯æˆ‘ä»¬åŠ è½½çš„æ¨¡å‹
# inputæ˜¯å®é™…ä¸­æŠ•å…¥çš„inputï¼ˆTensorï¼‰å˜é‡

# åˆ©ç”¨clone()å»å¤åˆ¶ä¸€ä¸ªinputï¼Œè¿™æ ·ä¸ä¼šå¯¹inputé€ æˆå½±å“
input_ = input.clone()   
# ç¡®ä¿ä¸éœ€è¦è®¡ç®—æ¢¯åº¦ï¼Œå› ä¸ºæˆ‘ä»¬çš„ç›®çš„åªæ˜¯ä¸ºäº†è®¡ç®—ä¸­é—´å˜é‡è€Œå·²
input_.requires_grad_(requires_grad=False)

mods = list(model.modules())
out_sizes = []

for i in range(1, len(mods)):
    m = mods[i]
    # æ³¨æ„è¿™é‡Œï¼Œå¦‚æœreluæ¿€æ´»å‡½æ•°æ˜¯inplaceåˆ™ä¸ç”¨è®¡ç®—
    if isinstance(m, nn.ReLU):  
        if m.inplace:
            continue
    out = m(input_)
    out_sizes.append(np.array(out.size()))
    input_ = out

total_nums = 0
for i in range(len(out_sizes)):
    s = out_sizes[i]
    nums = np.prod(np.array(s))
    total_nums += nums
```

ä¸Šé¢å¾—åˆ°çš„å€¼æ˜¯æ¨¡å‹åœ¨è¿è¡Œæ—¶å€™äº§ç”Ÿæ‰€æœ‰çš„ä¸­é—´å˜é‡çš„â€œæ•°é‡â€ï¼Œå½“ç„¶æˆ‘ä»¬éœ€è¦æ¢ç®—ä¸€ä¸‹ï¼š

```python
# æ‰“å°ä¸¤ç§ï¼Œåªæœ‰ forward å’Œ forewardã€backwardçš„æƒ…å†µ
print('Model {} : intermedite variables: {:3f} M (without backward)'
        .format(model._get_name(), total_nums * type_size / 1000 / 1000))
print('Model {} : intermedite variables: {:3f} M (with backward)'
        .format(model._get_name(), total_nums * type_size*2 / 1000 / 1000))
```

å› ä¸ºåœ¨`backward`çš„æ—¶å€™æ‰€æœ‰çš„ä¸­é—´å˜é‡éœ€è¦ä¿å­˜ä¸‹æ¥å†æ¥è¿›è¡Œè®¡ç®—ï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨è®¡ç®—`backward`çš„æ—¶å€™ï¼Œè®¡ç®—å‡ºæ¥çš„ä¸­é—´å˜é‡éœ€è¦ä¹˜ä¸ª2ã€‚

ç„¶åæˆ‘ä»¬å¾—å‡ºï¼Œä¸Šé¢è¿™ä¸ªæ¨¡å‹çš„ä¸­é—´å˜é‡éœ€è¦çš„å ç”¨çš„æ˜¾å­˜ï¼Œå¾ˆæ˜¾ç„¶ï¼Œä¸­é—´å˜é‡å ç”¨çš„å€¼æ¯”æ¨¡å‹æœ¬èº«çš„æƒé‡å€¼å¤šå¤šäº†ã€‚å¦‚æœè¿›è¡Œä¸€æ¬¡backwardé‚£ä¹ˆéœ€è¦çš„å°±æ›´å¤šã€‚

```powershell
Model Sequential : intermedite variables: 336.089600 M (without backward)
Model Sequential : intermedite variables: 672.179200 M (with backward)
```

æˆ‘ä»¬æ€»ç»“ä¸€ä¸‹ä¹‹å‰çš„ä»£ç ï¼š

```python
# æ¨¡å‹æ˜¾å­˜å ç”¨ç›‘æµ‹å‡½æ•°
# modelï¼šè¾“å…¥çš„æ¨¡å‹
# inputï¼šå®é™…ä¸­éœ€è¦è¾“å…¥çš„Tensorå˜é‡
# type_size é»˜è®¤ä¸º 4 é»˜è®¤ç±»å‹ä¸º float32 

def modelsize(model, input, type_size=4):
    para = sum([np.prod(list(p.size())) for p in model.parameters()])
    print('Model {} : params: {:4f}M'.format(model._get_name(), para * type_size / 1000 / 1000))

    input_ = input.clone()
    input_.requires_grad_(requires_grad=False)

    mods = list(model.modules())
    out_sizes = []

    for i in range(1, len(mods)):
        m = mods[i]
        if isinstance(m, nn.ReLU):
            if m.inplace:
                continue
        out = m(input_)
        out_sizes.append(np.array(out.size()))
        input_ = out

    total_nums = 0
    for i in range(len(out_sizes)):
        s = out_sizes[i]
        nums = np.prod(np.array(s))
        total_nums += nums


    print('Model {} : intermedite variables: {:3f} M (without backward)'
          .format(model._get_name(), total_nums * type_size / 1000 / 1000))
    print('Model {} : intermedite variables: {:3f} M (with backward)'
          .format(model._get_name(), total_nums * type_size*2 / 1000 / 1000))
```

å½“ç„¶æˆ‘ä»¬è®¡ç®—å‡ºæ¥çš„å ç”¨æ˜¾å­˜å€¼ä»…ä»…æ˜¯åšå‚è€ƒä½œç”¨ï¼Œå› ä¸ºPytorchåœ¨è¿è¡Œçš„æ—¶å€™éœ€è¦é¢å¤–çš„æ˜¾å­˜å€¼å¼€é”€ï¼Œæ‰€ä»¥å®é™…çš„æ˜¾å­˜ä¼šæ¯”æˆ‘ä»¬è®¡ç®—çš„ç¨å¾®å¤§ä¸€äº›ã€‚

### 2.2 å…³äº`inplace=False`

æˆ‘ä»¬éƒ½çŸ¥é“æ¿€æ´»å‡½æ•°`Relu()`æœ‰ä¸€ä¸ªé»˜è®¤å‚æ•°`inplace`ï¼Œé»˜è®¤è®¾ç½®ä¸ºFalseï¼Œå½“è®¾ç½®ä¸ºTrueæ—¶ï¼Œæˆ‘ä»¬åœ¨é€šè¿‡relu()è®¡ç®—æ—¶çš„å¾—åˆ°çš„æ–°å€¼ä¸ä¼šå ç”¨æ–°çš„ç©ºé—´è€Œæ˜¯ç›´æ¥è¦†ç›–åŸæ¥çš„å€¼ï¼Œè¿™ä¹Ÿå°±æ˜¯ä¸ºä»€ä¹ˆå½“inplaceå‚æ•°è®¾ç½®ä¸ºTrueæ—¶å¯ä»¥èŠ‚çœä¸€éƒ¨åˆ†å†…å­˜çš„ç¼˜æ•…ã€‚



![ã€Šå¦‚ä½•åœ¨Pytorchä¸­ç²¾ç»†åŒ–åˆ©ç”¨æ˜¾å­˜ã€‹](imgs\relu.png)

### 2.3 ç‰ºç‰²è®¡ç®—é€Ÿåº¦å‡å°‘æ˜¾å­˜ä½¿ç”¨é‡

åœ¨`Pytorch-0.4.0`å‡ºæ¥äº†ä¸€ä¸ªæ–°çš„åŠŸèƒ½ï¼Œå¯ä»¥å°†ä¸€ä¸ªè®¡ç®—è¿‡ç¨‹åˆ†æˆä¸¤åŠï¼Œä¹Ÿå°±æ˜¯å¦‚æœä¸€ä¸ªæ¨¡å‹éœ€è¦å ç”¨çš„æ˜¾å­˜å¤ªå¤§äº†ï¼Œæˆ‘ä»¬å°±å¯ä»¥å…ˆè®¡ç®—ä¸€åŠï¼Œä¿å­˜åä¸€åŠéœ€è¦çš„ä¸­é—´ç»“æœï¼Œç„¶åå†è®¡ç®—åä¸€åŠã€‚

ä¹Ÿå°±æ˜¯è¯´ï¼Œæ–°çš„`checkpoint`å…è®¸æˆ‘ä»¬åªå­˜å‚¨åå‘ä¼ æ’­æ‰€éœ€è¦çš„éƒ¨åˆ†å†…å®¹ã€‚å¦‚æœå½“ä¸­ç¼ºå°‘ä¸€ä¸ªè¾“å‡º(ä¸ºäº†èŠ‚çœå†…å­˜è€Œå¯¼è‡´çš„)ï¼Œ`checkpoint`å°†ä¼šä»æœ€è¿‘çš„æ£€æŸ¥ç‚¹é‡æ–°è®¡ç®—ä¸­é—´è¾“å‡ºï¼Œä»¥ä¾¿å‡å°‘å†…å­˜ä½¿ç”¨(å½“ç„¶è®¡ç®—æ—¶é—´å¢åŠ äº†)ï¼š

```python
# è¾“å…¥
input = torch.rand(1, 10)
# å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªéå¸¸æ·±çš„ç½‘ç»œ
layers = [nn.Linear(10, 10) for _ in range(1000)]
model = nn.Sequential(*layers)output = model(input)
```

ä¸Šé¢çš„æ¨¡å‹éœ€è¦å ç”¨å¾ˆå¤šçš„å†…å­˜ï¼Œå› ä¸ºè®¡ç®—ä¸­ä¼šäº§ç”Ÿå¾ˆå¤šçš„ä¸­é—´å˜é‡ã€‚ä¸ºæ­¤`checkpoint`å°±å¯ä»¥å¸®åŠ©æˆ‘ä»¬æ¥èŠ‚çœå†…å­˜çš„å ç”¨äº†ã€‚

```python
# é¦–å…ˆè®¾ç½®è¾“å…¥çš„input=>requires_grad=True
# å¦‚æœä¸è®¾ç½®å¯èƒ½ä¼šå¯¼è‡´å¾—åˆ°çš„gradientä¸º0

input = torch.rand(1, 10, requires_grad=True)
layers = [nn.Linear(10, 10) for _ in range(1000)]


# å®šä¹‰è¦è®¡ç®—çš„å±‚å‡½æ•°ï¼Œå¯ä»¥çœ‹åˆ°æˆ‘ä»¬å®šä¹‰äº†ä¸¤ä¸ª
# ä¸€ä¸ªè®¡ç®—å‰500ä¸ªå±‚ï¼Œå¦ä¸€ä¸ªè®¡ç®—å500ä¸ªå±‚

def run_first_half(*args):
    x = args[0]
    for layer in layers[:500]:
        x = layer(x)
    return x

def run_second_half(*args):
    x = args[0]
    for layer in layers[500:-1]:
        x = layer(x)
    return x

# æˆ‘ä»¬å¼•å…¥æ–°åŠ çš„checkpoint
from torch.utils.checkpoint import checkpoint

x = checkpoint(run_first_half, input)
x = checkpoint(run_second_half, x)
# æœ€åä¸€å±‚å•ç‹¬è°ƒå‡ºæ¥æ‰§è¡Œ
x = layers[-1](x)
x.sum.backward()  # è¿™æ ·å°±å¯ä»¥äº†
```

å¯¹äºSequential-modelæ¥è¯´ï¼Œå› ä¸º`Sequential()`ä¸­å¯ä»¥åŒ…å«å¾ˆå¤šçš„blockï¼Œæ‰€ä»¥å®˜æ–¹æä¾›äº†å¦ä¸€ä¸ªåŠŸèƒ½åŒ…ï¼š

```python
input = torch.rand(1, 10, requires_grad=True)
layers = [nn.Linear(10, 10) for _ in range(1000)]
model = nn.Sequential(*layers)

from torch.utils.checkpoint import checkpoint_sequential

# åˆ†æˆä¸¤ä¸ªéƒ¨åˆ†
num_segments = 2
x = checkpoint_sequential(model, num_segments, input)
x.sum().backward()  # è¿™æ ·å°±å¯ä»¥äº†
```

### 2.4 è·Ÿè¸ªæ˜¾å­˜ä½¿ç”¨æƒ…å†µ

æ˜¾å­˜çš„ä½¿ç”¨æƒ…å†µï¼Œåœ¨ç¼–å†™ç¨‹åºä¸­æˆ‘ä»¬å¯èƒ½æ— æ³•ç²¾ç¡®è®¡ç®—ï¼Œä½†æ˜¯æˆ‘ä»¬å¯ä»¥é€šè¿‡[pynvml](https://github.com/gpuopenanalytics/pynvml)è¿™ä¸ªNvidiaçš„Pythonç¯å¢ƒåº“å’ŒPythonçš„åƒåœ¾å›æ”¶å·¥å…·ï¼Œå¯ä»¥å®æ—¶åœ°æ‰“å°æˆ‘ä»¬ä½¿ç”¨çš„æ˜¾å­˜ä»¥åŠå“ªäº›Tensorä½¿ç”¨äº†æˆ‘ä»¬çš„æ˜¾å­˜ã€‚

ç±»ä¼¼äºä¸‹é¢çš„æŠ¥å‘Šï¼š

```none
# 08-Jun-18-17:56:51-gpu_mem_prof

At __main__ <module>: line 39                        Total Used Memory:399.4  Mb
At __main__ <module>: line 40                        Total Used Memory:992.5  Mb
+ __main__ <module>: line 40                         (1, 1, 682, 700)     1.82 M <class 'torch.Tensor'>
+ __main__ <module>: line 40                         (1, 3, 682, 700)     5.46 M <class 'torch.Tensor'>
At __main__ <module>: line 126                       Total Used Memory:1088.5 Mb
+ __main__ <module>: line 126                        (64, 64, 3, 3)       0.14 M <class 'torch.nn.parameter.Parameter'>
+ __main__ <module>: line 126                        (128, 64, 3, 3)      0.28 M <class 'torch.nn.parameter.Parameter'>
+ __main__ <module>: line 126                        (128, 128, 3, 3)     0.56 M <class 'torch.nn.parameter.Parameter'>
+ __main__ <module>: line 126                        (64, 3, 3, 3)        0.00 M <class 'torch.nn.parameter.Parameter'>
+ __main__ <module>: line 126                        (256, 256, 3, 3)     2.25 M <class 'torch.nn.parameter.Parameter'>
+ __main__ <module>: line 126                        (512, 256, 3, 3)     4.5 M <class 'torch.nn.parameter.Parameter'>
+ __main__ <module>: line 126                        (512, 512, 3, 3)     9.0 M <class 'torch.nn.parameter.Parameter'>
+ __main__ <module>: line 126                        (64,)                0.00 M <class 'torch.nn.parameter.Parameter'>
+ __main__ <module>: line 126                        (1, 3, 682, 700)     5.46 M <class 'torch.Tensor'>
+ __main__ <module>: line 126                        (128,)               0.00 M <class 'torch.nn.parameter.Parameter'>
+ __main__ <module>: line 126                        (256,)               0.00 M <class 'torch.nn.parameter.Parameter'>
+ __main__ <module>: line 126                        (512,)               0.00 M <class 'torch.nn.parameter.Parameter'>
+ __main__ <module>: line 126                        (3,)                 1.14 M <class 'torch.Tensor'>
+ __main__ <module>: line 126                        (256, 128, 3, 3)     1.12 M <class 'torch.nn.parameter.Parameter'>
...
```

ä»¥ä¸‹æ˜¯ç›¸å…³çš„ä»£ç ï¼Œç›®å‰ä»£ç ä¾ç„¶æœ‰äº›åœ°æ–¹éœ€è¦ä¿®æ”¹ï¼Œç­‰ä¿®æ”¹å®Œå–„å¥½æˆ‘ä¼šå°†å®Œæ•´ä»£ç ä»¥åŠä½¿ç”¨è¯´æ˜æ”¾åˆ°githubä¸Šï¼šhttps://github.com/Oldpan/Pytorch-Memory-Utils

```python
import datetime
import linecache
import os

import gc
import pynvml
import torch
import numpy as np


print_tensor_sizes = True
last_tensor_sizes = set()
gpu_profile_fn = f'{datetime.datetime.now():%d-%b-%y-%H:%M:%S}-gpu_mem_prof.txt'

# if 'GPU_DEBUG' in os.environ:
#     print('profiling gpu usage to ', gpu_profile_fn)

lineno = None
func_name = None
filename = None
module_name = None

# fram = inspect.currentframe()
# func_name = fram.f_code.co_name
# filename = fram.f_globals["__file__"]
# ss = os.path.dirname(os.path.abspath(filename))
# module_name = fram.f_globals["__name__"]


def gpu_profile(frame, event):
    # it is _about to_ execute (!)
    global last_tensor_sizes
    global lineno, func_name, filename, module_name

    if event == 'line':
        try:
            # about _previous_ line (!)
            if lineno is not None:
                pynvml.nvmlInit()
                # handle = pynvml.nvmlDeviceGetHandleByIndex(int(os.environ['GPU_DEBUG']))
                handle = pynvml.nvmlDeviceGetHandleByIndex(0)
                meminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)
                line = linecache.getline(filename, lineno)
                where_str = module_name+' '+func_name+':'+' line '+str(lineno)

                with open(gpu_profile_fn, 'a+') as f:
                    f.write(f"At {where_str:<50}"
                            f"Total Used Memory:{meminfo.used/1024**2:<7.1f}Mb\n")

                    if print_tensor_sizes is True:
                        for tensor in get_tensors():
                            if not hasattr(tensor, 'dbg_alloc_where'):
                                tensor.dbg_alloc_where = where_str
                        new_tensor_sizes = {(type(x), tuple(x.size()), np.prod(np.array(x.size()))*4/1024**2,
                                             x.dbg_alloc_where) for x in get_tensors()}
                        for t, s, m, loc in new_tensor_sizes - last_tensor_sizes:
                            f.write(f'+ {loc:<50} {str(s):<20} {str(m)[:4]} M {str(t):<10}\n')
                        for t, s, m, loc in last_tensor_sizes - new_tensor_sizes:
                            f.write(f'- {loc:<50} {str(s):<20} {str(m)[:4]} M {str(t):<10}\n')
                        last_tensor_sizes = new_tensor_sizes
                pynvml.nvmlShutdown()

            # save details about line _to be_ executed
            lineno = None

            func_name = frame.f_code.co_name
            filename = frame.f_globals["__file__"]
            if (filename.endswith(".pyc") or
                    filename.endswith(".pyo")):
                filename = filename[:-1]
            module_name = frame.f_globals["__name__"]
            lineno = frame.f_lineno

            return gpu_profile

        except Exception as e:
            print('A exception occured: {}'.format(e))

    return gpu_profile


def get_tensors():
    for obj in gc.get_objects():
        try:
            if torch.is_tensor(obj):
                tensor = obj
            else:
                continue
            if tensor.is_cuda:
                yield tensor
        except Exception as e:
            print('A exception occured: {}'.format(e))
```

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œlinecacheä¸­çš„getlinesåªèƒ½è¯»å–ç¼“å†²è¿‡çš„æ–‡ä»¶ï¼Œå¦‚æœè¿™ä¸ªæ–‡ä»¶æ²¡æœ‰è¿è¡Œè¿‡åˆ™è¿”å›æ— æ•ˆå€¼ã€‚Python çš„åƒåœ¾æ”¶é›†æœºåˆ¶ä¼šåœ¨å˜é‡æ²¡æœ‰åº”å¼•ç”¨çš„æ—¶å€™ç«‹é©¬è¿›è¡Œå›æ”¶ï¼Œä½†æ˜¯ä¸ºä»€ä¹ˆæ¨¡å‹ä¸­è®¡ç®—çš„ä¸­é—´å˜é‡åœ¨æ‰§è¡Œç»“æŸåè¿˜ä¼šå­˜åœ¨å‘¢ã€‚æ—¢ç„¶éƒ½æ²¡æœ‰å¼•ç”¨äº†ä¸ºä»€ä¹ˆè¿˜ä¼šå ç”¨ç©ºé—´ï¼Ÿ

ä¸€ç§å¯èƒ½çš„æƒ…å†µæ˜¯è¿™äº›å¼•ç”¨ä¸åœ¨Pythonä»£ç ä¸­ï¼Œè€Œæ˜¯åœ¨ç¥ç»ç½‘ç»œå±‚çš„è¿è¡Œä¸­ä¸ºäº†backwardè¢«ä¿å­˜ä¸ºgradientï¼Œè¿™äº›å¼•ç”¨éƒ½åœ¨è®¡ç®—å›¾ä¸­ï¼Œæˆ‘ä»¬åœ¨ç¨‹åºä¸­æ˜¯æ— æ³•çœ‹åˆ°çš„ï¼š

![ã€Šå¦‚ä½•åœ¨Pytorchä¸­ç²¾ç»†åŒ–åˆ©ç”¨æ˜¾å­˜ã€‹](imgs\10.png)



## 3. å†æ¬¡æµ…è°ˆPytorchä¸­çš„æ˜¾å­˜åˆ©ç”¨é—®é¢˜(é™„å®Œå–„æ˜¾å­˜è·Ÿè¸ªä»£ç )

åœ¨**å¦‚ä½•è®¡ç®—æ¨¡å‹ä»¥åŠä¸­é—´å˜é‡çš„æ˜¾å­˜å ç”¨å¤§å°**å’Œ**å¦‚ä½•åœ¨Pytorchä¸­ç²¾ç»†åŒ–åˆ©ç”¨æ˜¾å­˜**ä¸­æˆ‘ä»¬å·²ç»è°ˆè®ºè¿‡äº†å¹³æ—¶ä½¿ç”¨ä¸­æ˜¾å­˜çš„å ç”¨æ¥è‡ªäºå“ªé‡Œï¼Œä»¥åŠå¦‚ä½•åœ¨Pytorchä¸­æ›´å¥½åœ°ä½¿ç”¨æ˜¾å­˜ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å€Ÿç”¨[Pytorch-Memory-Utils](https://github.com/Oldpan/Pytorch-Memory-Utils)è¿™ä¸ªå·¥å…·æ¥æ£€æµ‹æˆ‘ä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å…³äºæ˜¾å­˜çš„å˜åŒ–æƒ…å†µï¼Œåˆ†æå‡ºæˆ‘ä»¬å¦‚ä½•æ­£ç¡®é‡Šæ”¾å¤šä½™çš„æ˜¾å­˜ã€‚

åœ¨æ·±åº¦æ¢ç©¶å‰å…ˆäº†è§£ä¸‹æˆ‘ä»¬çš„è¾“å‡ºä¿¡æ¯ï¼Œé€šè¿‡[Pytorch-Memory-Utils](https://github.com/Oldpan/Pytorch-Memory-Utils)å·¥å…·ï¼Œæˆ‘ä»¬åœ¨ä½¿ç”¨æ˜¾å­˜çš„ä»£ç ä¸­é—´æ’å…¥æ£€æµ‹å‡½æ•°(å¦‚ä½•ä½¿ç”¨è§å·¥å…·githubé¡µé¢å’Œä¸‹æ–‡éƒ¨åˆ†)ï¼Œå°±å¯ä»¥è¾“å‡ºç±»ä¼¼äºä¸‹é¢çš„ä¿¡æ¯ï¼Œ`At __main__ <module>: line 13 Total Used Memory:696.5 Mb`è¡¨ç¤ºåœ¨å½“å‰è¡Œä»£ç æ—¶æ‰€å ç”¨çš„æ˜¾å­˜ï¼Œå³åœ¨æˆ‘ä»¬çš„ä»£ç ä¸­æ‰§è¡Œåˆ°13è¡Œçš„æ—¶å€™æ‰€å æ˜¾å­˜ä¸º695.5Mbã€‚`At __main__ <module>: line 15 Total Used Memory:1142.0 Mb`è¡¨ç¤ºç¨‹åºæ‰§è¡Œåˆ°15è¡Œæ—¶æ‰€å çš„æ˜¾å­˜ä¸º1142.0Mbã€‚ä¸¤æ¡æ•°æ®ä¹‹é—´è¡¨ç¤ºæ‰€å æ˜¾å­˜çš„`tensor`å˜é‡ã€‚

```markdown
# 12-Sep-18-21:48:45-gpu_mem_track.txt

GPU Memory Track | 12-Sep-18-21:48:45 | Total Used Memory:696.5  Mb

At __main__ <module>: line 13                        Total Used Memory:696.5  Mb

+ | 7 * Size:(512, 512, 3, 3)     | Memory: 66.060 M | <class 'torch.nn.parameter.Parameter'>
+ | 1 * Size:(512, 256, 3, 3)     | Memory: 4.7185 M | <class 'torch.nn.parameter.Parameter'>
+ | 1 * Size:(64, 64, 3, 3)       | Memory: 0.1474 M | <class 'torch.nn.parameter.Parameter'>
+ | 1 * Size:(128, 64, 3, 3)      | Memory: 0.2949 M | <class 'torch.nn.parameter.Parameter'>
+ | 1 * Size:(128, 128, 3, 3)     | Memory: 0.5898 M | <class 'torch.nn.parameter.Parameter'>
+ | 8 * Size:(512,)               | Memory: 0.0163 M | <class 'torch.nn.parameter.Parameter'>
+ | 3 * Size:(256, 256, 3, 3)     | Memory: 7.0778 M | <class 'torch.nn.parameter.Parameter'>
+ | 1 * Size:(256, 128, 3, 3)     | Memory: 1.1796 M | <class 'torch.nn.parameter.Parameter'>
+ | 2 * Size:(64,)                | Memory: 0.0005 M | <class 'torch.nn.parameter.Parameter'>
+ | 4 * Size:(256,)               | Memory: 0.0040 M | <class 'torch.nn.parameter.Parameter'>
+ | 2 * Size:(128,)               | Memory: 0.0010 M | <class 'torch.nn.parameter.Parameter'>
+ | 1 * Size:(64, 3, 3, 3)        | Memory: 0.0069 M | <class 'torch.nn.parameter.Parameter'>

At __main__ <module>: line 15                        Total Used Memory:1142.0 Mb

+ | 1 * Size:(60, 3, 512, 512)    | Memory: 188.74 M | <class 'torch.Tensor'>
+ | 1 * Size:(30, 3, 512, 512)    | Memory: 94.371 M | <class 'torch.Tensor'>
+ | 1 * Size:(40, 3, 512, 512)    | Memory: 125.82 M | <class 'torch.Tensor'>

At __main__ <module>: line 21                        Total Used Memory:1550.9 Mb

+ | 1 * Size:(120, 3, 512, 512)   | Memory: 377.48 M | <class 'torch.Tensor'>
+ | 1 * Size:(80, 3, 512, 512)    | Memory: 251.65 M | <class 'torch.Tensor'>

At __main__ <module>: line 26                        Total Used Memory:2180.1 Mb

- | 1 * Size:(120, 3, 512, 512)   | Memory: 377.48 M | <class 'torch.Tensor'> 
- | 1 * Size:(40, 3, 512, 512)    | Memory: 125.82 M | <class 'torch.Tensor'> 

At __main__ <module>: line 32                        Total Used Memory:1676.8 Mb
```

ä½¿ç”¨[Pytorch-Memory-Utils](https://github.com/Oldpan/Pytorch-Memory-Utils)å¾—åˆ°çš„æ˜¾å­˜è·Ÿè¸ªç»“æœã€‚

### 3.1 å¯¼åŒ…

äº†è§£äº†[Pytorch-Memory-Utils](https://github.com/Oldpan/Pytorch-Memory-Utils)å·¥å…·å¦‚ä½•ä½¿ç”¨åï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬é€šè¿‡è‹¥å¹²æ®µç¨‹åºä»£ç æ¥æ¼”ç¤ºåœ¨Pytorchè®­ç»ƒä¸­ï¼š

- å¹³æ—¶çš„æ˜¾å­˜æ˜¯å¦‚ä½•å˜åŒ–çš„ï¼Œåˆ°åº•æ˜¯ä»€ä¹ˆå ç”¨äº†æ˜¾å­˜ã€‚
- å¦‚ä½•å»é‡Šæ”¾ä¸éœ€è¦çš„æ˜¾å­˜ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬åœ¨ä¸‹æ®µä»£ç ä¸­å¯¼å…¥æˆ‘ä»¬éœ€è¦çš„åº“ï¼Œéšåå¼€å§‹æˆ‘ä»¬çš„æ˜¾å­˜æ£€æµ‹ç¨‹åºã€‚

```python
import torch
import inspect

from torchvision import models
from gpu_mem_track import MemTracker  # å¼•ç”¨æ˜¾å­˜è·Ÿè¸ªä»£ç 

device = torch.device('cuda:0')

frame = inspect.currentframe()     
gpu_tracker = MemTracker(frame)      # åˆ›å»ºæ˜¾å­˜æ£€æµ‹å¯¹è±¡

gpu_tracker.track()                  # å¼€å§‹æ£€æµ‹
```

### 3.2 é¢„è®­ç»ƒæƒé‡æ¨¡å‹

é¦–å…ˆæˆ‘ä»¬æ£€æµ‹ä¸€ä¸‹**ç¥ç»ç½‘ç»œæ¨¡å‹æƒé‡**æ‰€å ç”¨çš„æ˜¾å­˜ä¿¡æ¯ï¼Œä¸‹é¢ä»£ç ä¸­æˆ‘ä»¬å°è¯•åŠ è½½`VGG19`è¿™ä¸ªç»å…¸çš„ç½‘ç»œæ¨¡å‹ï¼Œå¹¶ä¸”å¯¼å…¥é¢„è®­ç»ƒå¥½çš„æƒé‡ã€‚

```python
gpu_tracker.track()
cnn = models.vgg19(pretrained=True).to(device)  # å¯¼å…¥VGG19æ¨¡å‹å¹¶ä¸”å°†æ•°æ®è½¬åˆ°æ˜¾å­˜ä¸­
gpu_tracker.track()
```

ç„¶åå¯ä»¥å‘ç°ç¨‹åºè¿è¡Œè¿‡ç¨‹ä¸­çš„æ˜¾å­˜å˜åŒ–ï¼ˆç¬¬ä¸€è¡Œæ˜¯è½½å…¥å‰çš„æ˜¾å­˜ï¼Œæœ€åä¸€è¡Œæ˜¯è½½å…¥åçš„æ˜¾å­˜ï¼‰ï¼š

```markdown
At __main__ <module>: line 13                        Total Used Memory:472.2  Mb

+ | 1 * Size:(128, 64, 3, 3)      | Memory: 0.2949 M | <class 'torch.nn.parameter.Parameter'>
+ | 1 * Size:(256, 128, 3, 3)     | Memory: 1.1796 M | <class 'torch.nn.parameter.Parameter'>
+ | 1 * Size:(64, 64, 3, 3)       | Memory: 0.1474 M | <class 'torch.nn.parameter.Parameter'>
+ | 2 * Size:(4096,)              | Memory: 0.0327 M | <class 'torch.nn.parameter.Parameter'>
+ | 1 * Size:(512, 256, 3, 3)     | Memory: 4.7185 M | <class 'torch.nn.parameter.Parameter'>
+ | 2 * Size:(128,)               | Memory: 0.0010 M | <class 'torch.nn.parameter.Parameter'>
+ | 1 * Size:(1000, 4096)         | Memory: 16.384 M | <class 'torch.nn.parameter.Parameter'>
+ | 6 * Size:(512,)               | Memory: 0.0122 M | <class 'torch.nn.parameter.Parameter'>
+ | 1 * Size:(64, 3, 3, 3)        | Memory: 0.0069 M | <class 'torch.nn.parameter.Parameter'>
+ | 1 * Size:(4096, 25088)        | Memory: 411.04 M | <class 'torch.nn.parameter.Parameter'>
+ | 1 * Size:(4096, 4096)         | Memory: 67.108 M | <class 'torch.nn.parameter.Parameter'>
+ | 5 * Size:(512, 512, 3, 3)     | Memory: 47.185 M | <class 'torch.nn.parameter.Parameter'>
+ | 2 * Size:(64,)                | Memory: 0.0005 M | <class 'torch.nn.parameter.Parameter'>
+ | 3 * Size:(256,)               | Memory: 0.0030 M | <class 'torch.nn.parameter.Parameter'>
+ | 1 * Size:(128, 128, 3, 3)     | Memory: 0.5898 M | <class 'torch.nn.parameter.Parameter'>
+ | 2 * Size:(256, 256, 3, 3)     | Memory: 4.7185 M | <class 'torch.nn.parameter.Parameter'>
+ | 1 * Size:(1000,)              | Memory: 0.004 M | <class 'torch.nn.parameter.Parameter'>

At __main__ <module>: line 15                        Total Used Memory:1387.5 Mb
```

é€šè¿‡ä¸Šé¢çš„æŠ¥å‘Šï¼Œå¾ˆå®¹æ˜“å‘ç°ä¸€ä¸ªé—®é¢˜ã€‚

é¦–å…ˆæˆ‘ä»¬çŸ¥é“VGG19æ‰€æœ‰å±‚çš„æƒé‡å¤§å°åŠ èµ·æ¥å¤§çº¦æ˜¯**548M**(è¿™ä¸ªæ•°å€¼æ¥æºäºPytorchå®˜æ–¹æä¾›çš„VGG19æƒé‡æ–‡ä»¶å¤§å°)ï¼Œæˆ‘ä»¬å°†ä¸Šé¢æŠ¥å‘Šæ‰“å°çš„Tensor-Memoryä¹Ÿéƒ½åŠ èµ·æ¥ç®—ä¸‹æ¥ä¹Ÿå·®ä¸å¤š**551.8Mb**ã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬ç®—äº†ä¸¤æ¬¡æ‰“å°çš„æ˜¾å­˜å®é™…å ç”¨ä¸­ï¼š**1387.5 â€“ 472.2 = 915.3 MB**ã€‚

å”‰ï¼Œæ€ä¹ˆå¤šç”¨äº†å·®ä¸å¤š400Mbå‘¢ï¼Ÿæ˜¯ä¸æ˜¯æŠ¥å‘Šå‡ºä»€ä¹ˆé—®é¢˜äº†ã€‚

è¿™æ ·ï¼Œæˆ‘ä»¬å†åŠ ç‚¹Tensorè¯•ä¸€ä¸‹ã€‚

```python
...

gpu_tracker.track()
cnn = models.vgg19(pretrained=True).to(device)
gpu_tracker.track()
# ä¸Šæ–¹ä¸ºä¹‹å‰çš„ä»£ç 

# æ–°å¢åŠ çš„tensor
dummy_tensor_1 = torch.randn(30, 3, 512, 512).float().to(device)  # 30*3*512*512*4/1000/1000 = 94.37M
dummy_tensor_2 = torch.randn(40, 3, 512, 512).float().to(device)  # 40*3*512*512*4/1000/1000 = 125.82M
dummy_tensor_3 = torch.randn(60, 3, 512, 512).float().to(device)  # 60*3*512*512*4/1000/1000 = 188.74M

gpu_tracker.track()   # å†æ¬¡æ‰“å°
```

å¦‚ä¸Šé¢çš„ä»£ç ï¼Œæˆ‘ä»¬åˆåŠ å…¥äº†ä¸‰ä¸ªTensorï¼Œå…¨éƒ¨æ”¾åˆ°æ˜¾å­˜ä¸­ã€‚æŠ¥å‘Šå¦‚ä¸‹ï¼š

```markdown
At __main__ <module>: line 15                        Total Used Memory:1387.5 Mb

+ | 1 * Size:(30, 3, 512, 512)    | Memory: 94.371 M | <class 'torch.Tensor'>
+ | 1 * Size:(40, 3, 512, 512)    | Memory: 125.82 M | <class 'torch.Tensor'>
+ | 1 * Size:(60, 3, 512, 512)    | Memory: 188.74 M | <class 'torch.Tensor'>

At __main__ <module>: line 21                        Total Used Memory:1807.0 Mb
```

ä¸Šé¢çš„æŠ¥å‘Šå°±æ¯”è¾ƒæ­£å¸¸äº†ï¼š**94.3 + 125.8 + 188.7 = 408.8** çº¦ç­‰äº **1807.0 â€“ 1387.5 = 419.5**ï¼Œè¯¯å·®å¯ä»¥å¿½ç•¥ï¼Œå› ä¸ºè‚¯å®šä¼šå­˜åœ¨ä¸€äº›å¼€é”€ä½¿ç”¨çš„æ˜¾å­˜ã€‚

é‚£ä¹‹å‰æ˜¯ä»€ä¹ˆæƒ…å†µï¼Ÿæ˜¯ä¸æ˜¯æ¨¡å‹çš„æƒé‡ä¿¡æ¯å å¾—æ˜¾å­˜å°±ç¨å¾®å¤šä¸€ç‚¹ï¼Ÿ

è¿™æ ·ï¼Œæˆ‘ä»¬å°†è½½å…¥VGG19æ¨¡å‹çš„ä»£ç æ³¨é‡Šæ‰ï¼Œåªå¯¹åé¢çš„ä¸‰ä¸ªTensorè¿›è¡Œæ£€æµ‹ã€‚

```python
...

gpu_tracker.track()
# cnn = models.vgg19(pretrained=True).to(device)   æ³¨é‡Šæ‰è¯»æƒé‡ä»£ç 
gpu_tracker.track()

...
```

å¯ä»¥å‘ç°ï¼š

```markdown
GPU Memory Track | 15-Sep-18-13:59:03 | Total Used Memory:513.3  Mb


At __main__ <module>: line 13                        Total Used Memory:513.3  Mb


At __main__ <module>: line 15                        Total Used Memory:513.3  Mb


At __main__ <module>: line 18                        Total Used Memory:513.3  Mb

+ | 1 * Size:(60, 3, 512, 512)    | Memory: 188.74 M | <class 'torch.Tensor'>
+ | 1 * Size:(30, 3, 512, 512)    | Memory: 94.371 M | <class 'torch.Tensor'>
+ | 1 * Size:(40, 3, 512, 512)    | Memory: 125.82 M | <class 'torch.Tensor'>

At __main__ <module>: line 24                        Total Used Memory:1271.3 Mb
```

åŒæ ·ï¼Œæ˜¾å­˜å ç”¨æ¯”æ‰€åˆ—å‡ºæ¥çš„Tensorå ç”¨å¤§ï¼Œ**æˆ‘ä»¬æš‚æ—¶å°†æ¬¡å½’ç»“ä¸ºPytorchåœ¨å¼€å§‹è¿è¡Œç¨‹åºæ—¶éœ€è¦é¢å¤–çš„æ˜¾å­˜å¼€é”€ï¼Œè¿™ç§é¢å¤–çš„æ˜¾å­˜å¼€é”€ä¸æˆ‘ä»¬å®é™…ä½¿ç”¨çš„æ¨¡å‹æƒé‡æ˜¾å­˜å¤§å°æ— å…³ã€‚**

### 3.3 Pytorchä½¿ç”¨çš„æ˜¾å­˜ç­–ç•¥

Pytorchå·²ç»å¯ä»¥è‡ªåŠ¨å›æ”¶æˆ‘ä»¬â€œä¸ç”¨çš„â€æ˜¾å­˜ï¼Œç±»ä¼¼äºpythonçš„å¼•ç”¨æœºåˆ¶ï¼Œå½“æŸä¸€å†…å­˜å†…çš„æ•°æ®ä¸å†æœ‰ä»»ä½•å˜é‡å¼•ç”¨æ—¶ï¼Œè¿™éƒ¨åˆ†çš„å†…å­˜ä¾¿ä¼šè¢«é‡Šæ”¾ã€‚ä½†æœ‰ä¸€ç‚¹éœ€è¦æ³¨æ„ï¼Œå½“æˆ‘ä»¬æœ‰ä¸€éƒ¨åˆ†æ˜¾å­˜ä¸å†ä½¿ç”¨çš„æ—¶å€™ï¼Œè¿™éƒ¨åˆ†é‡Šæ”¾çš„æ˜¾å­˜é€šè¿‡`Nvidia-smi`å‘½ä»¤æ˜¯çœ‹ä¸åˆ°çš„ï¼Œä¸¾ä¸ªä¾‹å­ï¼š

```python
device = torch.device('cuda:0')
# å®šä¹‰ä¸¤ä¸ªtensor
dummy_tensor_4 = torch.randn(120, 3, 512, 512).float().to(device)  # 120*3*512*512*4/1000/1000 = 377.48M
dummy_tensor_5 = torch.randn(80, 3, 512, 512).float().to(device)  # 80*3*512*512*4/1000/1000 = 251.64M

# ç„¶åé‡Šæ”¾
dummy_tensor_4 = dummy_tensor_4.cpu()
dummy_tensor_2 = dummy_tensor_2.cpu()
# è¿™é‡Œè™½ç„¶å°†ä¸Šé¢çš„æ˜¾å­˜é‡Šæ”¾äº†ï¼Œä½†æ˜¯æˆ‘ä»¬é€šè¿‡Nvidia-smiå‘½ä»¤çœ‹åˆ°æ˜¾å­˜ä¾ç„¶åœ¨å ç”¨
torch.cuda.empty_cache()
# åªæœ‰æ‰§è¡Œå®Œä¸Šé¢è¿™å¥ï¼Œæ˜¾å­˜æ‰ä¼šåœ¨Nvidia-smiä¸­é‡Šæ”¾
```

Pytorchçš„å¼€å‘è€…ä¹Ÿå¯¹æ­¤è¿›è¡Œè¯´æ˜äº†ï¼Œè¿™éƒ¨åˆ†é‡Šæ”¾åçš„æ˜¾å­˜å¯ä»¥ç”¨ï¼Œåªä¸è¿‡ä¸åœ¨Nvidia-smiä¸­æ˜¾ç¤ºç½¢äº†ã€‚

![ã€Šå†æ¬¡æµ…è°ˆPytorchä¸­çš„æ˜¾å­˜åˆ©ç”¨é—®é¢˜(F:/GitHub/README-1/notes/dataloader/imgs/11.png)ã€‹](imgs/11.png)

### 3.4 å…³äºæ¨¡å‹è°ƒç”¨

`torch.no_grad()`æ˜¯Pytorch-0.4ç‰ˆæœ¬æ—¶å€™æ›´æ–°çš„åŠŸèƒ½ï¼Œåœ¨æ­¤è¯­å¥çš„ä½œç”¨åŸŸä¸‹ï¼Œæ‰€æœ‰çš„tensorè¿ç®—ä¸ä¼šä¿å­˜æ¢¯åº¦å€¼ï¼Œç‰¹åˆ«é€‚åˆåœ¨`inference`çš„æ—¶å€™ä½¿ç”¨ï¼Œä»£æ›¿æ—§ç‰ˆæœ¬çš„`volatile`ã€‚

ç”¨ä¸€æ®µä»£ç æ¼”ç¤ºä¸‹ï¼Œè¿™é‡Œæˆ‘ä»¬æ ¹æ®VGG19ç½‘ç»œæ„é€ ä¸€ä¸ªç‰¹å¾æå–å™¨ï¼Œåˆ†åˆ«æå–`content_image`å’Œ`style_image`çš„ç‰¹å¾å›¾ï¼Œç„¶åå°†æå–çš„ç‰¹å¾å›¾å­˜åœ¨ä¸¤ä¸ªlistä¸­,æˆ‘ä»¬ä½¿ç”¨äº†`with torch.no_grad()`è¯­å¥(åœ¨æ²¡ä½¿ç”¨`no_grad`ä¹‹å‰å ç”¨çš„æ˜¾å­˜æ›´å¤šï¼Œä¸è¿‡è¿™é‡Œä¸è¿›è¡Œå±•ç¤ºäº†)ï¼š

```python
gpu_tracker.track()

layers = ['relu_1', 'relu_3', 'relu_5', 'relu_9']    # æå–çš„å±‚æ•°
layerIdx = 0

content_image = torch.randn(1, 3, 500, 500).float().to(device)
style_image = torch.randn(1, 3, 500, 500).float().to(device)
feature_extractor = nn.Sequential().to(device)           # ç‰¹å¾æå–å™¨
cnn = models.vgg19(pretrained=True).features.to(device)  # é‡‡å–VGG19


input_features = []      # ä¿å­˜æå–å‡ºçš„features
target_features = []     # ä¿å­˜æå–å‡ºçš„features
i = 0
# å¦‚æœä¸åŠ ä¸‹é¢è¿™ä¸€å¥,é‚£ä¹ˆæ˜¾å­˜çš„å ç”¨æå‡,å› ä¸ºä¿å­˜äº†ä¸­é—´è®¡ç®—çš„æ¢¯åº¦å€¼
with torch.no_grad():
    for layer in cnn.children():
        if layerIdx < len(layers):
            if isinstance(layer, nn.Conv2d):
                i += 1
                name = "conv_" + str(i)
                feature_extractor.add_module(name, layer)
            elif isinstance(layer, nn.MaxPool2d):
                name = "pool_" + str(i)
                feature_extractor.add_module(name, layer)
            elif isinstance(layer, nn.ReLU):
                name = "relu_" + str(i)
                feature_extractor.add_module(name, nn.ReLU(inplace=True))
            if name == layers[layerIdx]:
                input = feature_extractor(content_image)
                gpu_tracker.track()
                target = feature_extractor(style_image)
                gpu_tracker.track()

                input_features.append(input)
                target_features.append(target)

                del input
                del target

                layerIdx += 1

gpu_tracker.track()
```

è¿›è¡ŒGPUè·Ÿè¸ªåï¼Œè§‚å¯Ÿä¸‹æ˜¾å­˜å˜åŒ–ï¼š

```markdown
At __main__ <module>: line 33                        Total Used Memory:1313.3 Mb

+ | 2 * Size:(64,)                | Memory: 0.0005 M | <class 'torch.nn.parameter.Parameter'>
+ | 2 * Size:(1, 3, 500, 500)     | Memory: 6.0 M | <class 'torch.Tensor'>
+ | 1 * Size:(64, 64, 3, 3)       | Memory: 0.1474 M | <class 'torch.nn.parameter.Parameter'>
+ | 1 * Size:(128, 64, 3, 3)      | Memory: 0.2949 M | <class 'torch.nn.parameter.Parameter'>
+ | 2 * Size:(128,)               | Memory: 0.0010 M | <class 'torch.nn.parameter.Parameter'>
+ | 2 * Size:(1, 256, 125, 125)   | Memory: 32.0 M | <class 'torch.Tensor'>
+ | 1 * Size:(128, 128, 3, 3)     | Memory: 0.5898 M | <class 'torch.nn.parameter.Parameter'>
+ | 7 * Size:(512, 512, 3, 3)     | Memory: 66.060 M | <class 'torch.nn.parameter.Parameter'>
+ | 3 * Size:(256, 256, 3, 3)     | Memory: 7.0778 M | <class 'torch.nn.parameter.Parameter'>
+ | 2 * Size:(1, 512, 62, 62)     | Memory: 15.745 M | <class 'torch.Tensor'>
+ | 1 * Size:(64, 3, 3, 3)        | Memory: 0.0069 M | <class 'torch.nn.parameter.Parameter'>
+ | 2 * Size:(1, 128, 250, 250)   | Memory: 64.0 M | <class 'torch.Tensor'>
+ | 8 * Size:(512,)               | Memory: 0.0163 M | <class 'torch.nn.parameter.Parameter'>
+ | 4 * Size:(256,)               | Memory: 0.0040 M | <class 'torch.nn.parameter.Parameter'>
+ | 1 * Size:(256, 128, 3, 3)     | Memory: 1.1796 M | <class 'torch.nn.parameter.Parameter'>
+ | 1 * Size:(512, 256, 3, 3)     | Memory: 4.7185 M | <class 'torch.nn.parameter.Parameter'>
+ | 2 * Size:(1, 64, 500, 500)    | Memory: 128.0 M | <class 'torch.Tensor'>

At __main__ <module>: line 76                        Total Used Memory:1932.0 Mb
```

ä¸Šè¡¨ä¸­4*2ä¸ª`<class 'torch.Tensor'>`æ˜¯æå–å‡ºçš„ç‰¹å¾å›¾ï¼Œå…¶ä»–çš„`<class 'torch.nn.parameter.Parameter'>`åˆ™æ˜¯æ¨¡å‹çš„æƒé‡å€¼ï¼Œä½†æ˜¯å‘ç°ï¼Œæ‰€æœ‰çš„å€¼åŠ èµ·æ¥ï¼Œä¸æ€»æ˜¾å­˜å˜åŒ–åˆä¸åŒï¼Œé‚£ç©¶ç«Ÿå¤šäº†å“ªäº›å ç”¨æ˜¾å­˜çš„ä¸œè¥¿ï¼Ÿ

å…¶å®åŸå› å¾ˆç®€å•ï¼Œé™¤äº†åœ¨ç¨‹åºè¿è¡Œæ—¶çš„ä¸€äº›é¢å¤–æ˜¾å­˜å¼€é”€ï¼Œå¦å¤–ä¸€ä¸ªå ç”¨æ˜¾å­˜çš„ä¸œè¥¿å°±æ˜¯æˆ‘ä»¬åœ¨è®¡ç®—æ—¶å€™çš„ä¸´æ—¶ç¼“å†²å€¼ï¼Œè¿™äº›é›¶é›¶æ€»æ€»ä¹Ÿä¼šå ç”¨ä¸€éƒ¨åˆ†æ˜¾å­˜ï¼Œå¹¶ä¸”è¿™äº›ç¼“å†²å€¼é€šè¿‡Pythonçš„åƒåœ¾æ”¶é›†æ˜¯æ”¶é›†ä¸åˆ°çš„ã€‚

### 3.5 Asynchronous execution

åšè¿‡å¹¶è¡Œè®¡ç®—æˆ–è€…æ“ä½œç³»ç»Ÿçš„åŒå­¦å¯èƒ½çŸ¥é“ï¼ŒGPUçš„è®¡ç®—æ–¹å¼ä¸€èˆ¬æ˜¯å¼‚æ­¥çš„ã€‚å¼‚æ­¥è¿ç®—ä¸åƒåŒæ­¥è¿ç®—é‚£æ ·æ˜¯æŒ‰ç…§é¡ºåºä¸€æ­¥ä¸€æ­¥æ¥ï¼Œå¼‚æ­¥æ˜¯åŒæ—¶è¿›è¡Œçš„ï¼Œå¼‚æ­¥è®¡ç®—ä¸­ï¼Œä¸¤ç§ä¸ä¸€æ ·çš„æ“ä½œå¯èƒ½ä¼šå‘ç”ŸåŒæ—¶è§¦å‘çš„æƒ…å†µï¼Œè¿™æ˜¯å¤„ç†ä¸¤è€…é—´çš„å‰åå…³ç³»ã€ä¾èµ–å…³ç³»æˆ–è€…å†²çªå…³ç³»å°±æ¯”è¾ƒé‡è¦äº†ã€‚

æœ‰ä¸€ä¸ªä¼—æ‰€å‘¨çŸ¥çš„å°æŠ€å·§ï¼Œåœ¨æ‰§è¡Œè®­ç»ƒç¨‹åºçš„æ—¶å€™å°†ç¯å¢ƒå˜é‡`CUDA_LAUNCH_BLOCKING=1`è®¾ä¸º1(å¼ºåˆ¶åŒæ­¥)å¯ä»¥å‡†ç¡®å®šä½è§‚å¯Ÿåˆ°æˆ‘ä»¬æ˜¾å­˜æ“ä½œçš„é”™è¯¯ä»£ç è¡Œæ•°ã€‚



## 4. èŠ‚çº¦æ˜¾å­˜

> ç›®å½•: é‚£äº› 0 æˆæœ¬èŠ‚çœæ˜¾å­˜çš„æ–¹æ³•
>
> 1. æµ®ç‚¹æ··åˆè¿ç®— (èŠ‚çœ 20% å·¦å³) 
> 2. ä¼˜åŒ–æ¶æ„ä»£ç  (èŠ‚çœ 10% ~ 40% ä¸ç­‰)
> 3. è¯»å–æ•°æ®å¹¶è¡Œæ“ä½œ (èŠ‚çœ 0% ~ 50% ä¸ç­‰)



åŸºæœ¬æ“ä½œ check list:

- relu ç”¨ `inplace=True`
- ç”¨ `eval()` å’Œ `with torch.no_grad():`
- æ¯ä¸ª batch åè®¤çœŸçš„æŠŠæ‰€æœ‰å‚æ•°ä» GPU æ‹¿å‡ºæ¥ååˆ é™¤ 
- **è™½ç„¶å¾ˆå¤šå›ç­”å»ºè®®ç”¨, ä½†æˆ‘å»ºè®®ä¸è¦ç”¨ `torch.cuda.empty_cache()`** , è¿™åªæ˜¯é‡Šæ”¾ GPU ç¼“å­˜è€Œä½¿å¾— `nvidia-smi` èƒ½çœ‹å¾—è§ pytorch è‡ªåŠ¨é‡Šæ”¾çš„å†…å­˜è€Œå·². **99% çš„ç”¨æˆ·ä¸éœ€è¦ä½¿ç”¨è¿™ä¸ªå‘½ä»¤.** å¹¶æœ‰ç”¨æˆ·ååº”æ¯æ¬¡ç”¨**åè€Œä¼šå‡æ…¢ 1~2s**.
- æ³¨æ„: å½“**æ¯å¼  GPU é‡Œé¢çš„** batch_size å¤ªå°(<8)æ—¶ç”¨ batch_norm ä¼šå¯¼è‡´è®­ç»ƒä¸ç¨³å®š, é™¤éä½ ç”¨ä»¥ä¸‹æ‰€è¯´çš„ APEX æ¥å®ç°å¤š GPU `sync_bn`
- `torch.backends.cudnn.deterministic = True` ç”¨ä¸ç”¨å¯¹ GPU å†…å­˜å ç”¨å’Œæ•ˆç‡éƒ½æ²¡æœ‰ä»€ä¹ˆå¤ªå¤§çš„å½±å“. å»ºè®®å¼€ç€.
- ä¸è¦ç”¨ `.cpu()` æ¥å– GPU é‡Œé¢å‡ºæ¥çš„å›¾ç‰‡. è¿™æ ·åšçš„è¯è®­ç»ƒæ—¶é•¿å¯èƒ½ç¿»å€. 

ä»¥ä¸‹ä»‹ç»è¿›é˜¶æ“ä½œ

### 4.1  æµ®ç‚¹æ··åˆè¿ç®—

**ä½¿ç”¨ç¯å¢ƒ**: æ‰€æœ‰. å¯¹

**å®ç°**: ä½¿ç”¨ APEX: [https://github.com/NVIDIA/apex](https://www.readercache.com/redirect?url=https%3A//github.com/NVIDIA/apex)

å®‰è£…åªéœ€è¦ä¸‰è¡Œå‘½ä»¤: **ä½†æœ‰å‘, è¯·æŒ‰ç…§æ­¤æ•™ç¨‹ä¸€æ­¥ä¸€æ­¥æ¥**

```bash
$ git clone https://github.com/NVIDIA/apex
$ cd apex
$ pip install -v --no-cache-dir --global-option="--pyprof" --global-option="--cpp_ext" --global-option="--cuda_ext" ./
```

### 4.2 **ä¼˜åŒ–æ¶æ„ä»£ç **

**TL;DR**: ç”¨ `del` æˆ– `replacing variables` çš„æ–¹æ³•åƒå‡å°‘ cpu å†…å­˜å ç”¨ä¸€æ ·å‡å°‘ gpu æ˜¾å­˜å ç”¨.

**ä½¿ç”¨ç¯å¢ƒ**: è‡ªå®šä¹‰æˆ–å¤æ‚çš„ç½‘ç»œæ¶æ„; ç‰¹åˆ«æ˜¯ unet ç­‰éœ€è¦å†åˆ©ç”¨ `feature_map` çš„ç½‘ç»œæ¶æ„



![img](imgs\16.png)

**é—®é¢˜**: å¦‚å›¾, å¤§éƒ¨åˆ†çš„æ˜¾å­˜éƒ½è¢« `feature_map` å ç”¨ç”¨æ¥ `back prop`, è¯´æ˜ input çš„å¤§å°å’Œæ¨¡å‹çš„å¤§å°åŸºæœ¬ä¸Šå°±èƒ½å†³å®šæ˜¾å­˜çš„å ç”¨. ä½†æ˜¯ `forward` è®¡ç®—è¿‡ç¨‹ä¸­èƒ½ä¸èƒ½å·å·¥å‡æ–™?

**åŸç†**: ä¼—æ‰€å‘¨çŸ¥ pytorch æ¶æ„ä¸­çš„ forward æ˜¯å®é™…ä¸Š tensor åœ¨ gpu çš„èµ°æ³•ï¼Œé‚£ä¹ˆ forward ä¸­çš„ä»£ç å°±ä¼šå½±å“ gpu çš„ä½¿ç”¨.

**å®ç°**: ç ”ç©¶ pytorch å®˜æ–¹æ¶æ„å°±ä¼šå‘ç°å¤§éƒ¨åˆ† forward pass éƒ½æ˜¯ `x = self.conv(x)` çš„å½¢å¼, å¾ˆå°‘ introduce new variable. æ‰€ä»¥: (1) æŠŠä¸éœ€è¦çš„å˜é‡éƒ½ç”± `x` ä»£æ›¿; (2) å˜é‡ç”¨å®Œåç”¨ `del` åˆ é™¤.

**ä¾‹å­**

```python3
def forward(self, x):
    conv2 = self.conv2(self.conv1(x)) #1/4
    del x
    conv3 = self.conv3(conv2) #1/8
    conv4 = self.conv4(conv3) #1/16
    conv5 = self.conv5(conv4) #1/32

    center_64 = self.center_conv1x1(self.center_global_pool(conv5))

    d5 = self.decoder5(self.center(conv5), conv5)
    del conv5
    d4 = self.decoder4(d5, conv4)
    del conv4
    d3 = self.decoder3(d4, conv3)
    del conv3
    d2 = self.decoder2(d3, conv2)
    del conv2

    hypercol = F.dropout2d(torch.cat((
        self.decoder1(d2),
        F.upsample(d2, scale_factor=2,mode='bilinear'),
        F.upsample(d3, scale_factor=4, mode='bilinear'),
        F.upsample(d4, scale_factor=8, mode='bilinear'),
        F.upsample(d5, scale_factor=16, mode='bilinear')),1), p=0.50)

    hypercol_add_center = self.logits_final(torch.cat((
        hypercol,
        F.upsample(center_64, scale_factor=hypercol.shape[2],mode='bilinear')),1))

    return self.center_fc(center_64.view(center_64.size(0), -1)), self.logits_no_empty(hypercol), hypercol_add_center
```

*(æ­¤æ–¹æ³•ç»è¿‡å•å˜é‡æµ‹è¯•ç¡®å®æœ‰æ•ˆ-æˆåŠŸæ”¾å…¥1024x1024çš„å›¾ç‰‡-, ä½†ç†è®ºå°šæœªä¸¥æ ¼è¯æ˜, å¦‚æœ‰è¯´é”™è¯·æŒ‡å‡º. æœ¬æ–‡åªè§£å†³æ˜¾å­˜é—®é¢˜, å¯¹æ˜¾å¡åˆ©ç”¨æ•ˆç‡æ²¡æœ‰ç ”ç©¶. å¯èƒ½ä¼šå½±å“ backprop é€Ÿåº¦.)*

*Other Resources:*

[Manage memory differently on train and test time pytorch](https://stackoverflow.com/questions/55667005/manage-memory-differently-on-train-and-test-time-pytorch)

[How to Train a Very Large and Deep Model on One GPU? ](https://medium.com/syncedreview/how-to-train-a-very-large-and-deep-model-on-one-gpu-7b7edfe2d072)



### 4.3 è¯»å–æ•°æ®å¹¶è¡Œæ“ä½œ

æŒ‰ç…§ä»¥ä¸‹æ–¹å¼è®¾ç½® pytorch çš„ train_loader:

- num_workers: CPU ä½¿ç”¨çº¿ç¨‹. ä¸€èˆ¬å»ºè®®è¿™ä¸ªå€¼å¡«å†™ä½ æœºå™¨æ€»å…± CPU çš„æ•°é‡
- pin_memory: æ˜¯å¦å…ˆæŠŠæ•°æ®åŠ è½½åˆ°ç¼“å­˜å†åŠ è½½åˆ°GPU. å¦‚æœä½ ç”¨çš„ä¸æ˜¯ä½ ç§äººå·¥ä½œç”µè„‘, è¯·å¼€å¯.
- drop_last: å¦‚æœæ˜¯è¿™æ˜¯è®­ç»ƒä½¿ç”¨çš„ dataset, è¯·å¼€å¯, è¿™æ ·æœ€åä¸€ä¸ª batch å¦‚æœå°äºä½ çš„ batch_size, ä¼šæ‰”æ‰, è¿™æ ·è®­ç»ƒå°±ä¼šæ›´ç¨³å®š.

```python3
data_loader = data.DataLoader(YOUR_PYTORCH_DATASET,
                              num_workers=THE_NUMBER_OF_CPU_I_HAVE,
                              pin_memory=True,
                              drop_last=True,  # Last batch will mess up with batch norm https://github.com/pytorch/pytorch/issues/4534
                              ))
```

å¦‚æœä½ æŒ‰ç…§ä¸Šé¢çš„æ–¹æ³•æŠŠ `pin_memory` å¼€å¯äº†çš„è¯, è¯·æ•°æ®æ”¾å…¥ GPU çš„æ—¶å€™æŠŠ `non_blocking` å¼€å¯. è¿™æ ·å¦‚æœä½ åªæŠŠæ•°æ®æ”¾å…¥ GPU è€Œä¸æŠŠæ•°æ®ä» GPU æ‹¿å‡ºæ¥å†åšè®¡ç®—çš„è¯å°±ä¼šåŠ å¿«å¾ˆå¤š (æ®ç”¨æˆ·æŠ¥å‘Šå¯åŠ é€Ÿ 50%). å°±ç®—ä½ æŠŠ GPU ä¸­æ•°æ®æ‹¿å‡ºæ¥ (ie. ç”¨äº† `.cpu()` å‘½ä»¤, æœ€åçš„ç»“æœä¹Ÿæ˜¯ä¸ `non_blocking=False` ç›¸å½“:

```python3
"""Sync Point"""
image = image.cuda(non_blocking=True)
labels = labels.cuda(non_blocking=True).float()

"""Async Point"""
prediction = net(image)
```



## 5. èŠ‚çœæ˜¾å­˜å°æŠ€å·§

### 5.1 [Training Deep Nets with Sublinear Memory Cost](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1604.06174) 

å’¦ï¼Œå¤§å®¶éƒ½æ²¡çœ‹è¿‡é™ˆå¤©å¥‡çš„ [Training Deep Nets with Sublinear Memory Cost](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1604.06174) å—ï¼Ÿ

è®­ç»ƒ CNN æ—¶ï¼ŒMemory ä¸»è¦çš„å¼€é”€æ¥è‡ªäºå‚¨å­˜ç”¨äºè®¡ç®— backward çš„ activationï¼Œä¸€èˆ¬çš„ workflow æ˜¯è¿™æ ·çš„ 

![img](imgs\12.gif)

Vanilla backprop



å¯¹äºä¸€ä¸ªé•¿åº¦ä¸º N çš„ CNNï¼Œéœ€è¦ O(N) çš„å†…å­˜ã€‚è¿™ç¯‡è®ºæ–‡ç»™å‡ºäº†ä¸€ä¸ªæ€è·¯ï¼Œæ¯éš” sqrt(N) ä¸ª node å­˜ä¸€ä¸ª activationï¼Œä¸­éœ€è¦çš„æ—¶å€™å†ç®—ï¼Œè¿™æ ·æ˜¾å­˜å°±ä» O(N) é™åˆ°äº† O(sqrt(N))ã€‚

![](imgs\13.gif)

Checkpointed backprop



å¯¹äºè¶Šæ·±çš„æ¨¡å‹ï¼Œè¿™ä¸ªæ–¹æ³•çœçš„æ˜¾å­˜å°±è¶Šå¤šï¼Œä¸”é€Ÿåº¦ä¸ä¼šæ˜æ˜¾å˜æ…¢ã€‚

![img](imgs\14.png)



PyTorch æˆ‘å®ç°äº†ä¸€ç‰ˆï¼Œæœ‰å…´è¶£çš„åŒå­¦å¯ä»¥æ¥[è¯•è¯•]( https://github.com/TD-4/pytorch-memonger)

### 5.2 inplace=True

 å°½å¯èƒ½ä½¿ç”¨inplaceæ“ä½œï¼Œ æ¯”å¦‚relu å¯ä»¥ä½¿ç”¨ inplace=True ã€‚ä¸€ä¸ªç®€å•çš„ä½¿ç”¨æ–¹æ³•ï¼Œå¦‚ä¸‹ï¼š

```text
def inplace_relu(m):
    classname = m.__class__.__name__
    if classname.find('ReLU') != -1:
        m.inplace=True

model.apply(inplace_relu)
```

ä½œè€…ï¼šéƒ‘å“²ä¸œ
é“¾æ¥ï¼šhttps://www.zhihu.com/question/274635237/answer/573633662
æ¥æºï¼šçŸ¥ä¹
è‘—ä½œæƒå½’ä½œè€…æ‰€æœ‰ã€‚å•†ä¸šè½¬è½½è¯·è”ç³»ä½œè€…è·å¾—æˆæƒï¼Œéå•†ä¸šè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚



### 5.3 å°†batchnormå’Œreluæ‰“åŒ…æˆinplace

è¿›ä¸€æ­¥ï¼Œæ¯”å¦‚ResNet å’Œ DenseNet å¯ä»¥å°† batchnorm å’Œreluæ‰“åŒ…æˆinplaceï¼Œåœ¨bpæ—¶å†é‡æ–°è®¡ç®—ã€‚ä½¿ç”¨åˆ°äº†pytorchæ–°çš„checkpointç‰¹æ€§ï¼Œæœ‰ä»¥ä¸‹ä¸¤ä¸ªä»£ç ã€‚ç”±äºéœ€è¦é‡æ–°è®¡ç®—bnåçš„ç»“æœï¼Œæ‰€ä»¥ä¼šæ…¢ä¸€äº›ã€‚

- [gpleiss/efficient_densenet_pytorch](https://link.zhihu.com/?target=https%3A//github.com/gpleiss/efficient_densenet_pytorch)
- [mapillary/inplace_abn](https://link.zhihu.com/?target=https%3A//github.com/mapillary/inplace_abn)



![img](imgs\s.png)

### 5.4 æ··åˆç²¾åº¦

 ä½¿ç”¨float16ç²¾åº¦æ··åˆè®¡ç®—ã€‚æˆ‘ç”¨è¿‡ NVIDIA çš„apexã€‚

### 5.5 ä½¿ç”¨torch.no_grad

å¯¹äºä¸éœ€è¦bpçš„forwardï¼Œå¦‚validation è¯·ä½¿ç”¨ torch.no_grad , æ³¨æ„model.eval() ä¸ç­‰äº torch.no_grad() è¯·çœ‹å¦‚ä¸‹[è®¨è®º](https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615)ã€‚

> These two have different goals:
>
> - `model.eval()` will notify all your layers that you are in eval mode, that way, batchnorm or dropout layers will work in eval mode instead of training mode.
> - `torch.no_grad()` impacts the autograd engine and deactivate it. It will reduce memory usage and speed up computations but you wonâ€™t be able to backprop (which you donâ€™t want in an eval script).

### 5.6 torch.cuda.empty_cache() 

è¿™æ˜¯delçš„è¿›é˜¶ç‰ˆï¼Œä½¿ç”¨nvidia-smi ä¼šå‘ç°æ˜¾å­˜æœ‰æ˜æ˜¾çš„å˜åŒ–ã€‚ä½†æ˜¯è®­ç»ƒæ—¶æœ€å¤§çš„æ˜¾å­˜å ç”¨ä¼¼ä¹æ²¡å˜ã€‚

[How can we release GPU memory cache?](https://link.zhihu.com/?target=https%3A//discuss.pytorch.org/t/how-can-we-release-gpu-memory-cache/14530)



### 5.7 torch.backends.cudnn.benchmark = True

è®¾ç½®torch.backends.cudnn.benchmark = True

ä½¿ç”¨benchmarkä»¥å¯åŠ¨CUDNN_FINDè‡ªåŠ¨å¯»æ‰¾æœ€å¿«çš„æ“ä½œï¼Œå½“è®¡ç®—å›¾ä¸ä¼šæ”¹å˜çš„æ—¶å€™ï¼ˆæ¯æ¬¡è¾“å…¥å½¢çŠ¶ç›¸åŒï¼Œæ¨¡å‹ä¸æ”¹å˜ï¼‰çš„æƒ…å†µä¸‹å¯ä»¥æé«˜æ€§èƒ½ï¼Œåä¹‹åˆ™é™ä½æ€§èƒ½

é»˜è®¤è¿™ä¸ªé€‰é¡¹æ˜¯å…³é—­çš„ï¼Œå¯¹äºæˆ‘ä»¬å¤§å¤šæ•°çš„ä»»åŠ¡æ¥è¯´ï¼Œåœ¨å¼€å¯çš„æ—¶å€™cudnnå¯ä»¥æ ¹æ®å½“å‰çš„è®¾ç½®æ¥é€‰æ‹©æœ€ä¼˜ç®—æ³•æ¥åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚ä½†æ˜¯å¦‚æœæˆ‘ä»¬çš„è¾“å…¥åœ¨æ¯ä¸€æ¬¡çš„iterateçš„æ—¶å€™éƒ½è¿›è¡Œå˜åŒ–ï¼Œé‚£ä¹ˆbenchmarkå°±ä¼šåœ¨æ¯æ¬¡iterateçš„æ—¶å€™é‡æ–°é€‰æ‹©æœ€ä¼˜ç®—æ³•ï¼Œå½“é€‰é€‰æ‹©æ˜¯éœ€è¦èŠ±è´¹æ—¶é—´çš„ï¼Œåè€Œé€Ÿåº¦ä¼šå˜æ…¢ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå¦‚æœæˆ‘ä»¬æ¯æ¬¡è®­ç»ƒçš„è¾“å…¥æ•°æ®çš„sizeä¸å˜ï¼Œé‚£ä¹ˆå¼€å¯è¿™ä¸ªå°±ä¼šåŠ å¿«æˆ‘ä»¬çš„è®­ç»ƒé€Ÿåº¦ï¼š

### 5.8 torch.backends.cudnn.deterministic

è¿˜å¯ä»¥é‡‡ç”¨ç¡®å®šæ€§å·ç§¯ï¼šï¼ˆç›¸å½“äºæŠŠæ‰€æœ‰æ“ä½œçš„seed=0ï¼Œä»¥ä¾¿é‡ç°ï¼Œä¼šå˜æ…¢ï¼‰

torch.backends.cudnn.deterministic
